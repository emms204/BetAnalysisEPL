{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EPL_Prediction Engine.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWYj__6x3_hr"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "#Data Visualization Libraries\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#Linear Algebra\n",
        "import numpy as np\n",
        "from numpy import mean\n",
        " \n",
        "#Import Datetime module\n",
        "from datetime import datetime\n",
        "\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold, KFold,StratifiedKFold,GridSearchCV,RandomizedSearchCV, train_test_split #For splitting\n",
        "\n",
        "#Evaluation Metrics\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
        "\n",
        "#To ignore unnecessary warnings\n",
        "import warnings\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder,OneHotEncoder # for encoding categorical variables\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mMK9Xwo064kL"
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PxH2ljJz4RCJ",
        "outputId": "995fd2e1-50b3-45ea-d2ef-647159880f71"
      },
      "source": [
        "!pip install catboost"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting catboost\n",
            "  Downloading catboost-1.0.3-cp37-none-manylinux1_x86_64.whl (76.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 76.3 MB 1.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from catboost) (3.2.2)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.7/dist-packages (from catboost) (0.10.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from catboost) (1.4.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from catboost) (1.15.0)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.7/dist-packages (from catboost) (1.1.5)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from catboost) (1.19.5)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.7/dist-packages (from catboost) (4.4.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->catboost) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->catboost) (2018.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->catboost) (1.3.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->catboost) (3.0.6)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->catboost) (0.11.0)\n",
            "Requirement already satisfied: retrying>=1.3.3 in /usr/local/lib/python3.7/dist-packages (from plotly->catboost) (1.3.3)\n",
            "Installing collected packages: catboost\n",
            "Successfully installed catboost-1.0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zDIc8rg5tfGw",
        "outputId": "94622abd-573e-4362-be66-510b27192a0a"
      },
      "source": [
        "!pip install optuna"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-2.10.0-py3-none-any.whl (308 kB)\n",
            "\u001b[?25l\r\u001b[K     |█                               | 10 kB 17.2 MB/s eta 0:00:01\r\u001b[K     |██▏                             | 20 kB 23.3 MB/s eta 0:00:01\r\u001b[K     |███▏                            | 30 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |████▎                           | 40 kB 9.3 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 51 kB 4.7 MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 61 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 71 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 81 kB 6.1 MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 92 kB 4.7 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 102 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 112 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 122 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 133 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 143 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 153 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 163 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 174 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 184 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 194 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 204 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 215 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 225 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 235 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 245 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 256 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 266 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 276 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 286 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 296 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 307 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 308 kB 5.0 MB/s \n",
            "\u001b[?25hCollecting colorlog\n",
            "  Downloading colorlog-6.6.0-py2.py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: sqlalchemy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (1.4.27)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from optuna) (4.62.3)\n",
            "Collecting cliff\n",
            "  Downloading cliff-3.10.0-py3-none-any.whl (80 kB)\n",
            "\u001b[K     |████████████████████████████████| 80 kB 8.6 MB/s \n",
            "\u001b[?25hCollecting alembic\n",
            "  Downloading alembic-1.7.5-py3-none-any.whl (209 kB)\n",
            "\u001b[K     |████████████████████████████████| 209 kB 44.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (21.3)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from optuna) (3.13)\n",
            "Collecting cmaes>=0.8.2\n",
            "  Downloading cmaes-0.8.2-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: scipy!=1.4.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (1.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from optuna) (1.19.5)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->optuna) (3.0.6)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=1.1.0->optuna) (1.1.2)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=1.1.0->optuna) (4.8.2)\n",
            "Collecting Mako\n",
            "  Downloading Mako-1.1.6-py2.py3-none-any.whl (75 kB)\n",
            "\u001b[K     |████████████████████████████████| 75 kB 4.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from alembic->optuna) (5.4.0)\n",
            "Collecting pbr!=2.1.0,>=2.0.0\n",
            "  Downloading pbr-5.8.0-py2.py3-none-any.whl (112 kB)\n",
            "\u001b[K     |████████████████████████████████| 112 kB 47.2 MB/s \n",
            "\u001b[?25hCollecting autopage>=0.4.0\n",
            "  Downloading autopage-0.4.0-py3-none-any.whl (20 kB)\n",
            "Collecting cmd2>=1.0.0\n",
            "  Downloading cmd2-2.3.3-py3-none-any.whl (149 kB)\n",
            "\u001b[K     |████████████████████████████████| 149 kB 67.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PrettyTable>=0.7.2 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna) (2.4.0)\n",
            "Collecting stevedore>=2.0.1\n",
            "  Downloading stevedore-3.5.0-py3-none-any.whl (49 kB)\n",
            "\u001b[K     |████████████████████████████████| 49 kB 5.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wcwidth>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (0.2.5)\n",
            "Requirement already satisfied: attrs>=16.3.0 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (21.2.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (3.10.0.2)\n",
            "Collecting pyperclip>=1.6\n",
            "  Downloading pyperclip-1.8.2.tar.gz (20 kB)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->sqlalchemy>=1.1.0->optuna) (3.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.7/dist-packages (from Mako->alembic->optuna) (2.0.1)\n",
            "Building wheels for collected packages: pyperclip\n",
            "  Building wheel for pyperclip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyperclip: filename=pyperclip-1.8.2-py3-none-any.whl size=11136 sha256=85c4c48936e57686deb2f8a47d4ad8ea2f998e8dcbb6e96587250edf73acb3c6\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/18/84/8f69f8b08169c7bae2dde6bd7daf0c19fca8c8e500ee620a28\n",
            "Successfully built pyperclip\n",
            "Installing collected packages: pyperclip, pbr, stevedore, Mako, cmd2, autopage, colorlog, cmaes, cliff, alembic, optuna\n",
            "Successfully installed Mako-1.1.6 alembic-1.7.5 autopage-0.4.0 cliff-3.10.0 cmaes-0.8.2 cmd2-2.3.3 colorlog-6.6.0 optuna-2.10.0 pbr-5.8.0 pyperclip-1.8.2 stevedore-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dBQSSaXD4UIA"
      },
      "source": [
        "import catboost as ctb\n",
        "import lightgbm as ltb\n",
        "from xgboost import XGBClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "npDPu4QC0Vyh"
      },
      "source": [
        "data = pd.read_csv('/content/sample_data/EPL_Data.csv',parse_dates=['Date'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 661
        },
        "id": "N76NO3XPbML6",
        "outputId": "dd089642-b517-42e8-b4c4-30c5634434ac"
      },
      "source": [
        "data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-0de767ff-fbe4-47d8-b4b6-abf852bf8fcb\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>HomeTeam</th>\n",
              "      <th>AwayTeam</th>\n",
              "      <th>FTHG</th>\n",
              "      <th>FTAG</th>\n",
              "      <th>FTR</th>\n",
              "      <th>HTHG</th>\n",
              "      <th>HTAG</th>\n",
              "      <th>HTR</th>\n",
              "      <th>B365H</th>\n",
              "      <th>B365D</th>\n",
              "      <th>B365A</th>\n",
              "      <th>BWH</th>\n",
              "      <th>BWD</th>\n",
              "      <th>BWA</th>\n",
              "      <th>IWH</th>\n",
              "      <th>IWD</th>\n",
              "      <th>IWA</th>\n",
              "      <th>PSH</th>\n",
              "      <th>PSD</th>\n",
              "      <th>PSA</th>\n",
              "      <th>WHH</th>\n",
              "      <th>WHD</th>\n",
              "      <th>WHA</th>\n",
              "      <th>VCH</th>\n",
              "      <th>VCD</th>\n",
              "      <th>VCA</th>\n",
              "      <th>MaxH</th>\n",
              "      <th>MaxD</th>\n",
              "      <th>MaxA</th>\n",
              "      <th>AvgH</th>\n",
              "      <th>AvgD</th>\n",
              "      <th>AvgA</th>\n",
              "      <th>Max&gt;2.5</th>\n",
              "      <th>Max&lt;2.5</th>\n",
              "      <th>Avg&gt;2.5</th>\n",
              "      <th>Avg&lt;2.5</th>\n",
              "      <th>MaxAHH</th>\n",
              "      <th>MaxAHA</th>\n",
              "      <th>AvgAHH</th>\n",
              "      <th>AvgAHA</th>\n",
              "      <th>Year</th>\n",
              "      <th>Month</th>\n",
              "      <th>day</th>\n",
              "      <th>home_standev</th>\n",
              "      <th>home_var</th>\n",
              "      <th>away_standev</th>\n",
              "      <th>away_var</th>\n",
              "      <th>draw_standev</th>\n",
              "      <th>draw_var</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2020-08-08</td>\n",
              "      <td>Club Brugge</td>\n",
              "      <td>Charleroi</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>D</td>\n",
              "      <td>1.50</td>\n",
              "      <td>3.8</td>\n",
              "      <td>2.00</td>\n",
              "      <td>1.50</td>\n",
              "      <td>3.00</td>\n",
              "      <td>2.00</td>\n",
              "      <td>1.53</td>\n",
              "      <td>3.00</td>\n",
              "      <td>2.00</td>\n",
              "      <td>1.53</td>\n",
              "      <td>3.15</td>\n",
              "      <td>2.06</td>\n",
              "      <td>1.50</td>\n",
              "      <td>3.00</td>\n",
              "      <td>2.00</td>\n",
              "      <td>1.530000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>2.050000</td>\n",
              "      <td>1.59</td>\n",
              "      <td>3.24</td>\n",
              "      <td>2.10</td>\n",
              "      <td>1.52</td>\n",
              "      <td>3.07</td>\n",
              "      <td>2.01</td>\n",
              "      <td>2.00</td>\n",
              "      <td>1.99</td>\n",
              "      <td>1.91</td>\n",
              "      <td>1.87</td>\n",
              "      <td>2.02</td>\n",
              "      <td>1.97</td>\n",
              "      <td>1.94</td>\n",
              "      <td>1.89</td>\n",
              "      <td>2020.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>0.029761</td>\n",
              "      <td>0.000886</td>\n",
              "      <td>0.038079</td>\n",
              "      <td>0.001450</td>\n",
              "      <td>0.171277</td>\n",
              "      <td>0.029336</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2020-08-08</td>\n",
              "      <td>Antwerp</td>\n",
              "      <td>Mouscron</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>D</td>\n",
              "      <td>1.30</td>\n",
              "      <td>3.1</td>\n",
              "      <td>2.00</td>\n",
              "      <td>1.30</td>\n",
              "      <td>3.00</td>\n",
              "      <td>2.00</td>\n",
              "      <td>1.33</td>\n",
              "      <td>3.00</td>\n",
              "      <td>2.00</td>\n",
              "      <td>1.32</td>\n",
              "      <td>3.15</td>\n",
              "      <td>2.06</td>\n",
              "      <td>1.29</td>\n",
              "      <td>3.00</td>\n",
              "      <td>2.00</td>\n",
              "      <td>2.611007</td>\n",
              "      <td>3.886275</td>\n",
              "      <td>3.795705</td>\n",
              "      <td>1.37</td>\n",
              "      <td>3.24</td>\n",
              "      <td>2.10</td>\n",
              "      <td>1.31</td>\n",
              "      <td>3.07</td>\n",
              "      <td>2.01</td>\n",
              "      <td>1.78</td>\n",
              "      <td>1.64</td>\n",
              "      <td>1.68</td>\n",
              "      <td>1.57</td>\n",
              "      <td>1.83</td>\n",
              "      <td>1.90</td>\n",
              "      <td>1.99</td>\n",
              "      <td>1.84</td>\n",
              "      <td>2020.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>0.023905</td>\n",
              "      <td>0.000571</td>\n",
              "      <td>0.038079</td>\n",
              "      <td>0.001450</td>\n",
              "      <td>0.071913</td>\n",
              "      <td>0.005171</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2020-08-08</td>\n",
              "      <td>Standard</td>\n",
              "      <td>Cercle Brugge</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>D</td>\n",
              "      <td>1.40</td>\n",
              "      <td>3.1</td>\n",
              "      <td>2.00</td>\n",
              "      <td>1.40</td>\n",
              "      <td>3.00</td>\n",
              "      <td>2.00</td>\n",
              "      <td>1.43</td>\n",
              "      <td>3.00</td>\n",
              "      <td>2.00</td>\n",
              "      <td>1.43</td>\n",
              "      <td>3.15</td>\n",
              "      <td>2.06</td>\n",
              "      <td>1.40</td>\n",
              "      <td>3.00</td>\n",
              "      <td>2.00</td>\n",
              "      <td>1.440000</td>\n",
              "      <td>3.100000</td>\n",
              "      <td>2.050000</td>\n",
              "      <td>1.50</td>\n",
              "      <td>3.24</td>\n",
              "      <td>2.10</td>\n",
              "      <td>1.43</td>\n",
              "      <td>3.07</td>\n",
              "      <td>2.01</td>\n",
              "      <td>1.78</td>\n",
              "      <td>1.64</td>\n",
              "      <td>1.69</td>\n",
              "      <td>1.57</td>\n",
              "      <td>2.06</td>\n",
              "      <td>1.90</td>\n",
              "      <td>1.98</td>\n",
              "      <td>1.85</td>\n",
              "      <td>2020.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>0.033139</td>\n",
              "      <td>0.001098</td>\n",
              "      <td>0.038079</td>\n",
              "      <td>0.001450</td>\n",
              "      <td>0.071913</td>\n",
              "      <td>0.005171</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2020-09-08</td>\n",
              "      <td>St Truiden</td>\n",
              "      <td>Gent</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>D</td>\n",
              "      <td>1.50</td>\n",
              "      <td>3.8</td>\n",
              "      <td>1.65</td>\n",
              "      <td>1.50</td>\n",
              "      <td>3.00</td>\n",
              "      <td>1.67</td>\n",
              "      <td>1.50</td>\n",
              "      <td>3.00</td>\n",
              "      <td>1.70</td>\n",
              "      <td>1.53</td>\n",
              "      <td>3.15</td>\n",
              "      <td>1.72</td>\n",
              "      <td>1.52</td>\n",
              "      <td>3.00</td>\n",
              "      <td>1.70</td>\n",
              "      <td>2.611007</td>\n",
              "      <td>3.886275</td>\n",
              "      <td>3.795705</td>\n",
              "      <td>1.57</td>\n",
              "      <td>3.24</td>\n",
              "      <td>1.76</td>\n",
              "      <td>1.51</td>\n",
              "      <td>3.07</td>\n",
              "      <td>1.70</td>\n",
              "      <td>1.57</td>\n",
              "      <td>1.64</td>\n",
              "      <td>1.54</td>\n",
              "      <td>1.57</td>\n",
              "      <td>1.97</td>\n",
              "      <td>1.97</td>\n",
              "      <td>1.93</td>\n",
              "      <td>1.89</td>\n",
              "      <td>2020.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>0.023905</td>\n",
              "      <td>0.000571</td>\n",
              "      <td>0.038079</td>\n",
              "      <td>0.001450</td>\n",
              "      <td>0.071913</td>\n",
              "      <td>0.005171</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2020-09-08</td>\n",
              "      <td>Waregem</td>\n",
              "      <td>Genk</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>H</td>\n",
              "      <td>1.50</td>\n",
              "      <td>3.6</td>\n",
              "      <td>1.90</td>\n",
              "      <td>1.50</td>\n",
              "      <td>3.75</td>\n",
              "      <td>1.87</td>\n",
              "      <td>1.50</td>\n",
              "      <td>3.65</td>\n",
              "      <td>1.95</td>\n",
              "      <td>1.53</td>\n",
              "      <td>3.91</td>\n",
              "      <td>1.97</td>\n",
              "      <td>1.52</td>\n",
              "      <td>3.00</td>\n",
              "      <td>1.91</td>\n",
              "      <td>2.611007</td>\n",
              "      <td>3.886275</td>\n",
              "      <td>3.795705</td>\n",
              "      <td>1.57</td>\n",
              "      <td>3.24</td>\n",
              "      <td>2.01</td>\n",
              "      <td>1.51</td>\n",
              "      <td>3.79</td>\n",
              "      <td>1.93</td>\n",
              "      <td>1.53</td>\n",
              "      <td>1.64</td>\n",
              "      <td>1.49</td>\n",
              "      <td>1.57</td>\n",
              "      <td>1.97</td>\n",
              "      <td>1.98</td>\n",
              "      <td>1.89</td>\n",
              "      <td>1.92</td>\n",
              "      <td>2020.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>0.023905</td>\n",
              "      <td>0.000571</td>\n",
              "      <td>0.038079</td>\n",
              "      <td>0.001450</td>\n",
              "      <td>0.185282</td>\n",
              "      <td>0.034329</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60068</th>\n",
              "      <td>2021-12-09</td>\n",
              "      <td>Rizespor</td>\n",
              "      <td>Hatayspor</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>D</td>\n",
              "      <td>2.80</td>\n",
              "      <td>3.3</td>\n",
              "      <td>2.25</td>\n",
              "      <td>2.85</td>\n",
              "      <td>3.50</td>\n",
              "      <td>2.30</td>\n",
              "      <td>1.50</td>\n",
              "      <td>3.35</td>\n",
              "      <td>2.45</td>\n",
              "      <td>2.88</td>\n",
              "      <td>3.43</td>\n",
              "      <td>2.54</td>\n",
              "      <td>2.75</td>\n",
              "      <td>3.25</td>\n",
              "      <td>2.45</td>\n",
              "      <td>2.880000</td>\n",
              "      <td>3.300000</td>\n",
              "      <td>2.500000</td>\n",
              "      <td>2.93</td>\n",
              "      <td>3.56</td>\n",
              "      <td>2.56</td>\n",
              "      <td>2.83</td>\n",
              "      <td>3.36</td>\n",
              "      <td>2.44</td>\n",
              "      <td>1.95</td>\n",
              "      <td>2.08</td>\n",
              "      <td>1.88</td>\n",
              "      <td>1.93</td>\n",
              "      <td>1.83</td>\n",
              "      <td>1.85</td>\n",
              "      <td>1.77</td>\n",
              "      <td>1.79</td>\n",
              "      <td>2013.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>0.054756</td>\n",
              "      <td>0.002998</td>\n",
              "      <td>0.109406</td>\n",
              "      <td>0.011970</td>\n",
              "      <td>0.107096</td>\n",
              "      <td>0.011470</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60069</th>\n",
              "      <td>2021-12-09</td>\n",
              "      <td>Goztep</td>\n",
              "      <td>Buyuksehyr</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>A</td>\n",
              "      <td>2.55</td>\n",
              "      <td>3.2</td>\n",
              "      <td>2.62</td>\n",
              "      <td>2.65</td>\n",
              "      <td>3.20</td>\n",
              "      <td>2.65</td>\n",
              "      <td>2.65</td>\n",
              "      <td>3.30</td>\n",
              "      <td>2.65</td>\n",
              "      <td>2.70</td>\n",
              "      <td>3.36</td>\n",
              "      <td>2.78</td>\n",
              "      <td>2.62</td>\n",
              "      <td>3.10</td>\n",
              "      <td>2.62</td>\n",
              "      <td>2.630000</td>\n",
              "      <td>3.250000</td>\n",
              "      <td>2.750000</td>\n",
              "      <td>2.73</td>\n",
              "      <td>3.42</td>\n",
              "      <td>2.80</td>\n",
              "      <td>2.62</td>\n",
              "      <td>3.25</td>\n",
              "      <td>2.69</td>\n",
              "      <td>2.12</td>\n",
              "      <td>1.92</td>\n",
              "      <td>2.04</td>\n",
              "      <td>1.78</td>\n",
              "      <td>1.96</td>\n",
              "      <td>2.01</td>\n",
              "      <td>1.90</td>\n",
              "      <td>1.94</td>\n",
              "      <td>2013.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>0.054494</td>\n",
              "      <td>0.002970</td>\n",
              "      <td>0.072309</td>\n",
              "      <td>0.005229</td>\n",
              "      <td>0.100143</td>\n",
              "      <td>0.010029</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60070</th>\n",
              "      <td>2021-12-09</td>\n",
              "      <td>Trabzonspor</td>\n",
              "      <td>Galatasaray</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>A</td>\n",
              "      <td>2.20</td>\n",
              "      <td>3.4</td>\n",
              "      <td>3.00</td>\n",
              "      <td>2.45</td>\n",
              "      <td>3.40</td>\n",
              "      <td>2.70</td>\n",
              "      <td>2.35</td>\n",
              "      <td>3.45</td>\n",
              "      <td>2.90</td>\n",
              "      <td>2.41</td>\n",
              "      <td>3.57</td>\n",
              "      <td>3.02</td>\n",
              "      <td>2.35</td>\n",
              "      <td>3.40</td>\n",
              "      <td>2.80</td>\n",
              "      <td>2.380000</td>\n",
              "      <td>3.400000</td>\n",
              "      <td>2.900000</td>\n",
              "      <td>2.45</td>\n",
              "      <td>3.66</td>\n",
              "      <td>3.08</td>\n",
              "      <td>2.37</td>\n",
              "      <td>3.42</td>\n",
              "      <td>2.89</td>\n",
              "      <td>1.91</td>\n",
              "      <td>2.11</td>\n",
              "      <td>1.79</td>\n",
              "      <td>2.03</td>\n",
              "      <td>2.09</td>\n",
              "      <td>1.90</td>\n",
              "      <td>2.04</td>\n",
              "      <td>1.81</td>\n",
              "      <td>2013.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>0.079462</td>\n",
              "      <td>0.006314</td>\n",
              "      <td>0.122991</td>\n",
              "      <td>0.015127</td>\n",
              "      <td>0.098670</td>\n",
              "      <td>0.009736</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60071</th>\n",
              "      <td>2021-09-13</td>\n",
              "      <td>Alanyaspor</td>\n",
              "      <td>Giresunspor</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>H</td>\n",
              "      <td>1.90</td>\n",
              "      <td>3.4</td>\n",
              "      <td>3.60</td>\n",
              "      <td>1.95</td>\n",
              "      <td>3.40</td>\n",
              "      <td>3.75</td>\n",
              "      <td>2.05</td>\n",
              "      <td>3.50</td>\n",
              "      <td>3.60</td>\n",
              "      <td>2.04</td>\n",
              "      <td>3.58</td>\n",
              "      <td>3.77</td>\n",
              "      <td>2.00</td>\n",
              "      <td>3.40</td>\n",
              "      <td>3.50</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>3.500000</td>\n",
              "      <td>3.600000</td>\n",
              "      <td>2.13</td>\n",
              "      <td>3.68</td>\n",
              "      <td>3.83</td>\n",
              "      <td>2.03</td>\n",
              "      <td>3.48</td>\n",
              "      <td>3.56</td>\n",
              "      <td>1.95</td>\n",
              "      <td>2.00</td>\n",
              "      <td>1.89</td>\n",
              "      <td>1.91</td>\n",
              "      <td>1.83</td>\n",
              "      <td>1.88</td>\n",
              "      <td>2.04</td>\n",
              "      <td>1.82</td>\n",
              "      <td>2013.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>0.068817</td>\n",
              "      <td>0.004736</td>\n",
              "      <td>0.116427</td>\n",
              "      <td>0.013555</td>\n",
              "      <td>0.099103</td>\n",
              "      <td>0.009821</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60072</th>\n",
              "      <td>2021-09-13</td>\n",
              "      <td>Gaziantep</td>\n",
              "      <td>Antalyaspor</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>D</td>\n",
              "      <td>2.15</td>\n",
              "      <td>3.3</td>\n",
              "      <td>3.00</td>\n",
              "      <td>2.35</td>\n",
              "      <td>3.40</td>\n",
              "      <td>2.85</td>\n",
              "      <td>2.35</td>\n",
              "      <td>3.40</td>\n",
              "      <td>3.00</td>\n",
              "      <td>2.34</td>\n",
              "      <td>3.46</td>\n",
              "      <td>3.16</td>\n",
              "      <td>2.30</td>\n",
              "      <td>3.20</td>\n",
              "      <td>3.00</td>\n",
              "      <td>2.300000</td>\n",
              "      <td>3.300000</td>\n",
              "      <td>3.100000</td>\n",
              "      <td>2.40</td>\n",
              "      <td>3.56</td>\n",
              "      <td>3.20</td>\n",
              "      <td>2.31</td>\n",
              "      <td>3.37</td>\n",
              "      <td>3.01</td>\n",
              "      <td>2.02</td>\n",
              "      <td>1.94</td>\n",
              "      <td>1.93</td>\n",
              "      <td>1.88</td>\n",
              "      <td>2.09</td>\n",
              "      <td>1.92</td>\n",
              "      <td>2.00</td>\n",
              "      <td>1.85</td>\n",
              "      <td>2013.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>0.073630</td>\n",
              "      <td>0.005421</td>\n",
              "      <td>0.110454</td>\n",
              "      <td>0.012200</td>\n",
              "      <td>0.109927</td>\n",
              "      <td>0.012084</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>60073 rows × 50 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0de767ff-fbe4-47d8-b4b6-abf852bf8fcb')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0de767ff-fbe4-47d8-b4b6-abf852bf8fcb button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0de767ff-fbe4-47d8-b4b6-abf852bf8fcb');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "            Date     HomeTeam       AwayTeam  ...  away_var  draw_standev  draw_var\n",
              "0     2020-08-08  Club Brugge      Charleroi  ...  0.001450      0.171277  0.029336\n",
              "1     2020-08-08      Antwerp       Mouscron  ...  0.001450      0.071913  0.005171\n",
              "2     2020-08-08     Standard  Cercle Brugge  ...  0.001450      0.071913  0.005171\n",
              "3     2020-09-08   St Truiden           Gent  ...  0.001450      0.071913  0.005171\n",
              "4     2020-09-08      Waregem           Genk  ...  0.001450      0.185282  0.034329\n",
              "...          ...          ...            ...  ...       ...           ...       ...\n",
              "60068 2021-12-09     Rizespor      Hatayspor  ...  0.011970      0.107096  0.011470\n",
              "60069 2021-12-09       Goztep     Buyuksehyr  ...  0.005229      0.100143  0.010029\n",
              "60070 2021-12-09  Trabzonspor    Galatasaray  ...  0.015127      0.098670  0.009736\n",
              "60071 2021-09-13   Alanyaspor    Giresunspor  ...  0.013555      0.099103  0.009821\n",
              "60072 2021-09-13    Gaziantep    Antalyaspor  ...  0.012200      0.109927  0.012084\n",
              "\n",
              "[60073 rows x 50 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJRknukVH8Kz"
      },
      "source": [
        "data.rename(columns={'Max>2.5':'MaxOv2.5','Max<2.5':'MaxUn2.5','Avg>2.5':'AvgOv2.5','Avg<2.5':'AvgUn2.5'},inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "boring-instrumentation"
      },
      "source": [
        "def quality_report(df):\n",
        "    \"\"\"\n",
        "    Description: Displays quality of data in terms of missing values, \n",
        "    unique numbers, datatypes etc.\n",
        "    \n",
        "    Arguments: Dataframe\n",
        "    \"\"\"\n",
        "    dtypes = df.dtypes\n",
        "    nuniq = df.T.apply(lambda x: x.nunique(), axis=1)\n",
        "    total = df.isnull().sum().sort_values(ascending = False)\n",
        "    percent = (df.isnull().sum()/df.isnull().count()*100).sort_values(ascending = False)\n",
        "    quality_df  = pd.concat([total, percent, nuniq, dtypes], axis=1, keys=['Total NaN', 'Percent of NaN','Nunique', 'Dtype'])\n",
        "    display(quality_df)\n",
        "\n",
        "\n",
        "def object_count_plot(df):\n",
        "    \"\"\"\n",
        "    Description : Plot countplot for all categorical features \n",
        "    present in the dataframe passed\n",
        "    \n",
        "    Argument : Dataframe\n",
        "    \"\"\"\n",
        "    \n",
        "    for var in df.columns:\n",
        "        if df[var].dtype == 'object':\n",
        "            print(df[var].value_counts())\n",
        "            plt.figure(figsize=(12,5))\n",
        "            g = sns.countplot(x=var,data=df)\n",
        "            g.set_xticklabels(g.get_xticklabels(), rotation=90, ha=\"right\")\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "            \n",
        "def numeric_distribution_plot(df):\n",
        "    \"\"\"\n",
        "    Description : Gives distribution plot for all the numeric features\n",
        "    in the dataframe passed\n",
        "    \n",
        "    Argument : Dataframe\n",
        "    \"\"\"\n",
        "    for col in df.columns:\n",
        "        if df[col].dtype != 'object':\n",
        "            print(df[col].describe())\n",
        "            plt.figure(figsize=(12,5))\n",
        "            plt.title(\"Distribution of \"+col)\n",
        "            ax = sns.distplot(df[col].dropna())\n",
        "            plt.tight_layout()\n",
        "            plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eY4N8LoPu1ur"
      },
      "source": [
        "plt.scatter()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        },
        "id": "bb0vAtllvtfT",
        "outputId": "a39defbb-1cd2-45cb-83e6-4d0c8b46f574"
      },
      "source": [
        "sns.countplot(data['FTR'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
            "  FutureWarning\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f6783f7c890>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RtAvq7QcMVH"
      },
      "source": [
        "data[\"is_month_end\"] = data.Date.dt.is_month_end.astype(int)\n",
        "data[\"is_month_start\"] = data.Date.dt.is_month_start.astype(int)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rr7fI-SocCjT"
      },
      "source": [
        "data['Year'] = data.Date.dt.year\n",
        "data['Month'] = data.Date.dt.month\n",
        "data['Quarter'] = data.Date.dt.quarter\n",
        "data['day'] = data.Date.dt.day\n",
        "data['year_month'] = data['Year'].apply(str) + \"_\"  + data['Month'].apply(str)\n",
        "data['year_month_int'], _ = pd.factorize(data['year_month'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2kSD6qDabov"
      },
      "source": [
        "data.drop(['Date','HomeTeam','AwayTeam','HTR','HTR','FTHG','FTAG','HTHG','HTAG'], axis=1, inplace=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "NMc-hwBgeeZF",
        "outputId": "d39e19b4-c07b-42a2-f573-5a409dd458ec"
      },
      "source": [
        "data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-d6c07bb8-3e15-4267-a9cf-a4ea8fbb3e5b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>FTR</th>\n",
              "      <th>B365H</th>\n",
              "      <th>B365D</th>\n",
              "      <th>B365A</th>\n",
              "      <th>BWH</th>\n",
              "      <th>BWD</th>\n",
              "      <th>BWA</th>\n",
              "      <th>IWH</th>\n",
              "      <th>IWD</th>\n",
              "      <th>IWA</th>\n",
              "      <th>PSH</th>\n",
              "      <th>PSD</th>\n",
              "      <th>PSA</th>\n",
              "      <th>WHH</th>\n",
              "      <th>WHD</th>\n",
              "      <th>WHA</th>\n",
              "      <th>VCH</th>\n",
              "      <th>VCD</th>\n",
              "      <th>VCA</th>\n",
              "      <th>MaxH</th>\n",
              "      <th>MaxD</th>\n",
              "      <th>MaxA</th>\n",
              "      <th>AvgH</th>\n",
              "      <th>AvgD</th>\n",
              "      <th>AvgA</th>\n",
              "      <th>MaxOv2.5</th>\n",
              "      <th>MaxUn2.5</th>\n",
              "      <th>AvgOv2.5</th>\n",
              "      <th>AvgUn2.5</th>\n",
              "      <th>MaxAHH</th>\n",
              "      <th>MaxAHA</th>\n",
              "      <th>AvgAHH</th>\n",
              "      <th>AvgAHA</th>\n",
              "      <th>Year</th>\n",
              "      <th>Month</th>\n",
              "      <th>day</th>\n",
              "      <th>home_standev</th>\n",
              "      <th>home_var</th>\n",
              "      <th>away_standev</th>\n",
              "      <th>away_var</th>\n",
              "      <th>draw_standev</th>\n",
              "      <th>draw_var</th>\n",
              "      <th>is_month_end</th>\n",
              "      <th>is_month_start</th>\n",
              "      <th>Quarter</th>\n",
              "      <th>year_month</th>\n",
              "      <th>year_month_int</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1.50</td>\n",
              "      <td>3.8</td>\n",
              "      <td>2.00</td>\n",
              "      <td>1.50</td>\n",
              "      <td>3.00</td>\n",
              "      <td>2.00</td>\n",
              "      <td>1.53</td>\n",
              "      <td>3.00</td>\n",
              "      <td>2.00</td>\n",
              "      <td>1.53</td>\n",
              "      <td>3.15</td>\n",
              "      <td>2.06</td>\n",
              "      <td>1.50</td>\n",
              "      <td>3.00</td>\n",
              "      <td>2.00</td>\n",
              "      <td>1.530000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>2.050000</td>\n",
              "      <td>1.59</td>\n",
              "      <td>3.24</td>\n",
              "      <td>2.10</td>\n",
              "      <td>1.52</td>\n",
              "      <td>3.07</td>\n",
              "      <td>2.01</td>\n",
              "      <td>2.00</td>\n",
              "      <td>1.99</td>\n",
              "      <td>1.91</td>\n",
              "      <td>1.87</td>\n",
              "      <td>2.02</td>\n",
              "      <td>1.97</td>\n",
              "      <td>1.94</td>\n",
              "      <td>1.89</td>\n",
              "      <td>2020</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>0.029761</td>\n",
              "      <td>0.000886</td>\n",
              "      <td>0.038079</td>\n",
              "      <td>0.001450</td>\n",
              "      <td>0.171277</td>\n",
              "      <td>0.029336</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>2020_8</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1.30</td>\n",
              "      <td>3.1</td>\n",
              "      <td>2.00</td>\n",
              "      <td>1.30</td>\n",
              "      <td>3.00</td>\n",
              "      <td>2.00</td>\n",
              "      <td>1.33</td>\n",
              "      <td>3.00</td>\n",
              "      <td>2.00</td>\n",
              "      <td>1.32</td>\n",
              "      <td>3.15</td>\n",
              "      <td>2.06</td>\n",
              "      <td>1.29</td>\n",
              "      <td>3.00</td>\n",
              "      <td>2.00</td>\n",
              "      <td>2.611007</td>\n",
              "      <td>3.886275</td>\n",
              "      <td>3.795705</td>\n",
              "      <td>1.37</td>\n",
              "      <td>3.24</td>\n",
              "      <td>2.10</td>\n",
              "      <td>1.31</td>\n",
              "      <td>3.07</td>\n",
              "      <td>2.01</td>\n",
              "      <td>1.78</td>\n",
              "      <td>1.64</td>\n",
              "      <td>1.68</td>\n",
              "      <td>1.57</td>\n",
              "      <td>1.83</td>\n",
              "      <td>1.90</td>\n",
              "      <td>1.99</td>\n",
              "      <td>1.84</td>\n",
              "      <td>2020</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>0.023905</td>\n",
              "      <td>0.000571</td>\n",
              "      <td>0.038079</td>\n",
              "      <td>0.001450</td>\n",
              "      <td>0.071913</td>\n",
              "      <td>0.005171</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>2020_8</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>1.40</td>\n",
              "      <td>3.1</td>\n",
              "      <td>2.00</td>\n",
              "      <td>1.40</td>\n",
              "      <td>3.00</td>\n",
              "      <td>2.00</td>\n",
              "      <td>1.43</td>\n",
              "      <td>3.00</td>\n",
              "      <td>2.00</td>\n",
              "      <td>1.43</td>\n",
              "      <td>3.15</td>\n",
              "      <td>2.06</td>\n",
              "      <td>1.40</td>\n",
              "      <td>3.00</td>\n",
              "      <td>2.00</td>\n",
              "      <td>1.440000</td>\n",
              "      <td>3.100000</td>\n",
              "      <td>2.050000</td>\n",
              "      <td>1.50</td>\n",
              "      <td>3.24</td>\n",
              "      <td>2.10</td>\n",
              "      <td>1.43</td>\n",
              "      <td>3.07</td>\n",
              "      <td>2.01</td>\n",
              "      <td>1.78</td>\n",
              "      <td>1.64</td>\n",
              "      <td>1.69</td>\n",
              "      <td>1.57</td>\n",
              "      <td>2.06</td>\n",
              "      <td>1.90</td>\n",
              "      <td>1.98</td>\n",
              "      <td>1.85</td>\n",
              "      <td>2020</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>0.033139</td>\n",
              "      <td>0.001098</td>\n",
              "      <td>0.038079</td>\n",
              "      <td>0.001450</td>\n",
              "      <td>0.071913</td>\n",
              "      <td>0.005171</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>2020_8</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2</td>\n",
              "      <td>1.50</td>\n",
              "      <td>3.8</td>\n",
              "      <td>1.65</td>\n",
              "      <td>1.50</td>\n",
              "      <td>3.00</td>\n",
              "      <td>1.67</td>\n",
              "      <td>1.50</td>\n",
              "      <td>3.00</td>\n",
              "      <td>1.70</td>\n",
              "      <td>1.53</td>\n",
              "      <td>3.15</td>\n",
              "      <td>1.72</td>\n",
              "      <td>1.52</td>\n",
              "      <td>3.00</td>\n",
              "      <td>1.70</td>\n",
              "      <td>2.611007</td>\n",
              "      <td>3.886275</td>\n",
              "      <td>3.795705</td>\n",
              "      <td>1.57</td>\n",
              "      <td>3.24</td>\n",
              "      <td>1.76</td>\n",
              "      <td>1.51</td>\n",
              "      <td>3.07</td>\n",
              "      <td>1.70</td>\n",
              "      <td>1.57</td>\n",
              "      <td>1.64</td>\n",
              "      <td>1.54</td>\n",
              "      <td>1.57</td>\n",
              "      <td>1.97</td>\n",
              "      <td>1.97</td>\n",
              "      <td>1.93</td>\n",
              "      <td>1.89</td>\n",
              "      <td>2020</td>\n",
              "      <td>9</td>\n",
              "      <td>8</td>\n",
              "      <td>0.023905</td>\n",
              "      <td>0.000571</td>\n",
              "      <td>0.038079</td>\n",
              "      <td>0.001450</td>\n",
              "      <td>0.071913</td>\n",
              "      <td>0.005171</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>2020_9</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>1.50</td>\n",
              "      <td>3.6</td>\n",
              "      <td>1.90</td>\n",
              "      <td>1.50</td>\n",
              "      <td>3.75</td>\n",
              "      <td>1.87</td>\n",
              "      <td>1.50</td>\n",
              "      <td>3.65</td>\n",
              "      <td>1.95</td>\n",
              "      <td>1.53</td>\n",
              "      <td>3.91</td>\n",
              "      <td>1.97</td>\n",
              "      <td>1.52</td>\n",
              "      <td>3.00</td>\n",
              "      <td>1.91</td>\n",
              "      <td>2.611007</td>\n",
              "      <td>3.886275</td>\n",
              "      <td>3.795705</td>\n",
              "      <td>1.57</td>\n",
              "      <td>3.24</td>\n",
              "      <td>2.01</td>\n",
              "      <td>1.51</td>\n",
              "      <td>3.79</td>\n",
              "      <td>1.93</td>\n",
              "      <td>1.53</td>\n",
              "      <td>1.64</td>\n",
              "      <td>1.49</td>\n",
              "      <td>1.57</td>\n",
              "      <td>1.97</td>\n",
              "      <td>1.98</td>\n",
              "      <td>1.89</td>\n",
              "      <td>1.92</td>\n",
              "      <td>2020</td>\n",
              "      <td>9</td>\n",
              "      <td>8</td>\n",
              "      <td>0.023905</td>\n",
              "      <td>0.000571</td>\n",
              "      <td>0.038079</td>\n",
              "      <td>0.001450</td>\n",
              "      <td>0.185282</td>\n",
              "      <td>0.034329</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>2020_9</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60068</th>\n",
              "      <td>0</td>\n",
              "      <td>2.80</td>\n",
              "      <td>3.3</td>\n",
              "      <td>2.25</td>\n",
              "      <td>2.85</td>\n",
              "      <td>3.50</td>\n",
              "      <td>2.30</td>\n",
              "      <td>1.50</td>\n",
              "      <td>3.35</td>\n",
              "      <td>2.45</td>\n",
              "      <td>2.88</td>\n",
              "      <td>3.43</td>\n",
              "      <td>2.54</td>\n",
              "      <td>2.75</td>\n",
              "      <td>3.25</td>\n",
              "      <td>2.45</td>\n",
              "      <td>2.880000</td>\n",
              "      <td>3.300000</td>\n",
              "      <td>2.500000</td>\n",
              "      <td>2.93</td>\n",
              "      <td>3.56</td>\n",
              "      <td>2.56</td>\n",
              "      <td>2.83</td>\n",
              "      <td>3.36</td>\n",
              "      <td>2.44</td>\n",
              "      <td>1.95</td>\n",
              "      <td>2.08</td>\n",
              "      <td>1.88</td>\n",
              "      <td>1.93</td>\n",
              "      <td>1.83</td>\n",
              "      <td>1.85</td>\n",
              "      <td>1.77</td>\n",
              "      <td>1.79</td>\n",
              "      <td>2021</td>\n",
              "      <td>12</td>\n",
              "      <td>9</td>\n",
              "      <td>0.054756</td>\n",
              "      <td>0.002998</td>\n",
              "      <td>0.109406</td>\n",
              "      <td>0.011970</td>\n",
              "      <td>0.107096</td>\n",
              "      <td>0.011470</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>2021_12</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60069</th>\n",
              "      <td>2</td>\n",
              "      <td>2.55</td>\n",
              "      <td>3.2</td>\n",
              "      <td>2.62</td>\n",
              "      <td>2.65</td>\n",
              "      <td>3.20</td>\n",
              "      <td>2.65</td>\n",
              "      <td>2.65</td>\n",
              "      <td>3.30</td>\n",
              "      <td>2.65</td>\n",
              "      <td>2.70</td>\n",
              "      <td>3.36</td>\n",
              "      <td>2.78</td>\n",
              "      <td>2.62</td>\n",
              "      <td>3.10</td>\n",
              "      <td>2.62</td>\n",
              "      <td>2.630000</td>\n",
              "      <td>3.250000</td>\n",
              "      <td>2.750000</td>\n",
              "      <td>2.73</td>\n",
              "      <td>3.42</td>\n",
              "      <td>2.80</td>\n",
              "      <td>2.62</td>\n",
              "      <td>3.25</td>\n",
              "      <td>2.69</td>\n",
              "      <td>2.12</td>\n",
              "      <td>1.92</td>\n",
              "      <td>2.04</td>\n",
              "      <td>1.78</td>\n",
              "      <td>1.96</td>\n",
              "      <td>2.01</td>\n",
              "      <td>1.90</td>\n",
              "      <td>1.94</td>\n",
              "      <td>2021</td>\n",
              "      <td>12</td>\n",
              "      <td>9</td>\n",
              "      <td>0.054494</td>\n",
              "      <td>0.002970</td>\n",
              "      <td>0.072309</td>\n",
              "      <td>0.005229</td>\n",
              "      <td>0.100143</td>\n",
              "      <td>0.010029</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>2021_12</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60070</th>\n",
              "      <td>1</td>\n",
              "      <td>2.20</td>\n",
              "      <td>3.4</td>\n",
              "      <td>3.00</td>\n",
              "      <td>2.45</td>\n",
              "      <td>3.40</td>\n",
              "      <td>2.70</td>\n",
              "      <td>2.35</td>\n",
              "      <td>3.45</td>\n",
              "      <td>2.90</td>\n",
              "      <td>2.41</td>\n",
              "      <td>3.57</td>\n",
              "      <td>3.02</td>\n",
              "      <td>2.35</td>\n",
              "      <td>3.40</td>\n",
              "      <td>2.80</td>\n",
              "      <td>2.380000</td>\n",
              "      <td>3.400000</td>\n",
              "      <td>2.900000</td>\n",
              "      <td>2.45</td>\n",
              "      <td>3.66</td>\n",
              "      <td>3.08</td>\n",
              "      <td>2.37</td>\n",
              "      <td>3.42</td>\n",
              "      <td>2.89</td>\n",
              "      <td>1.91</td>\n",
              "      <td>2.11</td>\n",
              "      <td>1.79</td>\n",
              "      <td>2.03</td>\n",
              "      <td>2.09</td>\n",
              "      <td>1.90</td>\n",
              "      <td>2.04</td>\n",
              "      <td>1.81</td>\n",
              "      <td>2021</td>\n",
              "      <td>12</td>\n",
              "      <td>9</td>\n",
              "      <td>0.079462</td>\n",
              "      <td>0.006314</td>\n",
              "      <td>0.122991</td>\n",
              "      <td>0.015127</td>\n",
              "      <td>0.098670</td>\n",
              "      <td>0.009736</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>2021_12</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60071</th>\n",
              "      <td>2</td>\n",
              "      <td>1.90</td>\n",
              "      <td>3.4</td>\n",
              "      <td>3.60</td>\n",
              "      <td>1.95</td>\n",
              "      <td>3.40</td>\n",
              "      <td>3.75</td>\n",
              "      <td>2.05</td>\n",
              "      <td>3.50</td>\n",
              "      <td>3.60</td>\n",
              "      <td>2.04</td>\n",
              "      <td>3.58</td>\n",
              "      <td>3.77</td>\n",
              "      <td>2.00</td>\n",
              "      <td>3.40</td>\n",
              "      <td>3.50</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>3.500000</td>\n",
              "      <td>3.600000</td>\n",
              "      <td>2.13</td>\n",
              "      <td>3.68</td>\n",
              "      <td>3.83</td>\n",
              "      <td>2.03</td>\n",
              "      <td>3.48</td>\n",
              "      <td>3.56</td>\n",
              "      <td>1.95</td>\n",
              "      <td>2.00</td>\n",
              "      <td>1.89</td>\n",
              "      <td>1.91</td>\n",
              "      <td>1.83</td>\n",
              "      <td>1.88</td>\n",
              "      <td>2.04</td>\n",
              "      <td>1.82</td>\n",
              "      <td>2021</td>\n",
              "      <td>9</td>\n",
              "      <td>13</td>\n",
              "      <td>0.068817</td>\n",
              "      <td>0.004736</td>\n",
              "      <td>0.116427</td>\n",
              "      <td>0.013555</td>\n",
              "      <td>0.099103</td>\n",
              "      <td>0.009821</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>2021_9</td>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60072</th>\n",
              "      <td>2</td>\n",
              "      <td>2.15</td>\n",
              "      <td>3.3</td>\n",
              "      <td>3.00</td>\n",
              "      <td>2.35</td>\n",
              "      <td>3.40</td>\n",
              "      <td>2.85</td>\n",
              "      <td>2.35</td>\n",
              "      <td>3.40</td>\n",
              "      <td>3.00</td>\n",
              "      <td>2.34</td>\n",
              "      <td>3.46</td>\n",
              "      <td>3.16</td>\n",
              "      <td>2.30</td>\n",
              "      <td>3.20</td>\n",
              "      <td>3.00</td>\n",
              "      <td>2.300000</td>\n",
              "      <td>3.300000</td>\n",
              "      <td>3.100000</td>\n",
              "      <td>2.40</td>\n",
              "      <td>3.56</td>\n",
              "      <td>3.20</td>\n",
              "      <td>2.31</td>\n",
              "      <td>3.37</td>\n",
              "      <td>3.01</td>\n",
              "      <td>2.02</td>\n",
              "      <td>1.94</td>\n",
              "      <td>1.93</td>\n",
              "      <td>1.88</td>\n",
              "      <td>2.09</td>\n",
              "      <td>1.92</td>\n",
              "      <td>2.00</td>\n",
              "      <td>1.85</td>\n",
              "      <td>2021</td>\n",
              "      <td>9</td>\n",
              "      <td>13</td>\n",
              "      <td>0.073630</td>\n",
              "      <td>0.005421</td>\n",
              "      <td>0.110454</td>\n",
              "      <td>0.012200</td>\n",
              "      <td>0.109927</td>\n",
              "      <td>0.012084</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>2021_9</td>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>60073 rows × 47 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d6c07bb8-3e15-4267-a9cf-a4ea8fbb3e5b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d6c07bb8-3e15-4267-a9cf-a4ea8fbb3e5b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d6c07bb8-3e15-4267-a9cf-a4ea8fbb3e5b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "       FTR  B365H  B365D  ...  Quarter  year_month  year_month_int\n",
              "0        0   1.50    3.8  ...        3      2020_8               0\n",
              "1        1   1.30    3.1  ...        3      2020_8               0\n",
              "2        2   1.40    3.1  ...        3      2020_8               0\n",
              "3        2   1.50    3.8  ...        3      2020_9               1\n",
              "4        0   1.50    3.6  ...        3      2020_9               1\n",
              "...    ...    ...    ...  ...      ...         ...             ...\n",
              "60068    0   2.80    3.3  ...        4     2021_12              16\n",
              "60069    2   2.55    3.2  ...        4     2021_12              16\n",
              "60070    1   2.20    3.4  ...        4     2021_12              16\n",
              "60071    2   1.90    3.4  ...        3      2021_9              13\n",
              "60072    2   2.15    3.3  ...        3      2021_9              13\n",
              "\n",
              "[60073 rows x 47 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2FUj2TMnvtqZ"
      },
      "source": [
        "poly_feature_1 = ['B365H', 'B365D', 'B365A', 'BWH', 'BWD', 'BWA', 'IWH', 'IWD',\n",
        "                 'IWA', 'PSH', 'PSD', 'PSA', 'WHH', 'WHD', 'WHA', 'VCH', 'VCD', 'VCA']\n",
        "poly_feature_2 = ['MaxH', 'MaxD', 'MaxA', 'MaxOv2.5', 'MaxUn2.5','MaxAHH', 'MaxAHA']\n",
        "poly_feature_3 = ['AvgH', 'AvgD', 'AvgA','AvgOv2.5', 'AvgUn2.5','AvgAHH', 'AvgAHA',]\n",
        "poly_feature_4 = ['home_standev', 'home_var', 'away_standev', 'away_var','draw_standev', 'draw_var']\n",
        "poly_feature_5 = ['Year','Month','Quarter','day','year_month_int','is_month_end','is_month_start']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePNT9-wzvt0G"
      },
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLvXgIsGXq4H"
      },
      "source": [
        "poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
        "\n",
        "poly1 = poly.fit_transform(data[poly_feature_1])\n",
        "poly2 = poly.fit_transform(data[poly_feature_2])\n",
        "poly3 = poly.fit_transform(data[poly_feature_3])\n",
        "poly4 = poly.fit_transform(data[poly_feature_4])\n",
        "poly5 = poly.fit_transform(data[poly_feature_5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HIQS0c-1YRGM"
      },
      "source": [
        "df_poly1 = pd.DataFrame(poly1, columns=[f\"poly1_{i}\"for i in range(poly1.shape[1])])\n",
        "df_poly2 = pd.DataFrame(poly2, columns=[f\"poly2_{i}\"for i in range(poly2.shape[1])])\n",
        "df_poly3 = pd.DataFrame(poly3, columns=[f\"poly3_{i}\"for i in range(poly3.shape[1])])\n",
        "df_poly4 = pd.DataFrame(poly4, columns=[f\"poly4_{i}\"for i in range(poly4.shape[1])])\n",
        "df_poly5 = pd.DataFrame(poly5, columns=[f\"poly5_{i}\"for i in range(poly5.shape[1])])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zr2IS6J6ZJwA"
      },
      "source": [
        "new_data = pd.concat([data,df_poly1], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rMaMyRcaa_-s"
      },
      "source": [
        "new_data1 = pd.concat([new_data,df_poly2], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vl9y6UOqbADW"
      },
      "source": [
        "new_data2 = pd.concat([new_data1,df_poly3], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-SZp3AFlbAPs"
      },
      "source": [
        "new_data = pd.concat([new_data2,df_poly4], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_data = pd.concat([new_data, df_poly5], axis=1)"
      ],
      "metadata": {
        "id": "fFmIcMx_pc0D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9BCaCAtBbAXM",
        "outputId": "6d304804-7993-4352-c6b6-2bdcbac8e274"
      },
      "source": [
        "new_data.columns"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['FTR', 'B365H', 'B365D', 'B365A', 'BWH', 'BWD', 'BWA', 'IWH', 'IWD',\n",
              "       'IWA',\n",
              "       ...\n",
              "       'poly5_18', 'poly5_19', 'poly5_20', 'poly5_21', 'poly5_22', 'poly5_23',\n",
              "       'poly5_24', 'poly5_25', 'poly5_26', 'poly5_27'],\n",
              "      dtype='object', length=323)"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q7tk9dmAbAbe"
      },
      "source": [
        "features = new_data.select_dtypes(include = 'number').columns.drop('FTR')\n",
        "features1 = data.select_dtypes(include = 'number').columns.drop('FTR')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = new_data.iloc[:int(0.8*data.shape[0])]\n",
        "\n",
        "test = new_data.iloc[int(0.8*data.shape[0]):]"
      ],
      "metadata": {
        "id": "mZiXkX5du_HT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train1 = data.iloc[:int(0.8*data.shape[0])]\n",
        "\n",
        "test1 = data.iloc[int(0.8*data.shape[0]):]"
      ],
      "metadata": {
        "id": "k8SajWOUu_Xk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxe5LtmVnwFH"
      },
      "source": [
        "X = train[features]\n",
        "y = train.iloc[:,0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in X.columns:\n",
        "  X[f'Log_{i}'] = (X[i]+1).transform(np.log)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GF--_7Rh-twK",
        "outputId": "5ceac4ed-4c40-4bac-e892-6c83e74ab19d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X1 = train1[features1]\n",
        "y1 = train1.iloc[:,0]"
      ],
      "metadata": {
        "id": "9kaG9AAquWyL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in X1.columns:\n",
        "  X1[f'Log_{i}'] = (X1[i]+1).transform(np.log)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lKeaR7Kk_izq",
        "outputId": "7b90aa6b-3952-4833-d6b5-c5e4faf15694"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXQ9lbJHbAnW"
      },
      "source": [
        "#Standardizing The Data\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "Scaled_X = sc.fit_transform(X)\n",
        "#Standardizing has no effect on the data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uNBvxLyBqEKt"
      },
      "source": [
        "#Standardizing The Data\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "Scaled_X1 = sc.fit_transform(X1)\n",
        "#Standardizing has no effect on the data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(Scaled_X, y, test_size=0.2, random_state=42,stratify=y)"
      ],
      "metadata": {
        "id": "MJV8v26sNf7V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mMykJmz95_LU"
      },
      "source": [
        "#from imblearn.over_sampling import SMOTE\n",
        "from imblearn.over_sampling import RandomOverSampler \n",
        "sm = RandomOverSampler(random_state=42)\n",
        "X_, y_ = sm.fit_resample(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.combine import SMOTEENN\n",
        "sample = SMOTEENN()\n",
        "# fit and apply the transform\n",
        "X_over, y_over = sample.fit_resample(X_train, y_train)"
      ],
      "metadata": {
        "id": "_bsFZOxBkEll"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train1, X_test1, y_train1, y_test1 = train_test_split(Scaled_X1, y1, test_size=0.2, random_state=42,stratify=y1)"
      ],
      "metadata": {
        "id": "dMN-Ac9gu_BG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import RandomOverSampler \n",
        "sm = RandomOverSampler(random_state=42)\n",
        "X_1, y_1 = sm.fit_resample(X_train1, y_train1)"
      ],
      "metadata": {
        "id": "YSkV8UAZu4rW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.combine import SMOTEENN\n",
        "sample = SMOTEENN()\n",
        "# fit and apply the transform\n",
        "X_over1, y_over1 = sample.fit_resample(X_train1, y_train1)"
      ],
      "metadata": {
        "id": "zb1BKZ1OmXEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#X_train2, X_test2, y_train2, y_test2 = train_test_split(Scaled_X, y, test_size=0.2, random_state=42,stratify=y)"
      ],
      "metadata": {
        "id": "ALvWFK39HSkx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LC8F1lG6copP"
      },
      "source": [
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier,AdaBoostClassifier, BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn import metrics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils import class_weight\n",
        "classes_weights = class_weight.compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes = [0,1,2],\n",
        "    y = y_train2\n",
        ")\n",
        "classes_weights"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aAWFEPkpHflT",
        "outputId": "e8cd7710-a00a-4541-ed0a-c8032ce865ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1.11041793, 1.24808466, 0.77029112])"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def CreateBalancedSampleWeights(y_train, largest_class_weight_coef):\n",
        "    classes = np.unique(y_train, axis = 0)\n",
        "    classes.sort()\n",
        "    class_samples = np.bincount(y_train)\n",
        "    total_samples = class_samples.sum()\n",
        "    n_classes = len(class_samples)\n",
        "    weights = total_samples / (n_classes * class_samples * 1.0)\n",
        "    class_weight_dict = {key : value for (key, value) in zip(classes, weights)}\n",
        "    class_weight_dict[classes[1]] = class_weight_dict[classes[1]] * largest_class_weight_coef\n",
        "    sample_weights = [class_weight_dict[y] for y in y_train]\n",
        "    return sample_weights\n",
        "\n",
        "largest_class_weight_coef = max(y_train2.value_counts().values)/y_train2.shape[0]\n",
        "    \n",
        "#pass y_train as numpy array\n",
        "weight = CreateBalancedSampleWeights(y_train2, largest_class_weight_coef)"
      ],
      "metadata": {
        "id": "7cJn4shgNh-3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weight"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RgiVZ7reO6QC",
        "outputId": "eb0e618f-3022-44eb-e35d-ce8fcbdf8462"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.1104179302775612,\n",
              " 0.7702911181903789,\n",
              " 1.1104179302775612,\n",
              " 1.1104179302775612,\n",
              " 0.5400921958187249,\n",
              " 0.5400921958187249,\n",
              " 1.1104179302775612,\n",
              " 1.1104179302775612,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 0.5400921958187249,\n",
              " 1.1104179302775612,\n",
              " 1.1104179302775612,\n",
              " 1.1104179302775612,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 1.1104179302775612,\n",
              " 0.7702911181903789,\n",
              " 1.1104179302775612,\n",
              " 0.5400921958187249,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 0.5400921958187249,\n",
              " 1.1104179302775612,\n",
              " 0.7702911181903789,\n",
              " 0.5400921958187249,\n",
              " 0.5400921958187249,\n",
              " 1.1104179302775612,\n",
              " 1.1104179302775612,\n",
              " 0.7702911181903789,\n",
              " 0.5400921958187249,\n",
              " 1.1104179302775612,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 0.5400921958187249,\n",
              " 0.7702911181903789,\n",
              " 1.1104179302775612,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 0.5400921958187249,\n",
              " 1.1104179302775612,\n",
              " 0.5400921958187249,\n",
              " 1.1104179302775612,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 1.1104179302775612,\n",
              " 1.1104179302775612,\n",
              " 1.1104179302775612,\n",
              " 0.5400921958187249,\n",
              " 1.1104179302775612,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 1.1104179302775612,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 1.1104179302775612,\n",
              " 0.7702911181903789,\n",
              " 0.5400921958187249,\n",
              " 1.1104179302775612,\n",
              " 0.5400921958187249,\n",
              " 0.7702911181903789,\n",
              " 0.5400921958187249,\n",
              " 0.5400921958187249,\n",
              " 0.7702911181903789,\n",
              " 1.1104179302775612,\n",
              " 1.1104179302775612,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 1.1104179302775612,\n",
              " 1.1104179302775612,\n",
              " 1.1104179302775612,\n",
              " 0.5400921958187249,\n",
              " 1.1104179302775612,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 0.5400921958187249,\n",
              " 0.7702911181903789,\n",
              " 1.1104179302775612,\n",
              " 1.1104179302775612,\n",
              " 1.1104179302775612,\n",
              " 0.7702911181903789,\n",
              " 0.5400921958187249,\n",
              " 0.7702911181903789,\n",
              " 0.5400921958187249,\n",
              " 0.5400921958187249,\n",
              " 1.1104179302775612,\n",
              " 0.7702911181903789,\n",
              " 0.5400921958187249,\n",
              " 0.5400921958187249,\n",
              " 1.1104179302775612,\n",
              " 1.1104179302775612,\n",
              " 0.7702911181903789,\n",
              " 0.5400921958187249,\n",
              " 1.1104179302775612,\n",
              " 1.1104179302775612,\n",
              " 0.7702911181903789,\n",
              " 1.1104179302775612,\n",
              " 1.1104179302775612,\n",
              " 0.7702911181903789,\n",
              " 1.1104179302775612,\n",
              " 0.7702911181903789,\n",
              " 1.1104179302775612,\n",
              " 1.1104179302775612,\n",
              " 0.7702911181903789,\n",
              " 0.5400921958187249,\n",
              " 1.1104179302775612,\n",
              " 0.7702911181903789,\n",
              " 1.1104179302775612,\n",
              " 0.5400921958187249,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 0.5400921958187249,\n",
              " 0.5400921958187249,\n",
              " 0.7702911181903789,\n",
              " 0.5400921958187249,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 0.5400921958187249,\n",
              " 0.5400921958187249,\n",
              " 0.5400921958187249,\n",
              " 0.5400921958187249,\n",
              " 0.5400921958187249,\n",
              " 1.1104179302775612,\n",
              " 1.1104179302775612,\n",
              " 0.7702911181903789,\n",
              " 1.1104179302775612,\n",
              " 1.1104179302775612,\n",
              " 1.1104179302775612,\n",
              " 0.5400921958187249,\n",
              " 1.1104179302775612,\n",
              " 0.7702911181903789,\n",
              " 1.1104179302775612,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 1.1104179302775612,\n",
              " 0.7702911181903789,\n",
              " 0.5400921958187249,\n",
              " 0.5400921958187249,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 0.5400921958187249,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 0.5400921958187249,\n",
              " 1.1104179302775612,\n",
              " 0.7702911181903789,\n",
              " 1.1104179302775612,\n",
              " 0.7702911181903789,\n",
              " 1.1104179302775612,\n",
              " 0.7702911181903789,\n",
              " 1.1104179302775612,\n",
              " 0.5400921958187249,\n",
              " 0.7702911181903789,\n",
              " 1.1104179302775612,\n",
              " 1.1104179302775612,\n",
              " 1.1104179302775612,\n",
              " 0.7702911181903789,\n",
              " 0.5400921958187249,\n",
              " 0.5400921958187249,\n",
              " 1.1104179302775612,\n",
              " 0.5400921958187249,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 1.1104179302775612,\n",
              " 0.7702911181903789,\n",
              " 1.1104179302775612,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 0.5400921958187249,\n",
              " 1.1104179302775612,\n",
              " 0.7702911181903789,\n",
              " 0.5400921958187249,\n",
              " 0.7702911181903789,\n",
              " 1.1104179302775612,\n",
              " 0.5400921958187249,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 1.1104179302775612,\n",
              " 1.1104179302775612,\n",
              " 1.1104179302775612,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 0.5400921958187249,\n",
              " 1.1104179302775612,\n",
              " 0.7702911181903789,\n",
              " 0.5400921958187249,\n",
              " 0.5400921958187249,\n",
              " 0.7702911181903789,\n",
              " 1.1104179302775612,\n",
              " 0.7702911181903789,\n",
              " 1.1104179302775612,\n",
              " 1.1104179302775612,\n",
              " 0.7702911181903789,\n",
              " 1.1104179302775612,\n",
              " 0.7702911181903789,\n",
              " 1.1104179302775612,\n",
              " 0.7702911181903789,\n",
              " 1.1104179302775612,\n",
              " 0.7702911181903789,\n",
              " 1.1104179302775612,\n",
              " 0.7702911181903789,\n",
              " 1.1104179302775612,\n",
              " 0.7702911181903789,\n",
              " 0.5400921958187249,\n",
              " 0.7702911181903789,\n",
              " 1.1104179302775612,\n",
              " 0.7702911181903789,\n",
              " 0.5400921958187249,\n",
              " 0.7702911181903789,\n",
              " 1.1104179302775612,\n",
              " 0.7702911181903789,\n",
              " 1.1104179302775612,\n",
              " 0.7702911181903789,\n",
              " 1.1104179302775612,\n",
              " 0.7702911181903789,\n",
              " 0.5400921958187249,\n",
              " 1.1104179302775612,\n",
              " 1.1104179302775612,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 0.5400921958187249,\n",
              " 0.5400921958187249,\n",
              " 1.1104179302775612,\n",
              " 0.5400921958187249,\n",
              " 0.5400921958187249,\n",
              " 0.5400921958187249,\n",
              " 1.1104179302775612,\n",
              " 0.5400921958187249,\n",
              " 0.7702911181903789,\n",
              " 0.5400921958187249,\n",
              " 1.1104179302775612,\n",
              " 1.1104179302775612,\n",
              " 0.5400921958187249,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 1.1104179302775612,\n",
              " 0.5400921958187249,\n",
              " 0.7702911181903789,\n",
              " 0.5400921958187249,\n",
              " 1.1104179302775612,\n",
              " 0.5400921958187249,\n",
              " 1.1104179302775612,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 0.5400921958187249,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 0.5400921958187249,\n",
              " 0.7702911181903789,\n",
              " 1.1104179302775612,\n",
              " 0.7702911181903789,\n",
              " 1.1104179302775612,\n",
              " 0.5400921958187249,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 0.5400921958187249,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 1.1104179302775612,\n",
              " 1.1104179302775612,\n",
              " 0.7702911181903789,\n",
              " 1.1104179302775612,\n",
              " 0.7702911181903789,\n",
              " 0.5400921958187249,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 0.5400921958187249,\n",
              " 1.1104179302775612,\n",
              " 1.1104179302775612,\n",
              " 1.1104179302775612,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 1.1104179302775612,\n",
              " 1.1104179302775612,\n",
              " 1.1104179302775612,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 1.1104179302775612,\n",
              " 0.7702911181903789,\n",
              " 0.5400921958187249,\n",
              " 1.1104179302775612,\n",
              " 0.7702911181903789,\n",
              " 0.5400921958187249,\n",
              " 0.5400921958187249,\n",
              " 1.1104179302775612,\n",
              " 1.1104179302775612,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 0.5400921958187249,\n",
              " 0.5400921958187249,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 0.5400921958187249,\n",
              " 1.1104179302775612,\n",
              " 0.7702911181903789,\n",
              " 1.1104179302775612,\n",
              " 1.1104179302775612,\n",
              " 0.7702911181903789,\n",
              " 0.5400921958187249,\n",
              " 0.7702911181903789,\n",
              " 1.1104179302775612,\n",
              " 1.1104179302775612,\n",
              " 1.1104179302775612,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 1.1104179302775612,\n",
              " 0.7702911181903789,\n",
              " 0.5400921958187249,\n",
              " 1.1104179302775612,\n",
              " 0.5400921958187249,\n",
              " 1.1104179302775612,\n",
              " 0.7702911181903789,\n",
              " 0.5400921958187249,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 0.5400921958187249,\n",
              " 0.7702911181903789,\n",
              " 0.5400921958187249,\n",
              " 1.1104179302775612,\n",
              " 0.5400921958187249,\n",
              " 1.1104179302775612,\n",
              " 1.1104179302775612,\n",
              " 0.7702911181903789,\n",
              " 1.1104179302775612,\n",
              " 1.1104179302775612,\n",
              " 1.1104179302775612,\n",
              " 0.5400921958187249,\n",
              " 0.5400921958187249,\n",
              " 0.7702911181903789,\n",
              " 1.1104179302775612,\n",
              " 1.1104179302775612,\n",
              " 0.5400921958187249,\n",
              " 0.7702911181903789,\n",
              " 0.5400921958187249,\n",
              " 0.7702911181903789,\n",
              " 1.1104179302775612,\n",
              " 0.5400921958187249,\n",
              " 1.1104179302775612,\n",
              " 1.1104179302775612,\n",
              " 0.5400921958187249,\n",
              " 0.5400921958187249,\n",
              " 0.5400921958187249,\n",
              " 0.7702911181903789,\n",
              " 0.5400921958187249,\n",
              " 0.7702911181903789,\n",
              " 0.5400921958187249,\n",
              " 0.7702911181903789,\n",
              " 1.1104179302775612,\n",
              " 0.7702911181903789,\n",
              " 0.5400921958187249,\n",
              " 1.1104179302775612,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 1.1104179302775612,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 0.5400921958187249,\n",
              " 0.5400921958187249,\n",
              " 0.5400921958187249,\n",
              " 0.5400921958187249,\n",
              " 0.5400921958187249,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 0.5400921958187249,\n",
              " 1.1104179302775612,\n",
              " 1.1104179302775612,\n",
              " 1.1104179302775612,\n",
              " 1.1104179302775612,\n",
              " 0.5400921958187249,\n",
              " 1.1104179302775612,\n",
              " 1.1104179302775612,\n",
              " 0.7702911181903789,\n",
              " 1.1104179302775612,\n",
              " 0.7702911181903789,\n",
              " 0.5400921958187249,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 1.1104179302775612,\n",
              " 0.5400921958187249,\n",
              " 1.1104179302775612,\n",
              " 0.5400921958187249,\n",
              " 1.1104179302775612,\n",
              " 0.5400921958187249,\n",
              " 0.5400921958187249,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 1.1104179302775612,\n",
              " 1.1104179302775612,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 1.1104179302775612,\n",
              " 0.5400921958187249,\n",
              " 0.5400921958187249,\n",
              " 0.5400921958187249,\n",
              " 0.5400921958187249,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 1.1104179302775612,\n",
              " 0.7702911181903789,\n",
              " 1.1104179302775612,\n",
              " 0.7702911181903789,\n",
              " 1.1104179302775612,\n",
              " 0.7702911181903789,\n",
              " 1.1104179302775612,\n",
              " 0.5400921958187249,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 1.1104179302775612,\n",
              " 0.5400921958187249,\n",
              " 0.5400921958187249,\n",
              " 0.5400921958187249,\n",
              " 1.1104179302775612,\n",
              " 0.5400921958187249,\n",
              " 0.5400921958187249,\n",
              " 0.5400921958187249,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 1.1104179302775612,\n",
              " 0.5400921958187249,\n",
              " 0.7702911181903789,\n",
              " 0.5400921958187249,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 1.1104179302775612,\n",
              " 0.7702911181903789,\n",
              " 0.5400921958187249,\n",
              " 0.7702911181903789,\n",
              " 1.1104179302775612,\n",
              " 0.5400921958187249,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 1.1104179302775612,\n",
              " 0.5400921958187249,\n",
              " 0.7702911181903789,\n",
              " 0.5400921958187249,\n",
              " 0.5400921958187249,\n",
              " 0.7702911181903789,\n",
              " 0.5400921958187249,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 0.5400921958187249,\n",
              " 1.1104179302775612,\n",
              " 0.7702911181903789,\n",
              " 1.1104179302775612,\n",
              " 0.7702911181903789,\n",
              " 1.1104179302775612,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 0.5400921958187249,\n",
              " 0.5400921958187249,\n",
              " 0.7702911181903789,\n",
              " 1.1104179302775612,\n",
              " 0.5400921958187249,\n",
              " 0.5400921958187249,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 0.5400921958187249,\n",
              " 1.1104179302775612,\n",
              " 0.5400921958187249,\n",
              " 0.7702911181903789,\n",
              " 0.5400921958187249,\n",
              " 1.1104179302775612,\n",
              " 0.7702911181903789,\n",
              " 1.1104179302775612,\n",
              " 1.1104179302775612,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 1.1104179302775612,\n",
              " 1.1104179302775612,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 1.1104179302775612,\n",
              " 1.1104179302775612,\n",
              " 0.7702911181903789,\n",
              " 0.5400921958187249,\n",
              " 0.5400921958187249,\n",
              " 0.7702911181903789,\n",
              " 1.1104179302775612,\n",
              " 0.5400921958187249,\n",
              " 1.1104179302775612,\n",
              " 1.1104179302775612,\n",
              " 1.1104179302775612,\n",
              " 1.1104179302775612,\n",
              " 0.7702911181903789,\n",
              " 0.5400921958187249,\n",
              " 0.7702911181903789,\n",
              " 0.5400921958187249,\n",
              " 1.1104179302775612,\n",
              " 1.1104179302775612,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 0.5400921958187249,\n",
              " 0.5400921958187249,\n",
              " 1.1104179302775612,\n",
              " 1.1104179302775612,\n",
              " 1.1104179302775612,\n",
              " 0.7702911181903789,\n",
              " 1.1104179302775612,\n",
              " 0.5400921958187249,\n",
              " 0.7702911181903789,\n",
              " 1.1104179302775612,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 1.1104179302775612,\n",
              " 0.5400921958187249,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 1.1104179302775612,\n",
              " 1.1104179302775612,\n",
              " 1.1104179302775612,\n",
              " 0.7702911181903789,\n",
              " 0.5400921958187249,\n",
              " 1.1104179302775612,\n",
              " 1.1104179302775612,\n",
              " 0.5400921958187249,\n",
              " 1.1104179302775612,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 1.1104179302775612,\n",
              " 1.1104179302775612,\n",
              " 1.1104179302775612,\n",
              " 0.7702911181903789,\n",
              " 0.5400921958187249,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 0.5400921958187249,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 1.1104179302775612,\n",
              " 0.5400921958187249,\n",
              " 0.7702911181903789,\n",
              " 1.1104179302775612,\n",
              " 1.1104179302775612,\n",
              " 1.1104179302775612,\n",
              " 1.1104179302775612,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 0.5400921958187249,\n",
              " 1.1104179302775612,\n",
              " 0.5400921958187249,\n",
              " 1.1104179302775612,\n",
              " 0.7702911181903789,\n",
              " 0.5400921958187249,\n",
              " 1.1104179302775612,\n",
              " 0.7702911181903789,\n",
              " 1.1104179302775612,\n",
              " 0.7702911181903789,\n",
              " 1.1104179302775612,\n",
              " 0.5400921958187249,\n",
              " 0.5400921958187249,\n",
              " 0.7702911181903789,\n",
              " 1.1104179302775612,\n",
              " 1.1104179302775612,\n",
              " 0.7702911181903789,\n",
              " 0.5400921958187249,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 1.1104179302775612,\n",
              " 1.1104179302775612,\n",
              " 1.1104179302775612,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 0.5400921958187249,\n",
              " 1.1104179302775612,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 1.1104179302775612,\n",
              " 0.7702911181903789,\n",
              " 0.5400921958187249,\n",
              " 0.5400921958187249,\n",
              " 0.5400921958187249,\n",
              " 0.7702911181903789,\n",
              " 1.1104179302775612,\n",
              " 0.7702911181903789,\n",
              " 0.5400921958187249,\n",
              " 1.1104179302775612,\n",
              " 0.7702911181903789,\n",
              " 0.5400921958187249,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 0.5400921958187249,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 0.5400921958187249,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 0.5400921958187249,\n",
              " 1.1104179302775612,\n",
              " 0.5400921958187249,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 1.1104179302775612,\n",
              " 0.7702911181903789,\n",
              " 0.5400921958187249,\n",
              " 0.5400921958187249,\n",
              " 0.7702911181903789,\n",
              " 1.1104179302775612,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 0.5400921958187249,\n",
              " 1.1104179302775612,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 0.5400921958187249,\n",
              " 0.5400921958187249,\n",
              " 0.7702911181903789,\n",
              " 0.5400921958187249,\n",
              " 0.7702911181903789,\n",
              " 1.1104179302775612,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 1.1104179302775612,\n",
              " 0.5400921958187249,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 0.5400921958187249,\n",
              " 0.5400921958187249,\n",
              " 0.5400921958187249,\n",
              " 1.1104179302775612,\n",
              " 1.1104179302775612,\n",
              " 1.1104179302775612,\n",
              " 0.7702911181903789,\n",
              " 1.1104179302775612,\n",
              " 0.7702911181903789,\n",
              " 1.1104179302775612,\n",
              " 0.7702911181903789,\n",
              " 1.1104179302775612,\n",
              " 0.5400921958187249,\n",
              " 0.5400921958187249,\n",
              " 0.7702911181903789,\n",
              " 1.1104179302775612,\n",
              " 0.7702911181903789,\n",
              " 0.5400921958187249,\n",
              " 1.1104179302775612,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 0.5400921958187249,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 0.5400921958187249,\n",
              " 1.1104179302775612,\n",
              " 0.7702911181903789,\n",
              " 1.1104179302775612,\n",
              " 0.5400921958187249,\n",
              " 0.5400921958187249,\n",
              " 0.7702911181903789,\n",
              " 1.1104179302775612,\n",
              " 0.7702911181903789,\n",
              " 0.5400921958187249,\n",
              " 1.1104179302775612,\n",
              " 0.7702911181903789,\n",
              " 1.1104179302775612,\n",
              " 1.1104179302775612,\n",
              " 0.5400921958187249,\n",
              " 1.1104179302775612,\n",
              " 0.5400921958187249,\n",
              " 1.1104179302775612,\n",
              " 0.5400921958187249,\n",
              " 0.7702911181903789,\n",
              " 1.1104179302775612,\n",
              " 0.5400921958187249,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 1.1104179302775612,\n",
              " 0.5400921958187249,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 1.1104179302775612,\n",
              " 1.1104179302775612,\n",
              " 0.7702911181903789,\n",
              " 0.5400921958187249,\n",
              " 1.1104179302775612,\n",
              " 1.1104179302775612,\n",
              " 0.5400921958187249,\n",
              " 1.1104179302775612,\n",
              " 0.7702911181903789,\n",
              " 0.5400921958187249,\n",
              " 0.5400921958187249,\n",
              " 1.1104179302775612,\n",
              " 1.1104179302775612,\n",
              " 1.1104179302775612,\n",
              " 0.5400921958187249,\n",
              " 1.1104179302775612,\n",
              " 0.7702911181903789,\n",
              " 0.5400921958187249,\n",
              " 0.5400921958187249,\n",
              " 0.7702911181903789,\n",
              " 0.5400921958187249,\n",
              " 0.7702911181903789,\n",
              " 0.5400921958187249,\n",
              " 0.7702911181903789,\n",
              " 1.1104179302775612,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 0.5400921958187249,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 0.5400921958187249,\n",
              " 0.5400921958187249,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 1.1104179302775612,\n",
              " 0.5400921958187249,\n",
              " 1.1104179302775612,\n",
              " 1.1104179302775612,\n",
              " 1.1104179302775612,\n",
              " 0.5400921958187249,\n",
              " 0.7702911181903789,\n",
              " 1.1104179302775612,\n",
              " 0.7702911181903789,\n",
              " 1.1104179302775612,\n",
              " 1.1104179302775612,\n",
              " 0.7702911181903789,\n",
              " 0.5400921958187249,\n",
              " 0.5400921958187249,\n",
              " 0.7702911181903789,\n",
              " 0.5400921958187249,\n",
              " 0.7702911181903789,\n",
              " 1.1104179302775612,\n",
              " 1.1104179302775612,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 0.5400921958187249,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 0.5400921958187249,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 0.5400921958187249,\n",
              " 0.5400921958187249,\n",
              " 1.1104179302775612,\n",
              " 0.5400921958187249,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 0.5400921958187249,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 1.1104179302775612,\n",
              " 1.1104179302775612,\n",
              " 0.5400921958187249,\n",
              " 0.5400921958187249,\n",
              " 0.7702911181903789,\n",
              " 1.1104179302775612,\n",
              " 1.1104179302775612,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 0.5400921958187249,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 1.1104179302775612,\n",
              " 0.7702911181903789,\n",
              " 0.5400921958187249,\n",
              " 0.5400921958187249,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 1.1104179302775612,\n",
              " 0.5400921958187249,\n",
              " 1.1104179302775612,\n",
              " 0.5400921958187249,\n",
              " 1.1104179302775612,\n",
              " 0.7702911181903789,\n",
              " 0.5400921958187249,\n",
              " 0.7702911181903789,\n",
              " 1.1104179302775612,\n",
              " 1.1104179302775612,\n",
              " 0.5400921958187249,\n",
              " 0.5400921958187249,\n",
              " 1.1104179302775612,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 1.1104179302775612,\n",
              " 1.1104179302775612,\n",
              " 0.5400921958187249,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 0.5400921958187249,\n",
              " 0.7702911181903789,\n",
              " 1.1104179302775612,\n",
              " 1.1104179302775612,\n",
              " 1.1104179302775612,\n",
              " 1.1104179302775612,\n",
              " 1.1104179302775612,\n",
              " 0.7702911181903789,\n",
              " 0.5400921958187249,\n",
              " 0.5400921958187249,\n",
              " 0.7702911181903789,\n",
              " 0.5400921958187249,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 1.1104179302775612,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 0.5400921958187249,\n",
              " 0.5400921958187249,\n",
              " 0.5400921958187249,\n",
              " 0.7702911181903789,\n",
              " 1.1104179302775612,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 1.1104179302775612,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 0.5400921958187249,\n",
              " 0.5400921958187249,\n",
              " 0.7702911181903789,\n",
              " 0.5400921958187249,\n",
              " 1.1104179302775612,\n",
              " 0.5400921958187249,\n",
              " 0.7702911181903789,\n",
              " 1.1104179302775612,\n",
              " 1.1104179302775612,\n",
              " 0.7702911181903789,\n",
              " 1.1104179302775612,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 0.5400921958187249,\n",
              " 1.1104179302775612,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 0.5400921958187249,\n",
              " 0.5400921958187249,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 1.1104179302775612,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 0.5400921958187249,\n",
              " 0.5400921958187249,\n",
              " 0.7702911181903789,\n",
              " 1.1104179302775612,\n",
              " 0.5400921958187249,\n",
              " 1.1104179302775612,\n",
              " 0.7702911181903789,\n",
              " 1.1104179302775612,\n",
              " 0.7702911181903789,\n",
              " 1.1104179302775612,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 0.5400921958187249,\n",
              " 1.1104179302775612,\n",
              " 0.5400921958187249,\n",
              " 1.1104179302775612,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 1.1104179302775612,\n",
              " 0.7702911181903789,\n",
              " 1.1104179302775612,\n",
              " 1.1104179302775612,\n",
              " 0.7702911181903789,\n",
              " 1.1104179302775612,\n",
              " 1.1104179302775612,\n",
              " 0.7702911181903789,\n",
              " 0.5400921958187249,\n",
              " 0.5400921958187249,\n",
              " 0.5400921958187249,\n",
              " 1.1104179302775612,\n",
              " 1.1104179302775612,\n",
              " 0.7702911181903789,\n",
              " 0.5400921958187249,\n",
              " 0.7702911181903789,\n",
              " 0.5400921958187249,\n",
              " 0.5400921958187249,\n",
              " 0.7702911181903789,\n",
              " 0.5400921958187249,\n",
              " 0.7702911181903789,\n",
              " 0.5400921958187249,\n",
              " 0.5400921958187249,\n",
              " 0.5400921958187249,\n",
              " 0.5400921958187249,\n",
              " 0.5400921958187249,\n",
              " 1.1104179302775612,\n",
              " 0.7702911181903789,\n",
              " 0.5400921958187249,\n",
              " 1.1104179302775612,\n",
              " 0.7702911181903789,\n",
              " 1.1104179302775612,\n",
              " 0.5400921958187249,\n",
              " 0.7702911181903789,\n",
              " 1.1104179302775612,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 1.1104179302775612,\n",
              " 1.1104179302775612,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 0.5400921958187249,\n",
              " 0.7702911181903789,\n",
              " 1.1104179302775612,\n",
              " 0.7702911181903789,\n",
              " 0.5400921958187249,\n",
              " 0.5400921958187249,\n",
              " 0.5400921958187249,\n",
              " 0.5400921958187249,\n",
              " 0.5400921958187249,\n",
              " 0.7702911181903789,\n",
              " 1.1104179302775612,\n",
              " 0.5400921958187249,\n",
              " 0.7702911181903789,\n",
              " 1.1104179302775612,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 0.5400921958187249,\n",
              " 0.5400921958187249,\n",
              " 1.1104179302775612,\n",
              " 1.1104179302775612,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " 0.7702911181903789,\n",
              " ...]"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EBm4AxV4fuzF",
        "outputId": "3a4580e0-fefa-46ff-ff70-20ba6b24ab06"
      },
      "source": [
        "model_xgb = XGBClassifier()\n",
        "model_xgb.fit(X_over, y_over)\n",
        "prediction = model_xgb.predict(X_test)\n",
        "\n",
        "print(f1_score(prediction, y_test, average='macro'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.41818410114894183\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "XGB Classifier with unscaled data with polynomial features\n",
        "0.5174565301827437\n",
        "\n",
        "XGB Classifier with scaled data with polynomial features\n",
        "0.5054916482941431\n",
        "\n",
        "XGB Classifier with RandomSmoteSampling with scaled data with polynomial features 0.46410212741382767\n",
        "\n",
        "XGB Classifier with RandomSmoteSampling with scaled data with polynomial features with 100 features 0.46557950753351945\n",
        "\n",
        "XGB Classifier with RandomSmoteSampling in train_test_split with scaled data with polynomial features 0.4536514086133356"
      ],
      "metadata": {
        "id": "s5DKrbHu-8nn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_xgb = XGBClassifier()\n",
        "model_xgb.fit(X_over1, y_over1)\n",
        "prediction = model_xgb.predict(X_test1)\n",
        "\n",
        "print(f1_score(prediction, y_test1, average='macro'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P63Nmkprsref",
        "outputId": "66323199-6fec-4580-f63f-d09b14d26ea1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.4195842616497653\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "XGB Classifier with unscaled data without polynomial features\n",
        "0.5167938717679408\n",
        "\n",
        "XGB Classifier with scaled data without polynomial features\n",
        "0.5055265861429057\n",
        "\n",
        "XGB Classifier with RandomSmoteSampling with scaled data without polynomial features 0.46233422590032686\n",
        "\n",
        "XGB Classifier with RandomSmoteSampling in train_test_split with scaled data without polynomial features 0.45506104591264335"
      ],
      "metadata": {
        "id": "T_ic_HGv_PCB"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Buyh_ekWvXHP",
        "outputId": "ecbe6299-eff6-486a-d433-6790d111244b"
      },
      "source": [
        "model_ltb = ltb.LGBMClassifier()\n",
        "model_ltb.fit(X_over, y_over)\n",
        "prediction = model_ltb.predict(X_test)\n",
        "\n",
        "print(f1_score(prediction, y_test, average='macro'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.4189636757815032\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LTB Classifier with unscaled data with polynomial features \n",
        "0.5365023649206154\n",
        "\n",
        "LTB Classifier with scaled data with polynomial features\n",
        "0.5369698937240855\n",
        "\n",
        "LTB Classifier with RandomSmoteSampling with scaled data with polynomial features 0.5035252605750623\n",
        "\n",
        "LTB Classifier with RandomSmoteSampling in train_test_split with scaled data with polynomial features 0.4479388541942158"
      ],
      "metadata": {
        "id": "RlF6pJE-_eI9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_ltb = ltb.LGBMClassifier()\n",
        "model_ltb.fit(X_over1, y_over1)\n",
        "prediction = model_ltb.predict(X_test1)\n",
        "\n",
        "print(f1_score(prediction, y_test1, average='macro'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n8--dHIKtih4",
        "outputId": "88d3b085-ce92-41bb-a47c-74202b5e6842"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.30227145526999527\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LTB Classifier with unscaled data without polynomial features \n",
        "0.5384212077087581\n",
        "\n",
        "LTB CLassifier with scaled data without polynomial features\n",
        "0.5231638039281893\n",
        "\n",
        "LTB Classifier with RandomSmoteSampling with scaled data without polynomial features 0.48689742150068177\n",
        "\n",
        "LTB Classifier with RandomSmoteSampling in train_test_split with scaled data without polynomial features 0.4506663405132782"
      ],
      "metadata": {
        "id": "W-QaLv0Z_rlY"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X_Rh_5oUnM0e",
        "outputId": "d06727a1-e6f3-4af7-c233-4c95d6c7a045"
      },
      "source": [
        "model_rfc = RandomForestClassifier(class_weight='balanced')\n",
        "model_rfc.fit(X_train,y_train)\n",
        "prediction = model_rfc.predict(X_test)\n",
        "\n",
        "print(f1_score(prediction, y_test, average='macro'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.41723949423421525\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "RandomForestClassifier with unscaled data with polynomial features 0.5798520335871779\n",
        "\n",
        "RandomForestClassifier with scaled data with polynomial features 0.6468171759077922\n",
        "\n",
        "RandomForestClassifier with RandomSMOTESampling scaled data with polynomial features 0.6792769860446457\n",
        "\n",
        "RandomForestClassifier with RandomSmoteSampling in train_test_split with scaled data with polynomial features 0.43438379485748285"
      ],
      "metadata": {
        "id": "-S4lJ-5IAtFL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_rfc = RandomForestClassifier()\n",
        "model_rfc.fit(X_1,y_1)\n",
        "prediction = model_rfc.predict(X_test1)\n",
        "\n",
        "print(f1_score(prediction, y_test1, average='macro'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P5yudizCtnrr",
        "outputId": "e43a3813-0afd-4816-bd9e-2f6166cd0af3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.2985464791723604\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:439: UserWarning: X has feature names, but RandomForestClassifier was fitted without feature names\n",
            "  f\"X has feature names, but {self.__class__.__name__} was fitted without\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "RandomForestClassifier with unscaled data without polynomial features \n",
        "0.5906631865117259\n",
        "\n",
        "RandomForestClassifier with scaled data without polynomial features 0.6358335601346544\n",
        "\n",
        "RandomForestClassifier with RandomSMOTESampling scaled data without polynomial features 0.6774895102606676\n",
        "\n",
        "RandomForestClassifier with RandomSmoteSampling in train_test_split with scaled data without polynomial features 0.43993455153236444"
      ],
      "metadata": {
        "id": "TgqTcsDFA9DB"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qkt2fAxnpZ3M",
        "outputId": "1f4a5d68-35a4-4eb8-ad42-c84ecff8baaf"
      },
      "source": [
        "model_bc = BaggingClassifier(base_estimator = model_rfc)\n",
        "model_bc.fit(X_train,y_train)\n",
        "prediction = model_bc.predict(X_test)\n",
        "\n",
        "print(f1_score(prediction, y_test, average='macro'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.4486131486976765\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bagging Classifier with scaled data without polynomial features 0.5803651984160404\n",
        "\n",
        "Bagging Classifier with scaled data with polynomial features 0.6370693183199364\n",
        "\n",
        "Bagging Classifier with RandomSMOTESAMPLING with scaled data with polynomial features \n",
        "0.675361026420677\n",
        "\n",
        "Bagging Classifier with RandomSMOTESAMPLING with scaled data without polynomial features \n",
        "0.6742277586589861\n",
        "\n",
        "Bagging Classifier with RandomSmoteSampling in train_test_split with scaled data with polynomial features 0.4486131486976765"
      ],
      "metadata": {
        "id": "PRwl1u9cBRLb"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zdje9S1vpaXF",
        "outputId": "8a708bbb-e480-4695-9675-d94119ec5e1d"
      },
      "source": [
        "model_cbc = ctb.CatBoostClassifier()\n",
        "model_cbc.fit(X_train,y_train)\n",
        "prediction = model_cbc.predict(X_test)\n",
        "\n",
        "print(f1_score(prediction, y_test, average='macro'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Learning rate set to 0.09659\n",
            "0:\tlearn: 1.0876722\ttotal: 599ms\tremaining: 9m 58s\n",
            "1:\tlearn: 1.0786712\ttotal: 954ms\tremaining: 7m 55s\n",
            "2:\tlearn: 1.0715717\ttotal: 1.31s\tremaining: 7m 16s\n",
            "3:\tlearn: 1.0661133\ttotal: 1.67s\tremaining: 6m 54s\n",
            "4:\tlearn: 1.0612648\ttotal: 1.99s\tremaining: 6m 35s\n",
            "5:\tlearn: 1.0578088\ttotal: 2.3s\tremaining: 6m 21s\n",
            "6:\tlearn: 1.0543122\ttotal: 2.65s\tremaining: 6m 15s\n",
            "7:\tlearn: 1.0516582\ttotal: 3s\tremaining: 6m 12s\n",
            "8:\tlearn: 1.0492641\ttotal: 3.37s\tremaining: 6m 11s\n",
            "9:\tlearn: 1.0469514\ttotal: 3.7s\tremaining: 6m 6s\n",
            "10:\tlearn: 1.0447890\ttotal: 4.07s\tremaining: 6m 5s\n",
            "11:\tlearn: 1.0429335\ttotal: 4.45s\tremaining: 6m 6s\n",
            "12:\tlearn: 1.0413433\ttotal: 4.8s\tremaining: 6m 4s\n",
            "13:\tlearn: 1.0400482\ttotal: 5.14s\tremaining: 6m 2s\n",
            "14:\tlearn: 1.0391376\ttotal: 5.46s\tremaining: 5m 58s\n",
            "15:\tlearn: 1.0380826\ttotal: 5.83s\tremaining: 5m 58s\n",
            "16:\tlearn: 1.0371514\ttotal: 6.19s\tremaining: 5m 57s\n",
            "17:\tlearn: 1.0365106\ttotal: 6.54s\tremaining: 5m 56s\n",
            "18:\tlearn: 1.0359520\ttotal: 6.88s\tremaining: 5m 55s\n",
            "19:\tlearn: 1.0354315\ttotal: 7.17s\tremaining: 5m 51s\n",
            "20:\tlearn: 1.0349706\ttotal: 7.49s\tremaining: 5m 48s\n",
            "21:\tlearn: 1.0345626\ttotal: 7.79s\tremaining: 5m 46s\n",
            "22:\tlearn: 1.0341499\ttotal: 8.08s\tremaining: 5m 43s\n",
            "23:\tlearn: 1.0337697\ttotal: 8.38s\tremaining: 5m 40s\n",
            "24:\tlearn: 1.0333014\ttotal: 8.73s\tremaining: 5m 40s\n",
            "25:\tlearn: 1.0330271\ttotal: 9.08s\tremaining: 5m 40s\n",
            "26:\tlearn: 1.0327793\ttotal: 9.38s\tremaining: 5m 38s\n",
            "27:\tlearn: 1.0325692\ttotal: 9.67s\tremaining: 5m 35s\n",
            "28:\tlearn: 1.0322693\ttotal: 9.92s\tremaining: 5m 32s\n",
            "29:\tlearn: 1.0319131\ttotal: 10.3s\tremaining: 5m 33s\n",
            "30:\tlearn: 1.0314889\ttotal: 10.7s\tremaining: 5m 33s\n",
            "31:\tlearn: 1.0312499\ttotal: 11s\tremaining: 5m 32s\n",
            "32:\tlearn: 1.0310005\ttotal: 11.3s\tremaining: 5m 31s\n",
            "33:\tlearn: 1.0307381\ttotal: 11.7s\tremaining: 5m 32s\n",
            "34:\tlearn: 1.0304265\ttotal: 12s\tremaining: 5m 31s\n",
            "35:\tlearn: 1.0302877\ttotal: 12.3s\tremaining: 5m 30s\n",
            "36:\tlearn: 1.0299981\ttotal: 12.7s\tremaining: 5m 29s\n",
            "37:\tlearn: 1.0297305\ttotal: 13.1s\tremaining: 5m 30s\n",
            "38:\tlearn: 1.0292780\ttotal: 13.4s\tremaining: 5m 29s\n",
            "39:\tlearn: 1.0289804\ttotal: 13.8s\tremaining: 5m 30s\n",
            "40:\tlearn: 1.0286581\ttotal: 14.1s\tremaining: 5m 29s\n",
            "41:\tlearn: 1.0283850\ttotal: 14.4s\tremaining: 5m 28s\n",
            "42:\tlearn: 1.0280572\ttotal: 14.7s\tremaining: 5m 28s\n",
            "43:\tlearn: 1.0277753\ttotal: 15.1s\tremaining: 5m 27s\n",
            "44:\tlearn: 1.0276248\ttotal: 15.4s\tremaining: 5m 25s\n",
            "45:\tlearn: 1.0272321\ttotal: 15.8s\tremaining: 5m 27s\n",
            "46:\tlearn: 1.0269029\ttotal: 16.1s\tremaining: 5m 26s\n",
            "47:\tlearn: 1.0266406\ttotal: 16.4s\tremaining: 5m 25s\n",
            "48:\tlearn: 1.0263873\ttotal: 16.8s\tremaining: 5m 25s\n",
            "49:\tlearn: 1.0260506\ttotal: 17.1s\tremaining: 5m 24s\n",
            "50:\tlearn: 1.0257533\ttotal: 17.4s\tremaining: 5m 23s\n",
            "51:\tlearn: 1.0253988\ttotal: 17.8s\tremaining: 5m 23s\n",
            "52:\tlearn: 1.0250220\ttotal: 18.1s\tremaining: 5m 24s\n",
            "53:\tlearn: 1.0246782\ttotal: 18.5s\tremaining: 5m 24s\n",
            "54:\tlearn: 1.0243595\ttotal: 18.8s\tremaining: 5m 22s\n",
            "55:\tlearn: 1.0240670\ttotal: 19.1s\tremaining: 5m 22s\n",
            "56:\tlearn: 1.0237635\ttotal: 19.5s\tremaining: 5m 23s\n",
            "57:\tlearn: 1.0236177\ttotal: 19.9s\tremaining: 5m 22s\n",
            "58:\tlearn: 1.0232603\ttotal: 20.2s\tremaining: 5m 21s\n",
            "59:\tlearn: 1.0230681\ttotal: 20.5s\tremaining: 5m 20s\n",
            "60:\tlearn: 1.0226285\ttotal: 20.9s\tremaining: 5m 21s\n",
            "61:\tlearn: 1.0223585\ttotal: 21.2s\tremaining: 5m 21s\n",
            "62:\tlearn: 1.0221569\ttotal: 21.6s\tremaining: 5m 21s\n",
            "63:\tlearn: 1.0219243\ttotal: 21.9s\tremaining: 5m 20s\n",
            "64:\tlearn: 1.0217255\ttotal: 22.3s\tremaining: 5m 20s\n",
            "65:\tlearn: 1.0213947\ttotal: 22.6s\tremaining: 5m 19s\n",
            "66:\tlearn: 1.0212668\ttotal: 22.9s\tremaining: 5m 19s\n",
            "67:\tlearn: 1.0210691\ttotal: 23.3s\tremaining: 5m 19s\n",
            "68:\tlearn: 1.0207963\ttotal: 23.7s\tremaining: 5m 19s\n",
            "69:\tlearn: 1.0205379\ttotal: 24s\tremaining: 5m 18s\n",
            "70:\tlearn: 1.0202416\ttotal: 24.4s\tremaining: 5m 18s\n",
            "71:\tlearn: 1.0198958\ttotal: 24.7s\tremaining: 5m 18s\n",
            "72:\tlearn: 1.0195981\ttotal: 25.1s\tremaining: 5m 18s\n",
            "73:\tlearn: 1.0194012\ttotal: 25.4s\tremaining: 5m 18s\n",
            "74:\tlearn: 1.0192134\ttotal: 25.8s\tremaining: 5m 17s\n",
            "75:\tlearn: 1.0188529\ttotal: 26.2s\tremaining: 5m 18s\n",
            "76:\tlearn: 1.0186662\ttotal: 26.5s\tremaining: 5m 17s\n",
            "77:\tlearn: 1.0184449\ttotal: 26.8s\tremaining: 5m 16s\n",
            "78:\tlearn: 1.0183182\ttotal: 27.1s\tremaining: 5m 15s\n",
            "79:\tlearn: 1.0180938\ttotal: 27.4s\tremaining: 5m 15s\n",
            "80:\tlearn: 1.0177870\ttotal: 27.8s\tremaining: 5m 15s\n",
            "81:\tlearn: 1.0176360\ttotal: 28.1s\tremaining: 5m 14s\n",
            "82:\tlearn: 1.0173935\ttotal: 28.5s\tremaining: 5m 14s\n",
            "83:\tlearn: 1.0172723\ttotal: 28.7s\tremaining: 5m 13s\n",
            "84:\tlearn: 1.0169608\ttotal: 29.1s\tremaining: 5m 13s\n",
            "85:\tlearn: 1.0167418\ttotal: 29.4s\tremaining: 5m 12s\n",
            "86:\tlearn: 1.0164957\ttotal: 29.8s\tremaining: 5m 12s\n",
            "87:\tlearn: 1.0162344\ttotal: 30s\tremaining: 5m 11s\n",
            "88:\tlearn: 1.0160920\ttotal: 30.4s\tremaining: 5m 11s\n",
            "89:\tlearn: 1.0158626\ttotal: 30.8s\tremaining: 5m 11s\n",
            "90:\tlearn: 1.0156587\ttotal: 31.1s\tremaining: 5m 10s\n",
            "91:\tlearn: 1.0154606\ttotal: 31.4s\tremaining: 5m 10s\n",
            "92:\tlearn: 1.0152715\ttotal: 31.8s\tremaining: 5m 9s\n",
            "93:\tlearn: 1.0151549\ttotal: 32s\tremaining: 5m 8s\n",
            "94:\tlearn: 1.0149873\ttotal: 32.4s\tremaining: 5m 8s\n",
            "95:\tlearn: 1.0147316\ttotal: 32.7s\tremaining: 5m 8s\n",
            "96:\tlearn: 1.0144100\ttotal: 33.1s\tremaining: 5m 8s\n",
            "97:\tlearn: 1.0141521\ttotal: 33.4s\tremaining: 5m 7s\n",
            "98:\tlearn: 1.0139388\ttotal: 33.7s\tremaining: 5m 7s\n",
            "99:\tlearn: 1.0135821\ttotal: 34.1s\tremaining: 5m 7s\n",
            "100:\tlearn: 1.0133757\ttotal: 34.4s\tremaining: 5m 6s\n",
            "101:\tlearn: 1.0132297\ttotal: 34.7s\tremaining: 5m 5s\n",
            "102:\tlearn: 1.0128300\ttotal: 35.1s\tremaining: 5m 5s\n",
            "103:\tlearn: 1.0124736\ttotal: 35.4s\tremaining: 5m 5s\n",
            "104:\tlearn: 1.0121293\ttotal: 35.8s\tremaining: 5m 4s\n",
            "105:\tlearn: 1.0118211\ttotal: 36.1s\tremaining: 5m 4s\n",
            "106:\tlearn: 1.0115795\ttotal: 36.5s\tremaining: 5m 4s\n",
            "107:\tlearn: 1.0113705\ttotal: 36.8s\tremaining: 5m 3s\n",
            "108:\tlearn: 1.0111551\ttotal: 37.1s\tremaining: 5m 3s\n",
            "109:\tlearn: 1.0109381\ttotal: 37.4s\tremaining: 5m 2s\n",
            "110:\tlearn: 1.0106915\ttotal: 37.8s\tremaining: 5m 2s\n",
            "111:\tlearn: 1.0103611\ttotal: 38.1s\tremaining: 5m 2s\n",
            "112:\tlearn: 1.0100981\ttotal: 38.4s\tremaining: 5m 1s\n",
            "113:\tlearn: 1.0097817\ttotal: 38.8s\tremaining: 5m 1s\n",
            "114:\tlearn: 1.0095732\ttotal: 39.1s\tremaining: 5m 1s\n",
            "115:\tlearn: 1.0093608\ttotal: 39.4s\tremaining: 5m\n",
            "116:\tlearn: 1.0091891\ttotal: 39.7s\tremaining: 4m 59s\n",
            "117:\tlearn: 1.0088023\ttotal: 40.1s\tremaining: 4m 59s\n",
            "118:\tlearn: 1.0085542\ttotal: 40.5s\tremaining: 4m 59s\n",
            "119:\tlearn: 1.0081310\ttotal: 40.9s\tremaining: 4m 59s\n",
            "120:\tlearn: 1.0078949\ttotal: 41.2s\tremaining: 4m 59s\n",
            "121:\tlearn: 1.0076178\ttotal: 41.5s\tremaining: 4m 58s\n",
            "122:\tlearn: 1.0074346\ttotal: 41.8s\tremaining: 4m 58s\n",
            "123:\tlearn: 1.0071819\ttotal: 42.1s\tremaining: 4m 57s\n",
            "124:\tlearn: 1.0069346\ttotal: 42.5s\tremaining: 4m 57s\n",
            "125:\tlearn: 1.0067014\ttotal: 42.8s\tremaining: 4m 56s\n",
            "126:\tlearn: 1.0064345\ttotal: 43.1s\tremaining: 4m 56s\n",
            "127:\tlearn: 1.0061438\ttotal: 43.5s\tremaining: 4m 56s\n",
            "128:\tlearn: 1.0059159\ttotal: 43.7s\tremaining: 4m 55s\n",
            "129:\tlearn: 1.0056071\ttotal: 44.1s\tremaining: 4m 54s\n",
            "130:\tlearn: 1.0052380\ttotal: 44.4s\tremaining: 4m 54s\n",
            "131:\tlearn: 1.0050401\ttotal: 44.6s\tremaining: 4m 53s\n",
            "132:\tlearn: 1.0047230\ttotal: 44.9s\tremaining: 4m 53s\n",
            "133:\tlearn: 1.0044779\ttotal: 45.3s\tremaining: 4m 52s\n",
            "134:\tlearn: 1.0042945\ttotal: 45.5s\tremaining: 4m 51s\n",
            "135:\tlearn: 1.0039745\ttotal: 45.9s\tremaining: 4m 51s\n",
            "136:\tlearn: 1.0036031\ttotal: 46.2s\tremaining: 4m 51s\n",
            "137:\tlearn: 1.0032939\ttotal: 46.5s\tremaining: 4m 50s\n",
            "138:\tlearn: 1.0029247\ttotal: 46.9s\tremaining: 4m 50s\n",
            "139:\tlearn: 1.0026028\ttotal: 47.2s\tremaining: 4m 50s\n",
            "140:\tlearn: 1.0023300\ttotal: 47.5s\tremaining: 4m 49s\n",
            "141:\tlearn: 1.0019450\ttotal: 47.9s\tremaining: 4m 49s\n",
            "142:\tlearn: 1.0016087\ttotal: 48.2s\tremaining: 4m 48s\n",
            "143:\tlearn: 1.0012556\ttotal: 48.6s\tremaining: 4m 48s\n",
            "144:\tlearn: 1.0009943\ttotal: 48.9s\tremaining: 4m 48s\n",
            "145:\tlearn: 1.0007948\ttotal: 49.1s\tremaining: 4m 47s\n",
            "146:\tlearn: 1.0005260\ttotal: 49.5s\tremaining: 4m 47s\n",
            "147:\tlearn: 1.0001484\ttotal: 49.8s\tremaining: 4m 46s\n",
            "148:\tlearn: 0.9998243\ttotal: 50.2s\tremaining: 4m 46s\n",
            "149:\tlearn: 0.9995611\ttotal: 50.5s\tremaining: 4m 46s\n",
            "150:\tlearn: 0.9993350\ttotal: 50.8s\tremaining: 4m 45s\n",
            "151:\tlearn: 0.9991429\ttotal: 51.1s\tremaining: 4m 44s\n",
            "152:\tlearn: 0.9988709\ttotal: 51.3s\tremaining: 4m 44s\n",
            "153:\tlearn: 0.9986209\ttotal: 51.6s\tremaining: 4m 43s\n",
            "154:\tlearn: 0.9983114\ttotal: 51.9s\tremaining: 4m 43s\n",
            "155:\tlearn: 0.9979083\ttotal: 52.3s\tremaining: 4m 42s\n",
            "156:\tlearn: 0.9977304\ttotal: 52.6s\tremaining: 4m 42s\n",
            "157:\tlearn: 0.9974176\ttotal: 52.9s\tremaining: 4m 41s\n",
            "158:\tlearn: 0.9971926\ttotal: 53.2s\tremaining: 4m 41s\n",
            "159:\tlearn: 0.9969436\ttotal: 53.5s\tremaining: 4m 40s\n",
            "160:\tlearn: 0.9965228\ttotal: 53.8s\tremaining: 4m 40s\n",
            "161:\tlearn: 0.9962646\ttotal: 54.1s\tremaining: 4m 39s\n",
            "162:\tlearn: 0.9959581\ttotal: 54.4s\tremaining: 4m 39s\n",
            "163:\tlearn: 0.9955300\ttotal: 54.7s\tremaining: 4m 38s\n",
            "164:\tlearn: 0.9952869\ttotal: 55s\tremaining: 4m 38s\n",
            "165:\tlearn: 0.9949790\ttotal: 55.4s\tremaining: 4m 38s\n",
            "166:\tlearn: 0.9946874\ttotal: 55.7s\tremaining: 4m 37s\n",
            "167:\tlearn: 0.9943250\ttotal: 56s\tremaining: 4m 37s\n",
            "168:\tlearn: 0.9940306\ttotal: 56.4s\tremaining: 4m 37s\n",
            "169:\tlearn: 0.9937370\ttotal: 56.7s\tremaining: 4m 36s\n",
            "170:\tlearn: 0.9933396\ttotal: 57.1s\tremaining: 4m 36s\n",
            "171:\tlearn: 0.9931097\ttotal: 57.4s\tremaining: 4m 36s\n",
            "172:\tlearn: 0.9927525\ttotal: 57.8s\tremaining: 4m 36s\n",
            "173:\tlearn: 0.9924817\ttotal: 58s\tremaining: 4m 35s\n",
            "174:\tlearn: 0.9921326\ttotal: 58.4s\tremaining: 4m 35s\n",
            "175:\tlearn: 0.9917531\ttotal: 58.7s\tremaining: 4m 34s\n",
            "176:\tlearn: 0.9914298\ttotal: 59s\tremaining: 4m 34s\n",
            "177:\tlearn: 0.9911607\ttotal: 59.3s\tremaining: 4m 33s\n",
            "178:\tlearn: 0.9908630\ttotal: 59.6s\tremaining: 4m 33s\n",
            "179:\tlearn: 0.9905936\ttotal: 60s\tremaining: 4m 33s\n",
            "180:\tlearn: 0.9902903\ttotal: 1m\tremaining: 4m 32s\n",
            "181:\tlearn: 0.9899627\ttotal: 1m\tremaining: 4m 32s\n",
            "182:\tlearn: 0.9896714\ttotal: 1m\tremaining: 4m 32s\n",
            "183:\tlearn: 0.9893438\ttotal: 1m 1s\tremaining: 4m 31s\n",
            "184:\tlearn: 0.9891388\ttotal: 1m 1s\tremaining: 4m 31s\n",
            "185:\tlearn: 0.9888206\ttotal: 1m 2s\tremaining: 4m 31s\n",
            "186:\tlearn: 0.9885738\ttotal: 1m 2s\tremaining: 4m 31s\n",
            "187:\tlearn: 0.9883222\ttotal: 1m 2s\tremaining: 4m 30s\n",
            "188:\tlearn: 0.9880890\ttotal: 1m 2s\tremaining: 4m 30s\n",
            "189:\tlearn: 0.9877836\ttotal: 1m 3s\tremaining: 4m 29s\n",
            "190:\tlearn: 0.9874953\ttotal: 1m 3s\tremaining: 4m 29s\n",
            "191:\tlearn: 0.9872115\ttotal: 1m 3s\tremaining: 4m 29s\n",
            "192:\tlearn: 0.9870055\ttotal: 1m 4s\tremaining: 4m 28s\n",
            "193:\tlearn: 0.9867820\ttotal: 1m 4s\tremaining: 4m 28s\n",
            "194:\tlearn: 0.9864887\ttotal: 1m 4s\tremaining: 4m 28s\n",
            "195:\tlearn: 0.9861156\ttotal: 1m 5s\tremaining: 4m 27s\n",
            "196:\tlearn: 0.9858537\ttotal: 1m 5s\tremaining: 4m 27s\n",
            "197:\tlearn: 0.9855412\ttotal: 1m 5s\tremaining: 4m 26s\n",
            "198:\tlearn: 0.9852915\ttotal: 1m 6s\tremaining: 4m 26s\n",
            "199:\tlearn: 0.9850308\ttotal: 1m 6s\tremaining: 4m 25s\n",
            "200:\tlearn: 0.9846531\ttotal: 1m 6s\tremaining: 4m 25s\n",
            "201:\tlearn: 0.9843694\ttotal: 1m 7s\tremaining: 4m 25s\n",
            "202:\tlearn: 0.9840290\ttotal: 1m 7s\tremaining: 4m 24s\n",
            "203:\tlearn: 0.9837136\ttotal: 1m 7s\tremaining: 4m 24s\n",
            "204:\tlearn: 0.9832553\ttotal: 1m 8s\tremaining: 4m 24s\n",
            "205:\tlearn: 0.9829335\ttotal: 1m 8s\tremaining: 4m 23s\n",
            "206:\tlearn: 0.9827064\ttotal: 1m 8s\tremaining: 4m 23s\n",
            "207:\tlearn: 0.9824125\ttotal: 1m 9s\tremaining: 4m 23s\n",
            "208:\tlearn: 0.9820998\ttotal: 1m 9s\tremaining: 4m 23s\n",
            "209:\tlearn: 0.9817588\ttotal: 1m 9s\tremaining: 4m 22s\n",
            "210:\tlearn: 0.9814641\ttotal: 1m 10s\tremaining: 4m 22s\n",
            "211:\tlearn: 0.9812449\ttotal: 1m 10s\tremaining: 4m 22s\n",
            "212:\tlearn: 0.9809798\ttotal: 1m 10s\tremaining: 4m 21s\n",
            "213:\tlearn: 0.9807040\ttotal: 1m 11s\tremaining: 4m 21s\n",
            "214:\tlearn: 0.9804993\ttotal: 1m 11s\tremaining: 4m 21s\n",
            "215:\tlearn: 0.9801403\ttotal: 1m 11s\tremaining: 4m 21s\n",
            "216:\tlearn: 0.9797750\ttotal: 1m 12s\tremaining: 4m 20s\n",
            "217:\tlearn: 0.9794934\ttotal: 1m 12s\tremaining: 4m 20s\n",
            "218:\tlearn: 0.9792236\ttotal: 1m 13s\tremaining: 4m 20s\n",
            "219:\tlearn: 0.9788978\ttotal: 1m 13s\tremaining: 4m 19s\n",
            "220:\tlearn: 0.9786326\ttotal: 1m 13s\tremaining: 4m 19s\n",
            "221:\tlearn: 0.9783061\ttotal: 1m 13s\tremaining: 4m 19s\n",
            "222:\tlearn: 0.9781194\ttotal: 1m 14s\tremaining: 4m 18s\n",
            "223:\tlearn: 0.9778029\ttotal: 1m 14s\tremaining: 4m 18s\n",
            "224:\tlearn: 0.9775629\ttotal: 1m 14s\tremaining: 4m 17s\n",
            "225:\tlearn: 0.9772383\ttotal: 1m 15s\tremaining: 4m 17s\n",
            "226:\tlearn: 0.9769445\ttotal: 1m 15s\tremaining: 4m 17s\n",
            "227:\tlearn: 0.9767783\ttotal: 1m 15s\tremaining: 4m 16s\n",
            "228:\tlearn: 0.9764581\ttotal: 1m 16s\tremaining: 4m 16s\n",
            "229:\tlearn: 0.9762515\ttotal: 1m 16s\tremaining: 4m 15s\n",
            "230:\tlearn: 0.9759982\ttotal: 1m 16s\tremaining: 4m 15s\n",
            "231:\tlearn: 0.9756952\ttotal: 1m 17s\tremaining: 4m 15s\n",
            "232:\tlearn: 0.9754444\ttotal: 1m 17s\tremaining: 4m 14s\n",
            "233:\tlearn: 0.9750974\ttotal: 1m 17s\tremaining: 4m 14s\n",
            "234:\tlearn: 0.9749483\ttotal: 1m 17s\tremaining: 4m 13s\n",
            "235:\tlearn: 0.9746298\ttotal: 1m 18s\tremaining: 4m 13s\n",
            "236:\tlearn: 0.9742929\ttotal: 1m 18s\tremaining: 4m 13s\n",
            "237:\tlearn: 0.9740431\ttotal: 1m 18s\tremaining: 4m 12s\n",
            "238:\tlearn: 0.9737650\ttotal: 1m 19s\tremaining: 4m 12s\n",
            "239:\tlearn: 0.9734327\ttotal: 1m 19s\tremaining: 4m 11s\n",
            "240:\tlearn: 0.9731811\ttotal: 1m 19s\tremaining: 4m 11s\n",
            "241:\tlearn: 0.9730156\ttotal: 1m 20s\tremaining: 4m 11s\n",
            "242:\tlearn: 0.9727864\ttotal: 1m 20s\tremaining: 4m 10s\n",
            "243:\tlearn: 0.9724990\ttotal: 1m 20s\tremaining: 4m 10s\n",
            "244:\tlearn: 0.9722423\ttotal: 1m 21s\tremaining: 4m 9s\n",
            "245:\tlearn: 0.9719776\ttotal: 1m 21s\tremaining: 4m 9s\n",
            "246:\tlearn: 0.9715999\ttotal: 1m 21s\tremaining: 4m 9s\n",
            "247:\tlearn: 0.9713221\ttotal: 1m 22s\tremaining: 4m 8s\n",
            "248:\tlearn: 0.9710053\ttotal: 1m 22s\tremaining: 4m 8s\n",
            "249:\tlearn: 0.9707357\ttotal: 1m 22s\tremaining: 4m 8s\n",
            "250:\tlearn: 0.9705364\ttotal: 1m 22s\tremaining: 4m 7s\n",
            "251:\tlearn: 0.9701919\ttotal: 1m 23s\tremaining: 4m 7s\n",
            "252:\tlearn: 0.9698777\ttotal: 1m 23s\tremaining: 4m 7s\n",
            "253:\tlearn: 0.9695301\ttotal: 1m 24s\tremaining: 4m 6s\n",
            "254:\tlearn: 0.9691801\ttotal: 1m 24s\tremaining: 4m 6s\n",
            "255:\tlearn: 0.9688847\ttotal: 1m 24s\tremaining: 4m 6s\n",
            "256:\tlearn: 0.9686507\ttotal: 1m 25s\tremaining: 4m 5s\n",
            "257:\tlearn: 0.9682637\ttotal: 1m 25s\tremaining: 4m 5s\n",
            "258:\tlearn: 0.9681076\ttotal: 1m 25s\tremaining: 4m 5s\n",
            "259:\tlearn: 0.9677782\ttotal: 1m 25s\tremaining: 4m 4s\n",
            "260:\tlearn: 0.9675748\ttotal: 1m 26s\tremaining: 4m 4s\n",
            "261:\tlearn: 0.9673925\ttotal: 1m 26s\tremaining: 4m 3s\n",
            "262:\tlearn: 0.9670892\ttotal: 1m 26s\tremaining: 4m 3s\n",
            "263:\tlearn: 0.9668130\ttotal: 1m 27s\tremaining: 4m 3s\n",
            "264:\tlearn: 0.9665878\ttotal: 1m 27s\tremaining: 4m 2s\n",
            "265:\tlearn: 0.9664022\ttotal: 1m 27s\tremaining: 4m 2s\n",
            "266:\tlearn: 0.9661940\ttotal: 1m 28s\tremaining: 4m 2s\n",
            "267:\tlearn: 0.9659325\ttotal: 1m 28s\tremaining: 4m 1s\n",
            "268:\tlearn: 0.9656817\ttotal: 1m 28s\tremaining: 4m 1s\n",
            "269:\tlearn: 0.9653906\ttotal: 1m 29s\tremaining: 4m 1s\n",
            "270:\tlearn: 0.9651288\ttotal: 1m 29s\tremaining: 4m\n",
            "271:\tlearn: 0.9648346\ttotal: 1m 29s\tremaining: 4m\n",
            "272:\tlearn: 0.9644983\ttotal: 1m 30s\tremaining: 3m 59s\n",
            "273:\tlearn: 0.9641573\ttotal: 1m 30s\tremaining: 3m 59s\n",
            "274:\tlearn: 0.9638278\ttotal: 1m 30s\tremaining: 3m 59s\n",
            "275:\tlearn: 0.9636461\ttotal: 1m 31s\tremaining: 3m 58s\n",
            "276:\tlearn: 0.9633818\ttotal: 1m 31s\tremaining: 3m 58s\n",
            "277:\tlearn: 0.9630547\ttotal: 1m 31s\tremaining: 3m 58s\n",
            "278:\tlearn: 0.9627514\ttotal: 1m 32s\tremaining: 3m 58s\n",
            "279:\tlearn: 0.9624222\ttotal: 1m 32s\tremaining: 3m 57s\n",
            "280:\tlearn: 0.9621403\ttotal: 1m 32s\tremaining: 3m 57s\n",
            "281:\tlearn: 0.9618652\ttotal: 1m 33s\tremaining: 3m 56s\n",
            "282:\tlearn: 0.9614817\ttotal: 1m 33s\tremaining: 3m 56s\n",
            "283:\tlearn: 0.9612092\ttotal: 1m 33s\tremaining: 3m 56s\n",
            "284:\tlearn: 0.9609395\ttotal: 1m 34s\tremaining: 3m 55s\n",
            "285:\tlearn: 0.9607369\ttotal: 1m 34s\tremaining: 3m 55s\n",
            "286:\tlearn: 0.9603758\ttotal: 1m 34s\tremaining: 3m 55s\n",
            "287:\tlearn: 0.9601088\ttotal: 1m 35s\tremaining: 3m 55s\n",
            "288:\tlearn: 0.9597947\ttotal: 1m 35s\tremaining: 3m 54s\n",
            "289:\tlearn: 0.9594246\ttotal: 1m 35s\tremaining: 3m 54s\n",
            "290:\tlearn: 0.9592065\ttotal: 1m 36s\tremaining: 3m 53s\n",
            "291:\tlearn: 0.9589972\ttotal: 1m 36s\tremaining: 3m 53s\n",
            "292:\tlearn: 0.9588206\ttotal: 1m 36s\tremaining: 3m 53s\n",
            "293:\tlearn: 0.9584829\ttotal: 1m 37s\tremaining: 3m 53s\n",
            "294:\tlearn: 0.9582084\ttotal: 1m 37s\tremaining: 3m 52s\n",
            "295:\tlearn: 0.9579491\ttotal: 1m 37s\tremaining: 3m 52s\n",
            "296:\tlearn: 0.9576760\ttotal: 1m 38s\tremaining: 3m 52s\n",
            "297:\tlearn: 0.9574232\ttotal: 1m 38s\tremaining: 3m 51s\n",
            "298:\tlearn: 0.9572089\ttotal: 1m 38s\tremaining: 3m 51s\n",
            "299:\tlearn: 0.9569054\ttotal: 1m 39s\tremaining: 3m 51s\n",
            "300:\tlearn: 0.9566034\ttotal: 1m 39s\tremaining: 3m 50s\n",
            "301:\tlearn: 0.9562944\ttotal: 1m 39s\tremaining: 3m 50s\n",
            "302:\tlearn: 0.9560678\ttotal: 1m 40s\tremaining: 3m 50s\n",
            "303:\tlearn: 0.9559030\ttotal: 1m 40s\tremaining: 3m 49s\n",
            "304:\tlearn: 0.9556109\ttotal: 1m 40s\tremaining: 3m 49s\n",
            "305:\tlearn: 0.9552535\ttotal: 1m 41s\tremaining: 3m 49s\n",
            "306:\tlearn: 0.9549532\ttotal: 1m 41s\tremaining: 3m 49s\n",
            "307:\tlearn: 0.9547645\ttotal: 1m 41s\tremaining: 3m 48s\n",
            "308:\tlearn: 0.9543310\ttotal: 1m 42s\tremaining: 3m 48s\n",
            "309:\tlearn: 0.9540574\ttotal: 1m 42s\tremaining: 3m 48s\n",
            "310:\tlearn: 0.9538280\ttotal: 1m 42s\tremaining: 3m 48s\n",
            "311:\tlearn: 0.9535257\ttotal: 1m 43s\tremaining: 3m 47s\n",
            "312:\tlearn: 0.9532670\ttotal: 1m 43s\tremaining: 3m 47s\n",
            "313:\tlearn: 0.9529971\ttotal: 1m 43s\tremaining: 3m 47s\n",
            "314:\tlearn: 0.9526809\ttotal: 1m 44s\tremaining: 3m 46s\n",
            "315:\tlearn: 0.9523481\ttotal: 1m 44s\tremaining: 3m 46s\n",
            "316:\tlearn: 0.9521096\ttotal: 1m 44s\tremaining: 3m 46s\n",
            "317:\tlearn: 0.9518136\ttotal: 1m 45s\tremaining: 3m 45s\n",
            "318:\tlearn: 0.9515631\ttotal: 1m 45s\tremaining: 3m 45s\n",
            "319:\tlearn: 0.9513410\ttotal: 1m 45s\tremaining: 3m 45s\n",
            "320:\tlearn: 0.9510648\ttotal: 1m 46s\tremaining: 3m 44s\n",
            "321:\tlearn: 0.9509082\ttotal: 1m 46s\tremaining: 3m 44s\n",
            "322:\tlearn: 0.9507147\ttotal: 1m 46s\tremaining: 3m 44s\n",
            "323:\tlearn: 0.9503998\ttotal: 1m 47s\tremaining: 3m 43s\n",
            "324:\tlearn: 0.9501098\ttotal: 1m 47s\tremaining: 3m 43s\n",
            "325:\tlearn: 0.9498286\ttotal: 1m 47s\tremaining: 3m 43s\n",
            "326:\tlearn: 0.9495164\ttotal: 1m 48s\tremaining: 3m 42s\n",
            "327:\tlearn: 0.9492486\ttotal: 1m 48s\tremaining: 3m 42s\n",
            "328:\tlearn: 0.9489760\ttotal: 1m 48s\tremaining: 3m 42s\n",
            "329:\tlearn: 0.9487354\ttotal: 1m 49s\tremaining: 3m 41s\n",
            "330:\tlearn: 0.9483634\ttotal: 1m 49s\tremaining: 3m 41s\n",
            "331:\tlearn: 0.9480925\ttotal: 1m 49s\tremaining: 3m 41s\n",
            "332:\tlearn: 0.9478430\ttotal: 1m 50s\tremaining: 3m 40s\n",
            "333:\tlearn: 0.9476372\ttotal: 1m 50s\tremaining: 3m 40s\n",
            "334:\tlearn: 0.9474183\ttotal: 1m 50s\tremaining: 3m 40s\n",
            "335:\tlearn: 0.9472824\ttotal: 1m 51s\tremaining: 3m 39s\n",
            "336:\tlearn: 0.9469780\ttotal: 1m 51s\tremaining: 3m 39s\n",
            "337:\tlearn: 0.9467363\ttotal: 1m 51s\tremaining: 3m 39s\n",
            "338:\tlearn: 0.9464428\ttotal: 1m 52s\tremaining: 3m 38s\n",
            "339:\tlearn: 0.9462868\ttotal: 1m 52s\tremaining: 3m 38s\n",
            "340:\tlearn: 0.9459991\ttotal: 1m 52s\tremaining: 3m 37s\n",
            "341:\tlearn: 0.9457666\ttotal: 1m 53s\tremaining: 3m 37s\n",
            "342:\tlearn: 0.9455119\ttotal: 1m 53s\tremaining: 3m 37s\n",
            "343:\tlearn: 0.9453074\ttotal: 1m 53s\tremaining: 3m 37s\n",
            "344:\tlearn: 0.9449921\ttotal: 1m 54s\tremaining: 3m 36s\n",
            "345:\tlearn: 0.9447212\ttotal: 1m 54s\tremaining: 3m 36s\n",
            "346:\tlearn: 0.9443799\ttotal: 1m 54s\tremaining: 3m 35s\n",
            "347:\tlearn: 0.9441487\ttotal: 1m 55s\tremaining: 3m 35s\n",
            "348:\tlearn: 0.9439375\ttotal: 1m 55s\tremaining: 3m 35s\n",
            "349:\tlearn: 0.9436424\ttotal: 1m 55s\tremaining: 3m 34s\n",
            "350:\tlearn: 0.9433015\ttotal: 1m 56s\tremaining: 3m 34s\n",
            "351:\tlearn: 0.9430360\ttotal: 1m 56s\tremaining: 3m 34s\n",
            "352:\tlearn: 0.9428506\ttotal: 1m 56s\tremaining: 3m 33s\n",
            "353:\tlearn: 0.9425440\ttotal: 1m 56s\tremaining: 3m 33s\n",
            "354:\tlearn: 0.9422768\ttotal: 1m 57s\tremaining: 3m 33s\n",
            "355:\tlearn: 0.9420259\ttotal: 1m 57s\tremaining: 3m 32s\n",
            "356:\tlearn: 0.9416920\ttotal: 1m 58s\tremaining: 3m 32s\n",
            "357:\tlearn: 0.9415302\ttotal: 1m 58s\tremaining: 3m 32s\n",
            "358:\tlearn: 0.9412448\ttotal: 1m 58s\tremaining: 3m 32s\n",
            "359:\tlearn: 0.9410588\ttotal: 1m 59s\tremaining: 3m 31s\n",
            "360:\tlearn: 0.9407588\ttotal: 1m 59s\tremaining: 3m 31s\n",
            "361:\tlearn: 0.9404428\ttotal: 1m 59s\tremaining: 3m 31s\n",
            "362:\tlearn: 0.9402310\ttotal: 2m\tremaining: 3m 30s\n",
            "363:\tlearn: 0.9399589\ttotal: 2m\tremaining: 3m 30s\n",
            "364:\tlearn: 0.9397639\ttotal: 2m\tremaining: 3m 29s\n",
            "365:\tlearn: 0.9395290\ttotal: 2m\tremaining: 3m 29s\n",
            "366:\tlearn: 0.9392860\ttotal: 2m 1s\tremaining: 3m 29s\n",
            "367:\tlearn: 0.9390701\ttotal: 2m 1s\tremaining: 3m 28s\n",
            "368:\tlearn: 0.9388211\ttotal: 2m 2s\tremaining: 3m 28s\n",
            "369:\tlearn: 0.9385354\ttotal: 2m 2s\tremaining: 3m 28s\n",
            "370:\tlearn: 0.9383152\ttotal: 2m 2s\tremaining: 3m 27s\n",
            "371:\tlearn: 0.9380239\ttotal: 2m 2s\tremaining: 3m 27s\n",
            "372:\tlearn: 0.9377970\ttotal: 2m 3s\tremaining: 3m 27s\n",
            "373:\tlearn: 0.9376067\ttotal: 2m 3s\tremaining: 3m 26s\n",
            "374:\tlearn: 0.9373826\ttotal: 2m 3s\tremaining: 3m 26s\n",
            "375:\tlearn: 0.9372501\ttotal: 2m 4s\tremaining: 3m 26s\n",
            "376:\tlearn: 0.9369909\ttotal: 2m 4s\tremaining: 3m 25s\n",
            "377:\tlearn: 0.9368283\ttotal: 2m 4s\tremaining: 3m 25s\n",
            "378:\tlearn: 0.9365397\ttotal: 2m 5s\tremaining: 3m 25s\n",
            "379:\tlearn: 0.9363541\ttotal: 2m 5s\tremaining: 3m 24s\n",
            "380:\tlearn: 0.9361447\ttotal: 2m 5s\tremaining: 3m 24s\n",
            "381:\tlearn: 0.9358860\ttotal: 2m 6s\tremaining: 3m 24s\n",
            "382:\tlearn: 0.9356739\ttotal: 2m 6s\tremaining: 3m 23s\n",
            "383:\tlearn: 0.9355035\ttotal: 2m 6s\tremaining: 3m 23s\n",
            "384:\tlearn: 0.9352676\ttotal: 2m 7s\tremaining: 3m 23s\n",
            "385:\tlearn: 0.9350235\ttotal: 2m 7s\tremaining: 3m 22s\n",
            "386:\tlearn: 0.9347435\ttotal: 2m 7s\tremaining: 3m 22s\n",
            "387:\tlearn: 0.9345092\ttotal: 2m 8s\tremaining: 3m 22s\n",
            "388:\tlearn: 0.9343025\ttotal: 2m 8s\tremaining: 3m 21s\n",
            "389:\tlearn: 0.9340875\ttotal: 2m 8s\tremaining: 3m 21s\n",
            "390:\tlearn: 0.9338298\ttotal: 2m 9s\tremaining: 3m 21s\n",
            "391:\tlearn: 0.9336162\ttotal: 2m 9s\tremaining: 3m 21s\n",
            "392:\tlearn: 0.9334358\ttotal: 2m 9s\tremaining: 3m 20s\n",
            "393:\tlearn: 0.9331861\ttotal: 2m 10s\tremaining: 3m 20s\n",
            "394:\tlearn: 0.9328159\ttotal: 2m 10s\tremaining: 3m 19s\n",
            "395:\tlearn: 0.9325943\ttotal: 2m 10s\tremaining: 3m 19s\n",
            "396:\tlearn: 0.9323402\ttotal: 2m 11s\tremaining: 3m 19s\n",
            "397:\tlearn: 0.9321284\ttotal: 2m 11s\tremaining: 3m 18s\n",
            "398:\tlearn: 0.9317945\ttotal: 2m 11s\tremaining: 3m 18s\n",
            "399:\tlearn: 0.9315142\ttotal: 2m 12s\tremaining: 3m 18s\n",
            "400:\tlearn: 0.9312556\ttotal: 2m 12s\tremaining: 3m 17s\n",
            "401:\tlearn: 0.9309939\ttotal: 2m 12s\tremaining: 3m 17s\n",
            "402:\tlearn: 0.9307463\ttotal: 2m 13s\tremaining: 3m 17s\n",
            "403:\tlearn: 0.9305315\ttotal: 2m 13s\tremaining: 3m 16s\n",
            "404:\tlearn: 0.9302950\ttotal: 2m 13s\tremaining: 3m 16s\n",
            "405:\tlearn: 0.9300746\ttotal: 2m 13s\tremaining: 3m 16s\n",
            "406:\tlearn: 0.9297440\ttotal: 2m 14s\tremaining: 3m 15s\n",
            "407:\tlearn: 0.9295015\ttotal: 2m 14s\tremaining: 3m 15s\n",
            "408:\tlearn: 0.9292253\ttotal: 2m 15s\tremaining: 3m 15s\n",
            "409:\tlearn: 0.9290356\ttotal: 2m 15s\tremaining: 3m 14s\n",
            "410:\tlearn: 0.9288337\ttotal: 2m 15s\tremaining: 3m 14s\n",
            "411:\tlearn: 0.9285900\ttotal: 2m 16s\tremaining: 3m 14s\n",
            "412:\tlearn: 0.9284086\ttotal: 2m 16s\tremaining: 3m 13s\n",
            "413:\tlearn: 0.9282554\ttotal: 2m 16s\tremaining: 3m 13s\n",
            "414:\tlearn: 0.9279761\ttotal: 2m 17s\tremaining: 3m 13s\n",
            "415:\tlearn: 0.9277506\ttotal: 2m 17s\tremaining: 3m 12s\n",
            "416:\tlearn: 0.9275179\ttotal: 2m 17s\tremaining: 3m 12s\n",
            "417:\tlearn: 0.9272934\ttotal: 2m 18s\tremaining: 3m 12s\n",
            "418:\tlearn: 0.9270477\ttotal: 2m 18s\tremaining: 3m 11s\n",
            "419:\tlearn: 0.9268581\ttotal: 2m 18s\tremaining: 3m 11s\n",
            "420:\tlearn: 0.9266430\ttotal: 2m 18s\tremaining: 3m 11s\n",
            "421:\tlearn: 0.9264323\ttotal: 2m 19s\tremaining: 3m 10s\n",
            "422:\tlearn: 0.9260854\ttotal: 2m 19s\tremaining: 3m 10s\n",
            "423:\tlearn: 0.9258659\ttotal: 2m 19s\tremaining: 3m 10s\n",
            "424:\tlearn: 0.9255914\ttotal: 2m 20s\tremaining: 3m 9s\n",
            "425:\tlearn: 0.9254623\ttotal: 2m 20s\tremaining: 3m 9s\n",
            "426:\tlearn: 0.9250768\ttotal: 2m 20s\tremaining: 3m 9s\n",
            "427:\tlearn: 0.9247062\ttotal: 2m 21s\tremaining: 3m 8s\n",
            "428:\tlearn: 0.9244874\ttotal: 2m 21s\tremaining: 3m 8s\n",
            "429:\tlearn: 0.9243840\ttotal: 2m 21s\tremaining: 3m 8s\n",
            "430:\tlearn: 0.9241307\ttotal: 2m 22s\tremaining: 3m 7s\n",
            "431:\tlearn: 0.9237911\ttotal: 2m 22s\tremaining: 3m 7s\n",
            "432:\tlearn: 0.9235834\ttotal: 2m 22s\tremaining: 3m 7s\n",
            "433:\tlearn: 0.9233600\ttotal: 2m 23s\tremaining: 3m 6s\n",
            "434:\tlearn: 0.9230615\ttotal: 2m 23s\tremaining: 3m 6s\n",
            "435:\tlearn: 0.9229575\ttotal: 2m 23s\tremaining: 3m 6s\n",
            "436:\tlearn: 0.9227065\ttotal: 2m 24s\tremaining: 3m 5s\n",
            "437:\tlearn: 0.9223818\ttotal: 2m 24s\tremaining: 3m 5s\n",
            "438:\tlearn: 0.9221395\ttotal: 2m 24s\tremaining: 3m 5s\n",
            "439:\tlearn: 0.9218987\ttotal: 2m 25s\tremaining: 3m 4s\n",
            "440:\tlearn: 0.9216274\ttotal: 2m 25s\tremaining: 3m 4s\n",
            "441:\tlearn: 0.9214263\ttotal: 2m 25s\tremaining: 3m 4s\n",
            "442:\tlearn: 0.9212187\ttotal: 2m 26s\tremaining: 3m 3s\n",
            "443:\tlearn: 0.9210671\ttotal: 2m 26s\tremaining: 3m 3s\n",
            "444:\tlearn: 0.9208875\ttotal: 2m 26s\tremaining: 3m 3s\n",
            "445:\tlearn: 0.9206278\ttotal: 2m 27s\tremaining: 3m 2s\n",
            "446:\tlearn: 0.9203788\ttotal: 2m 27s\tremaining: 3m 2s\n",
            "447:\tlearn: 0.9202167\ttotal: 2m 27s\tremaining: 3m 2s\n",
            "448:\tlearn: 0.9199017\ttotal: 2m 28s\tremaining: 3m 1s\n",
            "449:\tlearn: 0.9197371\ttotal: 2m 28s\tremaining: 3m 1s\n",
            "450:\tlearn: 0.9195207\ttotal: 2m 28s\tremaining: 3m 1s\n",
            "451:\tlearn: 0.9193526\ttotal: 2m 29s\tremaining: 3m\n",
            "452:\tlearn: 0.9191665\ttotal: 2m 29s\tremaining: 3m\n",
            "453:\tlearn: 0.9189002\ttotal: 2m 29s\tremaining: 3m\n",
            "454:\tlearn: 0.9186936\ttotal: 2m 30s\tremaining: 2m 59s\n",
            "455:\tlearn: 0.9184756\ttotal: 2m 30s\tremaining: 2m 59s\n",
            "456:\tlearn: 0.9182627\ttotal: 2m 30s\tremaining: 2m 59s\n",
            "457:\tlearn: 0.9180834\ttotal: 2m 31s\tremaining: 2m 58s\n",
            "458:\tlearn: 0.9178932\ttotal: 2m 31s\tremaining: 2m 58s\n",
            "459:\tlearn: 0.9176971\ttotal: 2m 31s\tremaining: 2m 58s\n",
            "460:\tlearn: 0.9174976\ttotal: 2m 32s\tremaining: 2m 57s\n",
            "461:\tlearn: 0.9172732\ttotal: 2m 32s\tremaining: 2m 57s\n",
            "462:\tlearn: 0.9170620\ttotal: 2m 32s\tremaining: 2m 56s\n",
            "463:\tlearn: 0.9168223\ttotal: 2m 32s\tremaining: 2m 56s\n",
            "464:\tlearn: 0.9165868\ttotal: 2m 33s\tremaining: 2m 56s\n",
            "465:\tlearn: 0.9163943\ttotal: 2m 33s\tremaining: 2m 55s\n",
            "466:\tlearn: 0.9162018\ttotal: 2m 33s\tremaining: 2m 55s\n",
            "467:\tlearn: 0.9159358\ttotal: 2m 34s\tremaining: 2m 55s\n",
            "468:\tlearn: 0.9157426\ttotal: 2m 34s\tremaining: 2m 55s\n",
            "469:\tlearn: 0.9155060\ttotal: 2m 34s\tremaining: 2m 54s\n",
            "470:\tlearn: 0.9152701\ttotal: 2m 35s\tremaining: 2m 54s\n",
            "471:\tlearn: 0.9150928\ttotal: 2m 35s\tremaining: 2m 54s\n",
            "472:\tlearn: 0.9148507\ttotal: 2m 35s\tremaining: 2m 53s\n",
            "473:\tlearn: 0.9145622\ttotal: 2m 36s\tremaining: 2m 53s\n",
            "474:\tlearn: 0.9142636\ttotal: 2m 36s\tremaining: 2m 53s\n",
            "475:\tlearn: 0.9138535\ttotal: 2m 36s\tremaining: 2m 52s\n",
            "476:\tlearn: 0.9136127\ttotal: 2m 37s\tremaining: 2m 52s\n",
            "477:\tlearn: 0.9133583\ttotal: 2m 37s\tremaining: 2m 52s\n",
            "478:\tlearn: 0.9131159\ttotal: 2m 37s\tremaining: 2m 51s\n",
            "479:\tlearn: 0.9129113\ttotal: 2m 38s\tremaining: 2m 51s\n",
            "480:\tlearn: 0.9126090\ttotal: 2m 38s\tremaining: 2m 51s\n",
            "481:\tlearn: 0.9124417\ttotal: 2m 39s\tremaining: 2m 50s\n",
            "482:\tlearn: 0.9122221\ttotal: 2m 39s\tremaining: 2m 50s\n",
            "483:\tlearn: 0.9119907\ttotal: 2m 39s\tremaining: 2m 50s\n",
            "484:\tlearn: 0.9117878\ttotal: 2m 40s\tremaining: 2m 49s\n",
            "485:\tlearn: 0.9114689\ttotal: 2m 40s\tremaining: 2m 49s\n",
            "486:\tlearn: 0.9112279\ttotal: 2m 40s\tremaining: 2m 49s\n",
            "487:\tlearn: 0.9109166\ttotal: 2m 41s\tremaining: 2m 49s\n",
            "488:\tlearn: 0.9107393\ttotal: 2m 41s\tremaining: 2m 48s\n",
            "489:\tlearn: 0.9104484\ttotal: 2m 41s\tremaining: 2m 48s\n",
            "490:\tlearn: 0.9101909\ttotal: 2m 42s\tremaining: 2m 48s\n",
            "491:\tlearn: 0.9099677\ttotal: 2m 42s\tremaining: 2m 47s\n",
            "492:\tlearn: 0.9097903\ttotal: 2m 42s\tremaining: 2m 47s\n",
            "493:\tlearn: 0.9096262\ttotal: 2m 43s\tremaining: 2m 47s\n",
            "494:\tlearn: 0.9093233\ttotal: 2m 43s\tremaining: 2m 46s\n",
            "495:\tlearn: 0.9090668\ttotal: 2m 43s\tremaining: 2m 46s\n",
            "496:\tlearn: 0.9088425\ttotal: 2m 44s\tremaining: 2m 46s\n",
            "497:\tlearn: 0.9086030\ttotal: 2m 44s\tremaining: 2m 45s\n",
            "498:\tlearn: 0.9083085\ttotal: 2m 44s\tremaining: 2m 45s\n",
            "499:\tlearn: 0.9079876\ttotal: 2m 45s\tremaining: 2m 45s\n",
            "500:\tlearn: 0.9077322\ttotal: 2m 45s\tremaining: 2m 44s\n",
            "501:\tlearn: 0.9074725\ttotal: 2m 45s\tremaining: 2m 44s\n",
            "502:\tlearn: 0.9072630\ttotal: 2m 46s\tremaining: 2m 44s\n",
            "503:\tlearn: 0.9070681\ttotal: 2m 46s\tremaining: 2m 43s\n",
            "504:\tlearn: 0.9068354\ttotal: 2m 46s\tremaining: 2m 43s\n",
            "505:\tlearn: 0.9064967\ttotal: 2m 46s\tremaining: 2m 42s\n",
            "506:\tlearn: 0.9062534\ttotal: 2m 47s\tremaining: 2m 42s\n",
            "507:\tlearn: 0.9059343\ttotal: 2m 47s\tremaining: 2m 42s\n",
            "508:\tlearn: 0.9057050\ttotal: 2m 47s\tremaining: 2m 42s\n",
            "509:\tlearn: 0.9054038\ttotal: 2m 48s\tremaining: 2m 41s\n",
            "510:\tlearn: 0.9051679\ttotal: 2m 48s\tremaining: 2m 41s\n",
            "511:\tlearn: 0.9049022\ttotal: 2m 48s\tremaining: 2m 41s\n",
            "512:\tlearn: 0.9046675\ttotal: 2m 49s\tremaining: 2m 40s\n",
            "513:\tlearn: 0.9043889\ttotal: 2m 49s\tremaining: 2m 40s\n",
            "514:\tlearn: 0.9042420\ttotal: 2m 50s\tremaining: 2m 40s\n",
            "515:\tlearn: 0.9039153\ttotal: 2m 50s\tremaining: 2m 39s\n",
            "516:\tlearn: 0.9037198\ttotal: 2m 50s\tremaining: 2m 39s\n",
            "517:\tlearn: 0.9034546\ttotal: 2m 51s\tremaining: 2m 39s\n",
            "518:\tlearn: 0.9032728\ttotal: 2m 51s\tremaining: 2m 38s\n",
            "519:\tlearn: 0.9030965\ttotal: 2m 51s\tremaining: 2m 38s\n",
            "520:\tlearn: 0.9028689\ttotal: 2m 52s\tremaining: 2m 38s\n",
            "521:\tlearn: 0.9027372\ttotal: 2m 52s\tremaining: 2m 37s\n",
            "522:\tlearn: 0.9025511\ttotal: 2m 52s\tremaining: 2m 37s\n",
            "523:\tlearn: 0.9022945\ttotal: 2m 53s\tremaining: 2m 37s\n",
            "524:\tlearn: 0.9020855\ttotal: 2m 53s\tremaining: 2m 36s\n",
            "525:\tlearn: 0.9018315\ttotal: 2m 53s\tremaining: 2m 36s\n",
            "526:\tlearn: 0.9015860\ttotal: 2m 54s\tremaining: 2m 36s\n",
            "527:\tlearn: 0.9013349\ttotal: 2m 54s\tremaining: 2m 35s\n",
            "528:\tlearn: 0.9010808\ttotal: 2m 54s\tremaining: 2m 35s\n",
            "529:\tlearn: 0.9009261\ttotal: 2m 55s\tremaining: 2m 35s\n",
            "530:\tlearn: 0.9007347\ttotal: 2m 55s\tremaining: 2m 34s\n",
            "531:\tlearn: 0.9005402\ttotal: 2m 55s\tremaining: 2m 34s\n",
            "532:\tlearn: 0.9002616\ttotal: 2m 55s\tremaining: 2m 34s\n",
            "533:\tlearn: 0.9000761\ttotal: 2m 56s\tremaining: 2m 33s\n",
            "534:\tlearn: 0.8998298\ttotal: 2m 56s\tremaining: 2m 33s\n",
            "535:\tlearn: 0.8995362\ttotal: 2m 57s\tremaining: 2m 33s\n",
            "536:\tlearn: 0.8993127\ttotal: 2m 57s\tremaining: 2m 32s\n",
            "537:\tlearn: 0.8990150\ttotal: 2m 57s\tremaining: 2m 32s\n",
            "538:\tlearn: 0.8987476\ttotal: 2m 57s\tremaining: 2m 32s\n",
            "539:\tlearn: 0.8985170\ttotal: 2m 58s\tremaining: 2m 31s\n",
            "540:\tlearn: 0.8983038\ttotal: 2m 58s\tremaining: 2m 31s\n",
            "541:\tlearn: 0.8980745\ttotal: 2m 58s\tremaining: 2m 31s\n",
            "542:\tlearn: 0.8978547\ttotal: 2m 59s\tremaining: 2m 30s\n",
            "543:\tlearn: 0.8976674\ttotal: 2m 59s\tremaining: 2m 30s\n",
            "544:\tlearn: 0.8973339\ttotal: 3m\tremaining: 2m 30s\n",
            "545:\tlearn: 0.8972090\ttotal: 3m\tremaining: 2m 29s\n",
            "546:\tlearn: 0.8969138\ttotal: 3m\tremaining: 2m 29s\n",
            "547:\tlearn: 0.8966943\ttotal: 3m\tremaining: 2m 29s\n",
            "548:\tlearn: 0.8964108\ttotal: 3m 1s\tremaining: 2m 28s\n",
            "549:\tlearn: 0.8962466\ttotal: 3m 1s\tremaining: 2m 28s\n",
            "550:\tlearn: 0.8960184\ttotal: 3m 1s\tremaining: 2m 28s\n",
            "551:\tlearn: 0.8957679\ttotal: 3m 2s\tremaining: 2m 27s\n",
            "552:\tlearn: 0.8954500\ttotal: 3m 2s\tremaining: 2m 27s\n",
            "553:\tlearn: 0.8951701\ttotal: 3m 3s\tremaining: 2m 27s\n",
            "554:\tlearn: 0.8950031\ttotal: 3m 3s\tremaining: 2m 27s\n",
            "555:\tlearn: 0.8947894\ttotal: 3m 3s\tremaining: 2m 26s\n",
            "556:\tlearn: 0.8945220\ttotal: 3m 4s\tremaining: 2m 26s\n",
            "557:\tlearn: 0.8942498\ttotal: 3m 4s\tremaining: 2m 26s\n",
            "558:\tlearn: 0.8940379\ttotal: 3m 4s\tremaining: 2m 25s\n",
            "559:\tlearn: 0.8937192\ttotal: 3m 5s\tremaining: 2m 25s\n",
            "560:\tlearn: 0.8933809\ttotal: 3m 5s\tremaining: 2m 25s\n",
            "561:\tlearn: 0.8931851\ttotal: 3m 5s\tremaining: 2m 24s\n",
            "562:\tlearn: 0.8929106\ttotal: 3m 6s\tremaining: 2m 24s\n",
            "563:\tlearn: 0.8926575\ttotal: 3m 6s\tremaining: 2m 24s\n",
            "564:\tlearn: 0.8924033\ttotal: 3m 6s\tremaining: 2m 23s\n",
            "565:\tlearn: 0.8921407\ttotal: 3m 7s\tremaining: 2m 23s\n",
            "566:\tlearn: 0.8919475\ttotal: 3m 7s\tremaining: 2m 23s\n",
            "567:\tlearn: 0.8915967\ttotal: 3m 7s\tremaining: 2m 22s\n",
            "568:\tlearn: 0.8912416\ttotal: 3m 8s\tremaining: 2m 22s\n",
            "569:\tlearn: 0.8911448\ttotal: 3m 8s\tremaining: 2m 22s\n",
            "570:\tlearn: 0.8910091\ttotal: 3m 8s\tremaining: 2m 21s\n",
            "571:\tlearn: 0.8907653\ttotal: 3m 9s\tremaining: 2m 21s\n",
            "572:\tlearn: 0.8906630\ttotal: 3m 9s\tremaining: 2m 21s\n",
            "573:\tlearn: 0.8903579\ttotal: 3m 9s\tremaining: 2m 20s\n",
            "574:\tlearn: 0.8902358\ttotal: 3m 9s\tremaining: 2m 20s\n",
            "575:\tlearn: 0.8901279\ttotal: 3m 10s\tremaining: 2m 20s\n",
            "576:\tlearn: 0.8899311\ttotal: 3m 10s\tremaining: 2m 19s\n",
            "577:\tlearn: 0.8897066\ttotal: 3m 11s\tremaining: 2m 19s\n",
            "578:\tlearn: 0.8895072\ttotal: 3m 11s\tremaining: 2m 19s\n",
            "579:\tlearn: 0.8893051\ttotal: 3m 11s\tremaining: 2m 18s\n",
            "580:\tlearn: 0.8890197\ttotal: 3m 11s\tremaining: 2m 18s\n",
            "581:\tlearn: 0.8888850\ttotal: 3m 12s\tremaining: 2m 18s\n",
            "582:\tlearn: 0.8886842\ttotal: 3m 12s\tremaining: 2m 17s\n",
            "583:\tlearn: 0.8884145\ttotal: 3m 13s\tremaining: 2m 17s\n",
            "584:\tlearn: 0.8881970\ttotal: 3m 13s\tremaining: 2m 17s\n",
            "585:\tlearn: 0.8880102\ttotal: 3m 13s\tremaining: 2m 16s\n",
            "586:\tlearn: 0.8878077\ttotal: 3m 13s\tremaining: 2m 16s\n",
            "587:\tlearn: 0.8875619\ttotal: 3m 14s\tremaining: 2m 16s\n",
            "588:\tlearn: 0.8873356\ttotal: 3m 14s\tremaining: 2m 15s\n",
            "589:\tlearn: 0.8870274\ttotal: 3m 15s\tremaining: 2m 15s\n",
            "590:\tlearn: 0.8867041\ttotal: 3m 15s\tremaining: 2m 15s\n",
            "591:\tlearn: 0.8865382\ttotal: 3m 15s\tremaining: 2m 14s\n",
            "592:\tlearn: 0.8863185\ttotal: 3m 15s\tremaining: 2m 14s\n",
            "593:\tlearn: 0.8860819\ttotal: 3m 16s\tremaining: 2m 14s\n",
            "594:\tlearn: 0.8857193\ttotal: 3m 16s\tremaining: 2m 13s\n",
            "595:\tlearn: 0.8855426\ttotal: 3m 17s\tremaining: 2m 13s\n",
            "596:\tlearn: 0.8853025\ttotal: 3m 17s\tremaining: 2m 13s\n",
            "597:\tlearn: 0.8851111\ttotal: 3m 17s\tremaining: 2m 12s\n",
            "598:\tlearn: 0.8849312\ttotal: 3m 17s\tremaining: 2m 12s\n",
            "599:\tlearn: 0.8847204\ttotal: 3m 18s\tremaining: 2m 12s\n",
            "600:\tlearn: 0.8844416\ttotal: 3m 18s\tremaining: 2m 11s\n",
            "601:\tlearn: 0.8842191\ttotal: 3m 18s\tremaining: 2m 11s\n",
            "602:\tlearn: 0.8840367\ttotal: 3m 19s\tremaining: 2m 11s\n",
            "603:\tlearn: 0.8837970\ttotal: 3m 19s\tremaining: 2m 10s\n",
            "604:\tlearn: 0.8835664\ttotal: 3m 19s\tremaining: 2m 10s\n",
            "605:\tlearn: 0.8833616\ttotal: 3m 20s\tremaining: 2m 10s\n",
            "606:\tlearn: 0.8831207\ttotal: 3m 20s\tremaining: 2m 9s\n",
            "607:\tlearn: 0.8829170\ttotal: 3m 21s\tremaining: 2m 9s\n",
            "608:\tlearn: 0.8826771\ttotal: 3m 21s\tremaining: 2m 9s\n",
            "609:\tlearn: 0.8825454\ttotal: 3m 21s\tremaining: 2m 8s\n",
            "610:\tlearn: 0.8822507\ttotal: 3m 22s\tremaining: 2m 8s\n",
            "611:\tlearn: 0.8821072\ttotal: 3m 22s\tremaining: 2m 8s\n",
            "612:\tlearn: 0.8818145\ttotal: 3m 22s\tremaining: 2m 7s\n",
            "613:\tlearn: 0.8815865\ttotal: 3m 23s\tremaining: 2m 7s\n",
            "614:\tlearn: 0.8812828\ttotal: 3m 23s\tremaining: 2m 7s\n",
            "615:\tlearn: 0.8810958\ttotal: 3m 23s\tremaining: 2m 7s\n",
            "616:\tlearn: 0.8808760\ttotal: 3m 24s\tremaining: 2m 6s\n",
            "617:\tlearn: 0.8806933\ttotal: 3m 24s\tremaining: 2m 6s\n",
            "618:\tlearn: 0.8804311\ttotal: 3m 24s\tremaining: 2m 6s\n",
            "619:\tlearn: 0.8802742\ttotal: 3m 25s\tremaining: 2m 5s\n",
            "620:\tlearn: 0.8800371\ttotal: 3m 25s\tremaining: 2m 5s\n",
            "621:\tlearn: 0.8798614\ttotal: 3m 25s\tremaining: 2m 5s\n",
            "622:\tlearn: 0.8796505\ttotal: 3m 26s\tremaining: 2m 4s\n",
            "623:\tlearn: 0.8794676\ttotal: 3m 26s\tremaining: 2m 4s\n",
            "624:\tlearn: 0.8793113\ttotal: 3m 26s\tremaining: 2m 4s\n",
            "625:\tlearn: 0.8791311\ttotal: 3m 27s\tremaining: 2m 3s\n",
            "626:\tlearn: 0.8789028\ttotal: 3m 27s\tremaining: 2m 3s\n",
            "627:\tlearn: 0.8787048\ttotal: 3m 27s\tremaining: 2m 3s\n",
            "628:\tlearn: 0.8784775\ttotal: 3m 28s\tremaining: 2m 2s\n",
            "629:\tlearn: 0.8782340\ttotal: 3m 28s\tremaining: 2m 2s\n",
            "630:\tlearn: 0.8779505\ttotal: 3m 29s\tremaining: 2m 2s\n",
            "631:\tlearn: 0.8776880\ttotal: 3m 29s\tremaining: 2m 1s\n",
            "632:\tlearn: 0.8775283\ttotal: 3m 29s\tremaining: 2m 1s\n",
            "633:\tlearn: 0.8773406\ttotal: 3m 29s\tremaining: 2m 1s\n",
            "634:\tlearn: 0.8770687\ttotal: 3m 30s\tremaining: 2m\n",
            "635:\tlearn: 0.8767425\ttotal: 3m 30s\tremaining: 2m\n",
            "636:\tlearn: 0.8764708\ttotal: 3m 31s\tremaining: 2m\n",
            "637:\tlearn: 0.8761749\ttotal: 3m 31s\tremaining: 1m 59s\n",
            "638:\tlearn: 0.8758863\ttotal: 3m 31s\tremaining: 1m 59s\n",
            "639:\tlearn: 0.8757475\ttotal: 3m 32s\tremaining: 1m 59s\n",
            "640:\tlearn: 0.8755006\ttotal: 3m 32s\tremaining: 1m 58s\n",
            "641:\tlearn: 0.8752705\ttotal: 3m 32s\tremaining: 1m 58s\n",
            "642:\tlearn: 0.8750372\ttotal: 3m 33s\tremaining: 1m 58s\n",
            "643:\tlearn: 0.8748206\ttotal: 3m 33s\tremaining: 1m 57s\n",
            "644:\tlearn: 0.8745897\ttotal: 3m 33s\tremaining: 1m 57s\n",
            "645:\tlearn: 0.8743914\ttotal: 3m 34s\tremaining: 1m 57s\n",
            "646:\tlearn: 0.8741659\ttotal: 3m 34s\tremaining: 1m 56s\n",
            "647:\tlearn: 0.8740353\ttotal: 3m 34s\tremaining: 1m 56s\n",
            "648:\tlearn: 0.8739182\ttotal: 3m 34s\tremaining: 1m 56s\n",
            "649:\tlearn: 0.8737167\ttotal: 3m 35s\tremaining: 1m 55s\n",
            "650:\tlearn: 0.8734289\ttotal: 3m 35s\tremaining: 1m 55s\n",
            "651:\tlearn: 0.8730804\ttotal: 3m 36s\tremaining: 1m 55s\n",
            "652:\tlearn: 0.8728714\ttotal: 3m 36s\tremaining: 1m 54s\n",
            "653:\tlearn: 0.8726819\ttotal: 3m 36s\tremaining: 1m 54s\n",
            "654:\tlearn: 0.8724859\ttotal: 3m 37s\tremaining: 1m 54s\n",
            "655:\tlearn: 0.8722835\ttotal: 3m 37s\tremaining: 1m 54s\n",
            "656:\tlearn: 0.8721262\ttotal: 3m 37s\tremaining: 1m 53s\n",
            "657:\tlearn: 0.8719714\ttotal: 3m 38s\tremaining: 1m 53s\n",
            "658:\tlearn: 0.8717339\ttotal: 3m 38s\tremaining: 1m 52s\n",
            "659:\tlearn: 0.8715427\ttotal: 3m 38s\tremaining: 1m 52s\n",
            "660:\tlearn: 0.8713067\ttotal: 3m 38s\tremaining: 1m 52s\n",
            "661:\tlearn: 0.8710266\ttotal: 3m 39s\tremaining: 1m 51s\n",
            "662:\tlearn: 0.8707798\ttotal: 3m 39s\tremaining: 1m 51s\n",
            "663:\tlearn: 0.8705755\ttotal: 3m 39s\tremaining: 1m 51s\n",
            "664:\tlearn: 0.8703896\ttotal: 3m 40s\tremaining: 1m 50s\n",
            "665:\tlearn: 0.8701473\ttotal: 3m 40s\tremaining: 1m 50s\n",
            "666:\tlearn: 0.8699267\ttotal: 3m 40s\tremaining: 1m 50s\n",
            "667:\tlearn: 0.8695600\ttotal: 3m 41s\tremaining: 1m 50s\n",
            "668:\tlearn: 0.8693860\ttotal: 3m 41s\tremaining: 1m 49s\n",
            "669:\tlearn: 0.8692181\ttotal: 3m 42s\tremaining: 1m 49s\n",
            "670:\tlearn: 0.8689465\ttotal: 3m 42s\tremaining: 1m 49s\n",
            "671:\tlearn: 0.8687957\ttotal: 3m 42s\tremaining: 1m 48s\n",
            "672:\tlearn: 0.8685211\ttotal: 3m 43s\tremaining: 1m 48s\n",
            "673:\tlearn: 0.8682989\ttotal: 3m 43s\tremaining: 1m 48s\n",
            "674:\tlearn: 0.8681243\ttotal: 3m 43s\tremaining: 1m 47s\n",
            "675:\tlearn: 0.8679521\ttotal: 3m 43s\tremaining: 1m 47s\n",
            "676:\tlearn: 0.8677470\ttotal: 3m 44s\tremaining: 1m 47s\n",
            "677:\tlearn: 0.8675209\ttotal: 3m 44s\tremaining: 1m 46s\n",
            "678:\tlearn: 0.8673446\ttotal: 3m 44s\tremaining: 1m 46s\n",
            "679:\tlearn: 0.8670833\ttotal: 3m 45s\tremaining: 1m 46s\n",
            "680:\tlearn: 0.8668657\ttotal: 3m 45s\tremaining: 1m 45s\n",
            "681:\tlearn: 0.8666294\ttotal: 3m 46s\tremaining: 1m 45s\n",
            "682:\tlearn: 0.8662322\ttotal: 3m 46s\tremaining: 1m 45s\n",
            "683:\tlearn: 0.8660561\ttotal: 3m 46s\tremaining: 1m 44s\n",
            "684:\tlearn: 0.8658864\ttotal: 3m 47s\tremaining: 1m 44s\n",
            "685:\tlearn: 0.8656948\ttotal: 3m 47s\tremaining: 1m 44s\n",
            "686:\tlearn: 0.8655840\ttotal: 3m 47s\tremaining: 1m 43s\n",
            "687:\tlearn: 0.8652994\ttotal: 3m 48s\tremaining: 1m 43s\n",
            "688:\tlearn: 0.8651389\ttotal: 3m 48s\tremaining: 1m 43s\n",
            "689:\tlearn: 0.8650033\ttotal: 3m 48s\tremaining: 1m 42s\n",
            "690:\tlearn: 0.8647855\ttotal: 3m 49s\tremaining: 1m 42s\n",
            "691:\tlearn: 0.8646045\ttotal: 3m 49s\tremaining: 1m 42s\n",
            "692:\tlearn: 0.8643831\ttotal: 3m 49s\tremaining: 1m 41s\n",
            "693:\tlearn: 0.8642397\ttotal: 3m 49s\tremaining: 1m 41s\n",
            "694:\tlearn: 0.8640424\ttotal: 3m 50s\tremaining: 1m 41s\n",
            "695:\tlearn: 0.8638268\ttotal: 3m 50s\tremaining: 1m 40s\n",
            "696:\tlearn: 0.8635916\ttotal: 3m 51s\tremaining: 1m 40s\n",
            "697:\tlearn: 0.8632733\ttotal: 3m 51s\tremaining: 1m 40s\n",
            "698:\tlearn: 0.8630073\ttotal: 3m 51s\tremaining: 1m 39s\n",
            "699:\tlearn: 0.8627988\ttotal: 3m 52s\tremaining: 1m 39s\n",
            "700:\tlearn: 0.8626818\ttotal: 3m 52s\tremaining: 1m 39s\n",
            "701:\tlearn: 0.8625308\ttotal: 3m 52s\tremaining: 1m 38s\n",
            "702:\tlearn: 0.8623358\ttotal: 3m 53s\tremaining: 1m 38s\n",
            "703:\tlearn: 0.8621907\ttotal: 3m 53s\tremaining: 1m 38s\n",
            "704:\tlearn: 0.8620695\ttotal: 3m 53s\tremaining: 1m 37s\n",
            "705:\tlearn: 0.8618755\ttotal: 3m 54s\tremaining: 1m 37s\n",
            "706:\tlearn: 0.8616366\ttotal: 3m 54s\tremaining: 1m 37s\n",
            "707:\tlearn: 0.8613805\ttotal: 3m 54s\tremaining: 1m 36s\n",
            "708:\tlearn: 0.8612301\ttotal: 3m 55s\tremaining: 1m 36s\n",
            "709:\tlearn: 0.8610342\ttotal: 3m 55s\tremaining: 1m 36s\n",
            "710:\tlearn: 0.8607270\ttotal: 3m 55s\tremaining: 1m 35s\n",
            "711:\tlearn: 0.8605646\ttotal: 3m 56s\tremaining: 1m 35s\n",
            "712:\tlearn: 0.8603917\ttotal: 3m 56s\tremaining: 1m 35s\n",
            "713:\tlearn: 0.8601403\ttotal: 3m 56s\tremaining: 1m 34s\n",
            "714:\tlearn: 0.8599155\ttotal: 3m 57s\tremaining: 1m 34s\n",
            "715:\tlearn: 0.8596829\ttotal: 3m 57s\tremaining: 1m 34s\n",
            "716:\tlearn: 0.8594978\ttotal: 3m 57s\tremaining: 1m 33s\n",
            "717:\tlearn: 0.8592640\ttotal: 3m 58s\tremaining: 1m 33s\n",
            "718:\tlearn: 0.8590904\ttotal: 3m 58s\tremaining: 1m 33s\n",
            "719:\tlearn: 0.8588293\ttotal: 3m 58s\tremaining: 1m 32s\n",
            "720:\tlearn: 0.8586856\ttotal: 3m 59s\tremaining: 1m 32s\n",
            "721:\tlearn: 0.8585021\ttotal: 3m 59s\tremaining: 1m 32s\n",
            "722:\tlearn: 0.8582660\ttotal: 3m 59s\tremaining: 1m 31s\n",
            "723:\tlearn: 0.8579440\ttotal: 4m\tremaining: 1m 31s\n",
            "724:\tlearn: 0.8577183\ttotal: 4m\tremaining: 1m 31s\n",
            "725:\tlearn: 0.8575250\ttotal: 4m\tremaining: 1m 30s\n",
            "726:\tlearn: 0.8573166\ttotal: 4m\tremaining: 1m 30s\n",
            "727:\tlearn: 0.8569386\ttotal: 4m 1s\tremaining: 1m 30s\n",
            "728:\tlearn: 0.8567188\ttotal: 4m 1s\tremaining: 1m 29s\n",
            "729:\tlearn: 0.8565123\ttotal: 4m 2s\tremaining: 1m 29s\n",
            "730:\tlearn: 0.8562639\ttotal: 4m 2s\tremaining: 1m 29s\n",
            "731:\tlearn: 0.8560073\ttotal: 4m 2s\tremaining: 1m 28s\n",
            "732:\tlearn: 0.8558438\ttotal: 4m 2s\tremaining: 1m 28s\n",
            "733:\tlearn: 0.8555866\ttotal: 4m 3s\tremaining: 1m 28s\n",
            "734:\tlearn: 0.8554279\ttotal: 4m 3s\tremaining: 1m 27s\n",
            "735:\tlearn: 0.8552896\ttotal: 4m 4s\tremaining: 1m 27s\n",
            "736:\tlearn: 0.8550299\ttotal: 4m 4s\tremaining: 1m 27s\n",
            "737:\tlearn: 0.8548392\ttotal: 4m 4s\tremaining: 1m 26s\n",
            "738:\tlearn: 0.8546690\ttotal: 4m 4s\tremaining: 1m 26s\n",
            "739:\tlearn: 0.8545242\ttotal: 4m 5s\tremaining: 1m 26s\n",
            "740:\tlearn: 0.8543175\ttotal: 4m 5s\tremaining: 1m 25s\n",
            "741:\tlearn: 0.8540548\ttotal: 4m 6s\tremaining: 1m 25s\n",
            "742:\tlearn: 0.8537917\ttotal: 4m 6s\tremaining: 1m 25s\n",
            "743:\tlearn: 0.8536179\ttotal: 4m 6s\tremaining: 1m 24s\n",
            "744:\tlearn: 0.8534971\ttotal: 4m 7s\tremaining: 1m 24s\n",
            "745:\tlearn: 0.8533289\ttotal: 4m 7s\tremaining: 1m 24s\n",
            "746:\tlearn: 0.8531176\ttotal: 4m 7s\tremaining: 1m 23s\n",
            "747:\tlearn: 0.8529799\ttotal: 4m 7s\tremaining: 1m 23s\n",
            "748:\tlearn: 0.8527628\ttotal: 4m 8s\tremaining: 1m 23s\n",
            "749:\tlearn: 0.8525901\ttotal: 4m 8s\tremaining: 1m 22s\n",
            "750:\tlearn: 0.8523931\ttotal: 4m 9s\tremaining: 1m 22s\n",
            "751:\tlearn: 0.8522680\ttotal: 4m 9s\tremaining: 1m 22s\n",
            "752:\tlearn: 0.8520815\ttotal: 4m 9s\tremaining: 1m 21s\n",
            "753:\tlearn: 0.8518206\ttotal: 4m 9s\tremaining: 1m 21s\n",
            "754:\tlearn: 0.8516516\ttotal: 4m 10s\tremaining: 1m 21s\n",
            "755:\tlearn: 0.8514565\ttotal: 4m 10s\tremaining: 1m 20s\n",
            "756:\tlearn: 0.8512462\ttotal: 4m 10s\tremaining: 1m 20s\n",
            "757:\tlearn: 0.8509933\ttotal: 4m 11s\tremaining: 1m 20s\n",
            "758:\tlearn: 0.8508140\ttotal: 4m 11s\tremaining: 1m 19s\n",
            "759:\tlearn: 0.8506662\ttotal: 4m 12s\tremaining: 1m 19s\n",
            "760:\tlearn: 0.8504301\ttotal: 4m 12s\tremaining: 1m 19s\n",
            "761:\tlearn: 0.8503138\ttotal: 4m 12s\tremaining: 1m 18s\n",
            "762:\tlearn: 0.8500862\ttotal: 4m 13s\tremaining: 1m 18s\n",
            "763:\tlearn: 0.8498944\ttotal: 4m 13s\tremaining: 1m 18s\n",
            "764:\tlearn: 0.8496406\ttotal: 4m 13s\tremaining: 1m 17s\n",
            "765:\tlearn: 0.8495091\ttotal: 4m 14s\tremaining: 1m 17s\n",
            "766:\tlearn: 0.8492773\ttotal: 4m 14s\tremaining: 1m 17s\n",
            "767:\tlearn: 0.8491058\ttotal: 4m 14s\tremaining: 1m 16s\n",
            "768:\tlearn: 0.8489246\ttotal: 4m 15s\tremaining: 1m 16s\n",
            "769:\tlearn: 0.8487597\ttotal: 4m 15s\tremaining: 1m 16s\n",
            "770:\tlearn: 0.8485453\ttotal: 4m 15s\tremaining: 1m 15s\n",
            "771:\tlearn: 0.8482426\ttotal: 4m 16s\tremaining: 1m 15s\n",
            "772:\tlearn: 0.8480370\ttotal: 4m 16s\tremaining: 1m 15s\n",
            "773:\tlearn: 0.8477971\ttotal: 4m 16s\tremaining: 1m 14s\n",
            "774:\tlearn: 0.8474905\ttotal: 4m 17s\tremaining: 1m 14s\n",
            "775:\tlearn: 0.8472798\ttotal: 4m 17s\tremaining: 1m 14s\n",
            "776:\tlearn: 0.8471275\ttotal: 4m 17s\tremaining: 1m 14s\n",
            "777:\tlearn: 0.8469538\ttotal: 4m 18s\tremaining: 1m 13s\n",
            "778:\tlearn: 0.8466603\ttotal: 4m 18s\tremaining: 1m 13s\n",
            "779:\tlearn: 0.8464392\ttotal: 4m 18s\tremaining: 1m 13s\n",
            "780:\tlearn: 0.8462398\ttotal: 4m 19s\tremaining: 1m 12s\n",
            "781:\tlearn: 0.8460598\ttotal: 4m 19s\tremaining: 1m 12s\n",
            "782:\tlearn: 0.8458946\ttotal: 4m 19s\tremaining: 1m 12s\n",
            "783:\tlearn: 0.8457583\ttotal: 4m 20s\tremaining: 1m 11s\n",
            "784:\tlearn: 0.8455651\ttotal: 4m 20s\tremaining: 1m 11s\n",
            "785:\tlearn: 0.8452667\ttotal: 4m 20s\tremaining: 1m 11s\n",
            "786:\tlearn: 0.8451736\ttotal: 4m 21s\tremaining: 1m 10s\n",
            "787:\tlearn: 0.8450202\ttotal: 4m 21s\tremaining: 1m 10s\n",
            "788:\tlearn: 0.8448683\ttotal: 4m 21s\tremaining: 1m 10s\n",
            "789:\tlearn: 0.8447071\ttotal: 4m 22s\tremaining: 1m 9s\n",
            "790:\tlearn: 0.8445445\ttotal: 4m 22s\tremaining: 1m 9s\n",
            "791:\tlearn: 0.8443043\ttotal: 4m 22s\tremaining: 1m 9s\n",
            "792:\tlearn: 0.8440925\ttotal: 4m 23s\tremaining: 1m 8s\n",
            "793:\tlearn: 0.8438126\ttotal: 4m 23s\tremaining: 1m 8s\n",
            "794:\tlearn: 0.8435787\ttotal: 4m 23s\tremaining: 1m 8s\n",
            "795:\tlearn: 0.8434034\ttotal: 4m 24s\tremaining: 1m 7s\n",
            "796:\tlearn: 0.8431961\ttotal: 4m 24s\tremaining: 1m 7s\n",
            "797:\tlearn: 0.8429911\ttotal: 4m 24s\tremaining: 1m 7s\n",
            "798:\tlearn: 0.8428560\ttotal: 4m 25s\tremaining: 1m 6s\n",
            "799:\tlearn: 0.8426474\ttotal: 4m 25s\tremaining: 1m 6s\n",
            "800:\tlearn: 0.8424811\ttotal: 4m 25s\tremaining: 1m 6s\n",
            "801:\tlearn: 0.8422800\ttotal: 4m 26s\tremaining: 1m 5s\n",
            "802:\tlearn: 0.8421394\ttotal: 4m 26s\tremaining: 1m 5s\n",
            "803:\tlearn: 0.8419284\ttotal: 4m 27s\tremaining: 1m 5s\n",
            "804:\tlearn: 0.8416348\ttotal: 4m 27s\tremaining: 1m 4s\n",
            "805:\tlearn: 0.8414499\ttotal: 4m 27s\tremaining: 1m 4s\n",
            "806:\tlearn: 0.8412689\ttotal: 4m 28s\tremaining: 1m 4s\n",
            "807:\tlearn: 0.8411424\ttotal: 4m 28s\tremaining: 1m 3s\n",
            "808:\tlearn: 0.8409161\ttotal: 4m 28s\tremaining: 1m 3s\n",
            "809:\tlearn: 0.8407127\ttotal: 4m 29s\tremaining: 1m 3s\n",
            "810:\tlearn: 0.8405419\ttotal: 4m 29s\tremaining: 1m 2s\n",
            "811:\tlearn: 0.8403872\ttotal: 4m 29s\tremaining: 1m 2s\n",
            "812:\tlearn: 0.8402116\ttotal: 4m 30s\tremaining: 1m 2s\n",
            "813:\tlearn: 0.8400984\ttotal: 4m 30s\tremaining: 1m 1s\n",
            "814:\tlearn: 0.8399242\ttotal: 4m 30s\tremaining: 1m 1s\n",
            "815:\tlearn: 0.8397210\ttotal: 4m 31s\tremaining: 1m 1s\n",
            "816:\tlearn: 0.8395188\ttotal: 4m 31s\tremaining: 1m\n",
            "817:\tlearn: 0.8393219\ttotal: 4m 31s\tremaining: 1m\n",
            "818:\tlearn: 0.8391799\ttotal: 4m 32s\tremaining: 1m\n",
            "819:\tlearn: 0.8389553\ttotal: 4m 32s\tremaining: 59.8s\n",
            "820:\tlearn: 0.8388172\ttotal: 4m 32s\tremaining: 59.5s\n",
            "821:\tlearn: 0.8386751\ttotal: 4m 33s\tremaining: 59.2s\n",
            "822:\tlearn: 0.8384609\ttotal: 4m 33s\tremaining: 58.8s\n",
            "823:\tlearn: 0.8382435\ttotal: 4m 33s\tremaining: 58.5s\n",
            "824:\tlearn: 0.8379130\ttotal: 4m 34s\tremaining: 58.2s\n",
            "825:\tlearn: 0.8376090\ttotal: 4m 34s\tremaining: 57.8s\n",
            "826:\tlearn: 0.8374192\ttotal: 4m 34s\tremaining: 57.5s\n",
            "827:\tlearn: 0.8372803\ttotal: 4m 35s\tremaining: 57.2s\n",
            "828:\tlearn: 0.8370819\ttotal: 4m 35s\tremaining: 56.8s\n",
            "829:\tlearn: 0.8368772\ttotal: 4m 35s\tremaining: 56.5s\n",
            "830:\tlearn: 0.8366011\ttotal: 4m 36s\tremaining: 56.2s\n",
            "831:\tlearn: 0.8364980\ttotal: 4m 36s\tremaining: 55.8s\n",
            "832:\tlearn: 0.8363003\ttotal: 4m 36s\tremaining: 55.5s\n",
            "833:\tlearn: 0.8360586\ttotal: 4m 37s\tremaining: 55.2s\n",
            "834:\tlearn: 0.8359331\ttotal: 4m 37s\tremaining: 54.9s\n",
            "835:\tlearn: 0.8357778\ttotal: 4m 37s\tremaining: 54.5s\n",
            "836:\tlearn: 0.8355607\ttotal: 4m 38s\tremaining: 54.2s\n",
            "837:\tlearn: 0.8353317\ttotal: 4m 38s\tremaining: 53.9s\n",
            "838:\tlearn: 0.8351803\ttotal: 4m 39s\tremaining: 53.5s\n",
            "839:\tlearn: 0.8350280\ttotal: 4m 39s\tremaining: 53.2s\n",
            "840:\tlearn: 0.8348892\ttotal: 4m 39s\tremaining: 52.9s\n",
            "841:\tlearn: 0.8346700\ttotal: 4m 40s\tremaining: 52.6s\n",
            "842:\tlearn: 0.8344171\ttotal: 4m 40s\tremaining: 52.2s\n",
            "843:\tlearn: 0.8342031\ttotal: 4m 40s\tremaining: 51.9s\n",
            "844:\tlearn: 0.8340697\ttotal: 4m 41s\tremaining: 51.6s\n",
            "845:\tlearn: 0.8339007\ttotal: 4m 41s\tremaining: 51.2s\n",
            "846:\tlearn: 0.8336477\ttotal: 4m 41s\tremaining: 50.9s\n",
            "847:\tlearn: 0.8334255\ttotal: 4m 42s\tremaining: 50.6s\n",
            "848:\tlearn: 0.8332577\ttotal: 4m 42s\tremaining: 50.2s\n",
            "849:\tlearn: 0.8330344\ttotal: 4m 42s\tremaining: 49.9s\n",
            "850:\tlearn: 0.8328198\ttotal: 4m 43s\tremaining: 49.6s\n",
            "851:\tlearn: 0.8326563\ttotal: 4m 43s\tremaining: 49.2s\n",
            "852:\tlearn: 0.8324252\ttotal: 4m 43s\tremaining: 48.9s\n",
            "853:\tlearn: 0.8322648\ttotal: 4m 44s\tremaining: 48.6s\n",
            "854:\tlearn: 0.8321256\ttotal: 4m 44s\tremaining: 48.2s\n",
            "855:\tlearn: 0.8320178\ttotal: 4m 44s\tremaining: 47.9s\n",
            "856:\tlearn: 0.8318223\ttotal: 4m 44s\tremaining: 47.5s\n",
            "857:\tlearn: 0.8316585\ttotal: 4m 45s\tremaining: 47.2s\n",
            "858:\tlearn: 0.8314387\ttotal: 4m 45s\tremaining: 46.9s\n",
            "859:\tlearn: 0.8313028\ttotal: 4m 45s\tremaining: 46.6s\n",
            "860:\tlearn: 0.8311653\ttotal: 4m 46s\tremaining: 46.2s\n",
            "861:\tlearn: 0.8308992\ttotal: 4m 46s\tremaining: 45.9s\n",
            "862:\tlearn: 0.8305869\ttotal: 4m 47s\tremaining: 45.6s\n",
            "863:\tlearn: 0.8302595\ttotal: 4m 47s\tremaining: 45.2s\n",
            "864:\tlearn: 0.8300821\ttotal: 4m 47s\tremaining: 44.9s\n",
            "865:\tlearn: 0.8299001\ttotal: 4m 48s\tremaining: 44.6s\n",
            "866:\tlearn: 0.8296663\ttotal: 4m 48s\tremaining: 44.2s\n",
            "867:\tlearn: 0.8294480\ttotal: 4m 48s\tremaining: 43.9s\n",
            "868:\tlearn: 0.8292281\ttotal: 4m 49s\tremaining: 43.6s\n",
            "869:\tlearn: 0.8290565\ttotal: 4m 49s\tremaining: 43.3s\n",
            "870:\tlearn: 0.8288809\ttotal: 4m 49s\tremaining: 42.9s\n",
            "871:\tlearn: 0.8287056\ttotal: 4m 50s\tremaining: 42.6s\n",
            "872:\tlearn: 0.8284909\ttotal: 4m 50s\tremaining: 42.3s\n",
            "873:\tlearn: 0.8283901\ttotal: 4m 50s\tremaining: 41.9s\n",
            "874:\tlearn: 0.8281906\ttotal: 4m 51s\tremaining: 41.6s\n",
            "875:\tlearn: 0.8279309\ttotal: 4m 51s\tremaining: 41.2s\n",
            "876:\tlearn: 0.8277463\ttotal: 4m 51s\tremaining: 40.9s\n",
            "877:\tlearn: 0.8276013\ttotal: 4m 52s\tremaining: 40.6s\n",
            "878:\tlearn: 0.8274411\ttotal: 4m 52s\tremaining: 40.2s\n",
            "879:\tlearn: 0.8272524\ttotal: 4m 52s\tremaining: 39.9s\n",
            "880:\tlearn: 0.8271437\ttotal: 4m 52s\tremaining: 39.6s\n",
            "881:\tlearn: 0.8269221\ttotal: 4m 53s\tremaining: 39.2s\n",
            "882:\tlearn: 0.8267540\ttotal: 4m 53s\tremaining: 38.9s\n",
            "883:\tlearn: 0.8266583\ttotal: 4m 53s\tremaining: 38.6s\n",
            "884:\tlearn: 0.8264524\ttotal: 4m 54s\tremaining: 38.3s\n",
            "885:\tlearn: 0.8262027\ttotal: 4m 54s\tremaining: 37.9s\n",
            "886:\tlearn: 0.8259389\ttotal: 4m 55s\tremaining: 37.6s\n",
            "887:\tlearn: 0.8258004\ttotal: 4m 55s\tremaining: 37.3s\n",
            "888:\tlearn: 0.8254847\ttotal: 4m 55s\tremaining: 36.9s\n",
            "889:\tlearn: 0.8252711\ttotal: 4m 56s\tremaining: 36.6s\n",
            "890:\tlearn: 0.8251045\ttotal: 4m 56s\tremaining: 36.3s\n",
            "891:\tlearn: 0.8248807\ttotal: 4m 56s\tremaining: 35.9s\n",
            "892:\tlearn: 0.8246370\ttotal: 4m 57s\tremaining: 35.6s\n",
            "893:\tlearn: 0.8244898\ttotal: 4m 57s\tremaining: 35.3s\n",
            "894:\tlearn: 0.8243059\ttotal: 4m 57s\tremaining: 34.9s\n",
            "895:\tlearn: 0.8241042\ttotal: 4m 58s\tremaining: 34.6s\n",
            "896:\tlearn: 0.8239138\ttotal: 4m 58s\tremaining: 34.3s\n",
            "897:\tlearn: 0.8237632\ttotal: 4m 58s\tremaining: 33.9s\n",
            "898:\tlearn: 0.8235611\ttotal: 4m 59s\tremaining: 33.6s\n",
            "899:\tlearn: 0.8233542\ttotal: 4m 59s\tremaining: 33.3s\n",
            "900:\tlearn: 0.8231097\ttotal: 4m 59s\tremaining: 33s\n",
            "901:\tlearn: 0.8229123\ttotal: 5m\tremaining: 32.6s\n",
            "902:\tlearn: 0.8227609\ttotal: 5m\tremaining: 32.3s\n",
            "903:\tlearn: 0.8226098\ttotal: 5m\tremaining: 32s\n",
            "904:\tlearn: 0.8224000\ttotal: 5m 1s\tremaining: 31.6s\n",
            "905:\tlearn: 0.8222408\ttotal: 5m 1s\tremaining: 31.3s\n",
            "906:\tlearn: 0.8221186\ttotal: 5m 1s\tremaining: 31s\n",
            "907:\tlearn: 0.8218373\ttotal: 5m 2s\tremaining: 30.6s\n",
            "908:\tlearn: 0.8216962\ttotal: 5m 2s\tremaining: 30.3s\n",
            "909:\tlearn: 0.8214477\ttotal: 5m 2s\tremaining: 30s\n",
            "910:\tlearn: 0.8212068\ttotal: 5m 3s\tremaining: 29.6s\n",
            "911:\tlearn: 0.8209716\ttotal: 5m 3s\tremaining: 29.3s\n",
            "912:\tlearn: 0.8207502\ttotal: 5m 4s\tremaining: 29s\n",
            "913:\tlearn: 0.8204867\ttotal: 5m 4s\tremaining: 28.6s\n",
            "914:\tlearn: 0.8202499\ttotal: 5m 4s\tremaining: 28.3s\n",
            "915:\tlearn: 0.8200406\ttotal: 5m 5s\tremaining: 28s\n",
            "916:\tlearn: 0.8198557\ttotal: 5m 5s\tremaining: 27.6s\n",
            "917:\tlearn: 0.8196397\ttotal: 5m 5s\tremaining: 27.3s\n",
            "918:\tlearn: 0.8194799\ttotal: 5m 5s\tremaining: 27s\n",
            "919:\tlearn: 0.8192456\ttotal: 5m 6s\tremaining: 26.6s\n",
            "920:\tlearn: 0.8191092\ttotal: 5m 6s\tremaining: 26.3s\n",
            "921:\tlearn: 0.8189996\ttotal: 5m 7s\tremaining: 26s\n",
            "922:\tlearn: 0.8187377\ttotal: 5m 7s\tremaining: 25.6s\n",
            "923:\tlearn: 0.8185629\ttotal: 5m 7s\tremaining: 25.3s\n",
            "924:\tlearn: 0.8184072\ttotal: 5m 8s\tremaining: 25s\n",
            "925:\tlearn: 0.8182750\ttotal: 5m 8s\tremaining: 24.6s\n",
            "926:\tlearn: 0.8180624\ttotal: 5m 8s\tremaining: 24.3s\n",
            "927:\tlearn: 0.8178622\ttotal: 5m 9s\tremaining: 24s\n",
            "928:\tlearn: 0.8177092\ttotal: 5m 9s\tremaining: 23.7s\n",
            "929:\tlearn: 0.8174898\ttotal: 5m 9s\tremaining: 23.3s\n",
            "930:\tlearn: 0.8173296\ttotal: 5m 10s\tremaining: 23s\n",
            "931:\tlearn: 0.8171792\ttotal: 5m 10s\tremaining: 22.6s\n",
            "932:\tlearn: 0.8170874\ttotal: 5m 10s\tremaining: 22.3s\n",
            "933:\tlearn: 0.8169719\ttotal: 5m 11s\tremaining: 22s\n",
            "934:\tlearn: 0.8167643\ttotal: 5m 11s\tremaining: 21.6s\n",
            "935:\tlearn: 0.8165716\ttotal: 5m 11s\tremaining: 21.3s\n",
            "936:\tlearn: 0.8164108\ttotal: 5m 12s\tremaining: 21s\n",
            "937:\tlearn: 0.8162655\ttotal: 5m 12s\tremaining: 20.6s\n",
            "938:\tlearn: 0.8160442\ttotal: 5m 12s\tremaining: 20.3s\n",
            "939:\tlearn: 0.8158639\ttotal: 5m 13s\tremaining: 20s\n",
            "940:\tlearn: 0.8156542\ttotal: 5m 13s\tremaining: 19.6s\n",
            "941:\tlearn: 0.8154138\ttotal: 5m 13s\tremaining: 19.3s\n",
            "942:\tlearn: 0.8151963\ttotal: 5m 14s\tremaining: 19s\n",
            "943:\tlearn: 0.8149157\ttotal: 5m 14s\tremaining: 18.7s\n",
            "944:\tlearn: 0.8147023\ttotal: 5m 14s\tremaining: 18.3s\n",
            "945:\tlearn: 0.8144814\ttotal: 5m 15s\tremaining: 18s\n",
            "946:\tlearn: 0.8142875\ttotal: 5m 15s\tremaining: 17.7s\n",
            "947:\tlearn: 0.8141286\ttotal: 5m 15s\tremaining: 17.3s\n",
            "948:\tlearn: 0.8138944\ttotal: 5m 16s\tremaining: 17s\n",
            "949:\tlearn: 0.8137598\ttotal: 5m 16s\tremaining: 16.7s\n",
            "950:\tlearn: 0.8135311\ttotal: 5m 16s\tremaining: 16.3s\n",
            "951:\tlearn: 0.8133686\ttotal: 5m 17s\tremaining: 16s\n",
            "952:\tlearn: 0.8132421\ttotal: 5m 17s\tremaining: 15.7s\n",
            "953:\tlearn: 0.8129999\ttotal: 5m 17s\tremaining: 15.3s\n",
            "954:\tlearn: 0.8127141\ttotal: 5m 18s\tremaining: 15s\n",
            "955:\tlearn: 0.8126176\ttotal: 5m 18s\tremaining: 14.7s\n",
            "956:\tlearn: 0.8123445\ttotal: 5m 18s\tremaining: 14.3s\n",
            "957:\tlearn: 0.8121818\ttotal: 5m 19s\tremaining: 14s\n",
            "958:\tlearn: 0.8119298\ttotal: 5m 19s\tremaining: 13.7s\n",
            "959:\tlearn: 0.8118252\ttotal: 5m 19s\tremaining: 13.3s\n",
            "960:\tlearn: 0.8116612\ttotal: 5m 20s\tremaining: 13s\n",
            "961:\tlearn: 0.8114808\ttotal: 5m 20s\tremaining: 12.7s\n",
            "962:\tlearn: 0.8113662\ttotal: 5m 20s\tremaining: 12.3s\n",
            "963:\tlearn: 0.8112372\ttotal: 5m 21s\tremaining: 12s\n",
            "964:\tlearn: 0.8110354\ttotal: 5m 21s\tremaining: 11.7s\n",
            "965:\tlearn: 0.8108301\ttotal: 5m 21s\tremaining: 11.3s\n",
            "966:\tlearn: 0.8106307\ttotal: 5m 22s\tremaining: 11s\n",
            "967:\tlearn: 0.8104591\ttotal: 5m 22s\tremaining: 10.7s\n",
            "968:\tlearn: 0.8103237\ttotal: 5m 22s\tremaining: 10.3s\n",
            "969:\tlearn: 0.8100185\ttotal: 5m 23s\tremaining: 10s\n",
            "970:\tlearn: 0.8097974\ttotal: 5m 23s\tremaining: 9.67s\n",
            "971:\tlearn: 0.8095586\ttotal: 5m 24s\tremaining: 9.34s\n",
            "972:\tlearn: 0.8093804\ttotal: 5m 24s\tremaining: 9s\n",
            "973:\tlearn: 0.8091712\ttotal: 5m 24s\tremaining: 8.67s\n",
            "974:\tlearn: 0.8089833\ttotal: 5m 25s\tremaining: 8.34s\n",
            "975:\tlearn: 0.8087611\ttotal: 5m 25s\tremaining: 8s\n",
            "976:\tlearn: 0.8086059\ttotal: 5m 25s\tremaining: 7.67s\n",
            "977:\tlearn: 0.8085075\ttotal: 5m 26s\tremaining: 7.34s\n",
            "978:\tlearn: 0.8082654\ttotal: 5m 26s\tremaining: 7s\n",
            "979:\tlearn: 0.8081354\ttotal: 5m 26s\tremaining: 6.67s\n",
            "980:\tlearn: 0.8079377\ttotal: 5m 27s\tremaining: 6.34s\n",
            "981:\tlearn: 0.8077142\ttotal: 5m 27s\tremaining: 6s\n",
            "982:\tlearn: 0.8076197\ttotal: 5m 27s\tremaining: 5.67s\n",
            "983:\tlearn: 0.8074609\ttotal: 5m 28s\tremaining: 5.33s\n",
            "984:\tlearn: 0.8073325\ttotal: 5m 28s\tremaining: 5s\n",
            "985:\tlearn: 0.8071945\ttotal: 5m 28s\tremaining: 4.67s\n",
            "986:\tlearn: 0.8069252\ttotal: 5m 29s\tremaining: 4.33s\n",
            "987:\tlearn: 0.8067537\ttotal: 5m 29s\tremaining: 4s\n",
            "988:\tlearn: 0.8064813\ttotal: 5m 29s\tremaining: 3.67s\n",
            "989:\tlearn: 0.8063502\ttotal: 5m 30s\tremaining: 3.33s\n",
            "990:\tlearn: 0.8060735\ttotal: 5m 30s\tremaining: 3s\n",
            "991:\tlearn: 0.8058346\ttotal: 5m 30s\tremaining: 2.67s\n",
            "992:\tlearn: 0.8056751\ttotal: 5m 31s\tremaining: 2.33s\n",
            "993:\tlearn: 0.8054553\ttotal: 5m 31s\tremaining: 2s\n",
            "994:\tlearn: 0.8053695\ttotal: 5m 31s\tremaining: 1.67s\n",
            "995:\tlearn: 0.8051471\ttotal: 5m 32s\tremaining: 1.33s\n",
            "996:\tlearn: 0.8049319\ttotal: 5m 32s\tremaining: 1s\n",
            "997:\tlearn: 0.8046876\ttotal: 5m 32s\tremaining: 667ms\n",
            "998:\tlearn: 0.8044576\ttotal: 5m 33s\tremaining: 334ms\n",
            "999:\tlearn: 0.8042419\ttotal: 5m 33s\tremaining: 0us\n",
            "0.43615424001326764\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LOEfvKg2pah2",
        "outputId": "5a208985-cb49-4e59-8cba-e67015ddd401"
      },
      "source": [
        "\n",
        "model_BN = KNeighborsClassifier(n_neighbors=3)\n",
        "model_BN.fit(X_train,y_train)\n",
        "prediction = model_BN.predict(X_test)\n",
        "\n",
        "print(f1_score(prediction, y_test, average='macro'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.3859313883711379\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "KNeighborsClassifier with unscaled data with polynomial features 0.5250249043821201\n",
        "\n",
        "KNeighborsClassifier with scaled data with polynomial features 0.5696967560064748\n",
        "\n",
        "KNeighborsClassifier with RandomSMOTESampling with scaled data with polynomial features 0.5037778383891123\n",
        "\n",
        "KNeighborsClassifier with RandomSMOTESampling with scaled data without polynomial features 0.49872979987208166\n",
        "\n",
        "KNeighborsClassifier with RandomSMOTESampling in Train Test split  with scaled data with polynomial features 0.3859313883711379"
      ],
      "metadata": {
        "id": "QAEryT6PBi5K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_rfc11 = RandomForestClassifier()\n",
        "model_rfc11.fit(X_train2,y_train2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o5zz0lweslSY",
        "outputId": "748ba71e-fc77-4cf4-ab0b-e9055c67c711"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier()"
            ]
          },
          "metadata": {},
          "execution_count": 144
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdIyn1UdO4yV"
      },
      "source": [
        "feature_importance_df = pd.DataFrame(model_rfc11.feature_importances_, columns=['importance'])\n",
        "feature_importance_df['feature'] = X.columns\n",
        "\n",
        "#plt.figure(figsize=(20, 12));\n",
        "#sns.barplot(x=\"importance\", y=\"feature\", data=feature_importance_df.sort_values(by = ['importance'], ascending = False).head(100))\n",
        "#plt.title('Model features importance:')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3WbXHjafQ0w"
      },
      "source": [
        "new_feat = feature_importance_df.sort_values(by = ['importance'], ascending = False).head(100)['feature'].to_list()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EsITpBLJ5_Qa",
        "outputId": "1e76e416-2f38-4ad9-eb14-317a9de5bfdb"
      },
      "source": [
        "from collections import Counter\n",
        "print(Counter(y))\n",
        "print(Counter(y_))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counter({2: 26236, 0: 17807, 1: 16030})\n",
            "Counter({0: 26236, 1: 26236, 2: 26236})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GEU7SSMRE1Pz",
        "outputId": "07da2f1c-1c79-438f-a422-e749ee34a76d"
      },
      "source": [
        "oof_f1=[]\n",
        "\n",
        "\n",
        "\n",
        "fold=StratifiedKFold(n_splits=5)#15#5#10\n",
        "\n",
        "i=1\n",
        "for train_index, test_index in fold.split(X_res,y_res):\n",
        "        \n",
        "    X_train, X_test = X_res[train_index], X_res[test_index]\n",
        "    y_train, y_test = y_res[train_index], y_res[test_index]\n",
        "\n",
        "    model = ltb.LGBMClassifier(**params, n_estimators=1000)\n",
        "\n",
        "    model.fit(X_train,y_train,eval_set=[(X_train,y_train),(X_test, y_test)], early_stopping_rounds=300, verbose = False)#erly100\n",
        "    preds=model.predict(X_test)\n",
        "\n",
        "    print(f\"F1, fold {i}: {f1_score(y_test,preds,average='macro')}\")\n",
        "    i+=1                \n",
        "    oof_f1.append(f1_score(y_test,preds,average='macro'))\n",
        "    \n",
        "\n",
        "print(np.mean(oof_f1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "F1, fold 1: 0.4010994052203311\n",
            "F1, fold 2: 0.3331628175657059\n",
            "F1, fold 3: 0.39592396609140196\n",
            "F1, fold 4: 0.6887319543791568\n",
            "F1, fold 5: 0.7019573810662522\n",
            "0.5041751048645696\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "pressing-psychiatry",
        "outputId": "4bef1dec-0f3f-4377-96f7-3c18cc268d70"
      },
      "source": [
        "oof_f1=[]\n",
        "oof_predictions=[]\n",
        "\n",
        "\n",
        "\n",
        "fold=StratifiedKFold(n_splits=5)#15#5#10\n",
        "\n",
        "i=1\n",
        "for train_index, test_index in fold.split(X_res,y_res):\n",
        "        \n",
        "    X_train, X_test = X_res[train_index], X_res[test_index]\n",
        "    y_train, y_test = y_res[train_index], y_res[test_index]\n",
        "\n",
        "    model_rfc = RandomForestClassifier(**params)\n",
        "\n",
        "    model_rfc.fit(X_train,y_train)#erly100\n",
        "    preds=model_rfc.predict(X_test)\n",
        "\n",
        "    print(f\"F1, fold {i}: {f1_score(y_test,preds,average='macro')}\")\n",
        "    i+=1                \n",
        "    oof_f1.append(f1_score(y_test,preds,average='macro'))\n",
        "    \n",
        "\n",
        "print(np.mean(oof_f1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "F1, fold 1: 0.5727226964386132\n",
            "F1, fold 2: 0.39995116868667746\n",
            "F1, fold 3: 0.47517263686918226\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-46-cc0e8b582579>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mmodel_rfc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mmodel_rfc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#erly100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mpreds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_rfc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    457\u001b[0m                     \u001b[0mn_samples_bootstrap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_samples_bootstrap\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m                 )\n\u001b[0;32m--> 459\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrees\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    460\u001b[0m             )\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1044\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1046\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1047\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    859\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    860\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 861\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    862\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    863\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    777\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 263\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 263\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/fixes.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[0;34m(tree, forest, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurr_sample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    940\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 942\u001b[0;31m             \u001b[0mX_idx_sorted\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_idx_sorted\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    943\u001b[0m         )\n\u001b[1;32m    944\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    418\u001b[0m             )\n\u001b[1;32m    419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 420\u001b[0;31m         \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ayrpIxTIfsV5"
      },
      "source": [
        "import optuna"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f18GrB8vftm6"
      },
      "source": [
        "X = new_train[new_feat]\n",
        "y = new_train['FTR']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rapid-builder"
      },
      "source": [
        "def objective(trial):\n",
        "    oof_f1=[]\n",
        "    oof_predictions=[]\n",
        "\n",
        "\n",
        "    param = {}\n",
        "    param['n_estimators'] = trial.suggest_int(\"n_estimators\", 100, 2000, 50)\n",
        "    param['max_depth'] = trial.suggest_int(\"max_depth\", 1, 100)\n",
        "    param['criterion'] = trial.suggest_categorical(\"criterion\", [\"gini\",\"entropy\"])\n",
        "    param['min_samples_split'] = trial.suggest_int(\"min_samples_split\", 1, 10, 1)\n",
        "    param['min_samples_leaf'] = trial.suggest_int(\"min_samples_leaf\", 1, 5, 1)\n",
        "    param['max_features'] = trial.suggest_categorical(\"max_features\", [\"auto\",\"sqrt\",\"log2\"])\n",
        "    param['random_state'] = 0\n",
        "    param['class_weight'] = trial.suggest_categorical(\"class_weight\", [\"balanced\",\"balanced_subsample\",None])\n",
        "    #param['bootstrap'] = trial.suggest_categorical(\"bootstrap\", [True,False])\n",
        "    param['verbose'] = 0\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    fold=StratifiedKFold(n_splits=5)#15#5#10\n",
        "    i=1\n",
        "    for train_index, test_index in fold.split(X_res,y_res):\n",
        "        \n",
        "        X_train, X_test = X_res[train_index], X_res[test_index]\n",
        "        y_train, y_test = y_res[train_index], y_res[test_index]\n",
        "        model_rfc = RandomForestClassifier(**param)\n",
        "    \n",
        "\n",
        "        model_rfc.fit(X_train,y_train)#erly100\n",
        "        preds=model_rfc.predict(X_test)\n",
        "\n",
        "        oof_f1.append(f1_score(y_test,preds,average='macro'))\n",
        "\n",
        "    return np.mean(oof_f1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "AgZJRC9WgH5h",
        "outputId": "f249f23c-6165-40b8-90de-3eaa9a3f14fe"
      },
      "source": [
        "study = optuna.create_study(direction=\"maximize\")\n",
        "study.optimize(objective, n_trials=100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2021-11-18 05:14:13,872]\u001b[0m A new study created in memory with name: no-name-887920cd-310d-4f06-91fe-32d4499fa033\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "generic-great",
        "outputId": "44674645-3b81-4b3d-ed0d-be22d18715b7"
      },
      "source": [
        "print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
        "\n",
        "print(\"Best trial:\")\n",
        "trial = study.best_trial\n",
        "\n",
        "print(\"  Value: {}\".format(trial.value))\n",
        "\n",
        "print(\"  Params: \")\n",
        "for key, value in trial.params.items():\n",
        "    print(\"    {}: {}\".format(key, value))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of finished trials: 4\n",
            "Best trial:\n",
            "  Value: 0.6272473245848953\n",
            "  Params: \n",
            "    n_iterators: 1650\n",
            "    max_depth: 88\n",
            "    criterion: gini\n",
            "    min_samples_split: 6\n",
            "    min_samples_leaf: 3\n",
            "    max_features: log2\n",
            "    bootstrap: False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iV7BuV2Ptrn1",
        "outputId": "edc58010-5e1c-45f8-a14e-ac82d6a0dbd6"
      },
      "source": [
        "trial.params"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'bootstrap': False,\n",
              " 'criterion': 'gini',\n",
              " 'max_depth': 88,\n",
              " 'max_features': 'log2',\n",
              " 'min_samples_leaf': 3,\n",
              " 'min_samples_split': 6,\n",
              " 'n_iterators': 1650}"
            ]
          },
          "execution_count": 73,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NNXAokAagss9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}